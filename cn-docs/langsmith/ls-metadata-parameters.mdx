---
title: 元数据参数参考
sidebarTitle: Metadata parameters
---
使用 LangSmith 追踪 LLM 调用时，您通常希望[跟踪成本](/langsmith/cost-tracking)、比较模型配置以及分析不同供应商的性能。LangSmith 的原生集成（如 [LangChain](/langsmith/trace-with-langchain) 或 [OpenAI](/langsmith/trace-openai)/[Anthropic](/langsmith/trace-anthropic) 包装器）会自动处理这些，但自定义模型包装器和自托管模型需要一种标准化的方式来提供这些信息。为此，LangSmith 使用了 `ls_` 元数据参数。

这些元数据参数（均以 `ls_` 为前缀）允许您通过标准的 `metadata` 字段传递模型配置和识别信息。一旦设置，LangSmith 就可以自动计算成本、在 UI 中显示模型信息，并支持跨追踪进行过滤和分析。

使用 `ls_` 元数据参数可以：

- **为自定义或自托管模型启用自动成本跟踪**，通过识别供应商和模型名称。
- **跟踪模型配置**，如温度、最大令牌数和其他参数，以便进行实验比较。
- **按供应商或配置设置过滤和分析追踪**
- **通过准确记录每次运行使用的模型设置来改进调试**

## 基本用法示例

最常见的用例是为自定义模型包装器启用成本跟踪。为此，您需要提供两个关键信息：供应商名称 (`ls_provider`) 和模型名称 (`ls_model_name`)。它们共同作用，与 LangSmith 的定价数据库进行匹配。

<CodeGroup>

```python Python
from langsmith import traceable

@traceable(
    run_type="llm",
    metadata={
        "ls_provider": "my_provider",
        "ls_model_name": "my_custom_model"
    }
)
def my_custom_llm(prompt: str):
    return call_custom_api(prompt)
```

```typescript TypeScript
import { traceable } from "langsmith/traceable";

const myCustomLlm = traceable(
  async (prompt: string) => {
    return callCustomApi(prompt);
  },
  {
    run_type: "llm",
    metadata: {
      ls_provider: "my_provider",
      ls_model_name: "my_custom_model"
    }
  }
);
```

</CodeGroup>

这个最小化设置告诉 LangSmith 您正在使用的模型，如果该模型存在于定价数据库中或您已[配置自定义定价](/langsmith/cost-tracking#set-up-model-pricing)，则启用自动成本计算。

为了进行更全面的跟踪，您可以包含额外的配置参数。这在[运行实验](/langsmith/evaluation-quickstart)或比较不同模型设置时特别有用：

<CodeGroup>

```python Python
@traceable(
    run_type="llm",
    metadata={
        "ls_provider": "openai",
        "ls_model_name": "gpt-4o",
        "ls_temperature": 0.7,
        "ls_max_tokens": 4096,
        "ls_stop": ["END"],
        "ls_invocation_params": {
            "top_p": 0.9,
            "frequency_penalty": 0.5
        }
    }
)
def my_configured_llm(messages: list):
    return call_llm(messages)
```

```typescript TypeScript
const myConfiguredLlm = traceable(
  async (messages: Array<any>) => {
    return callLlm(messages);
  },
  {
    run_type: "llm",
    metadata: {
      ls_provider: "openai",
      ls_model_name: "gpt-4o",
      ls_temperature: 0.7,
      ls_max_tokens: 4096,
      ls_stop: ["END"],
      ls_invocation_params: {
        top_p: 0.9,
        frequency_penalty: 0.5
      }
    }
  }
);
```

</CodeGroup>

通过此设置，您可以稍后按温度过滤追踪、比较具有不同最大令牌设置的运行，或分析哪些配置参数产生最佳结果。除了成本跟踪所需的 `ls_provider` 和 `ls_model_name` 配对外，所有这些参数都是可选的。

## 所有参数

### 用户可配置参数

| 参数 | 类型 | 必填 | 描述 |
|-----------|------|----------|-------------|
| [`ls_provider`](#ls-provider) | `string` | 是* | 用于成本跟踪的 LLM 供应商名称 |
| [`ls_model_name`](#ls-model-name) | `string` | 是* | 用于成本跟踪的模型标识符 |
| [`ls_temperature`](#ls-temperature) | `number` | 否 | 使用的温度参数 |
| [`ls_max_tokens`](#ls-max-tokens) | `number` | 否 | 使用的最大令牌参数 |
| [`ls_stop`](#ls-stop) | `string[]` | 否 | 使用的停止序列 |
| [`ls_invocation_params`](#ls-invocation-params) | `object` | 否 | 额外的调用参数 |

\* 成本跟踪必须同时提供 `ls_provider` 和 `ls_model_name`

### 系统生成参数

| 参数 | 类型 | 描述 |
|-----------|------|-------------|
| [`ls_run_depth`](#ls-run-depth) | `integer` | 在追踪树中的深度（0=根，1=子节点等）- 自动计算 |
| [`ls_method`](#ls-method) | `string` | 使用的追踪方法（例如，"traceable"）- 由 SDK 设置 |

### 实验参数

| 参数 | 类型 | 描述 |
|-----------|------|-------------|
| [`ls_example_*`](#ls-example-) | `any` | 以 `ls_example_` 为前缀的示例元数据 - 在实验期间添加 |
| [`ls_experiment_id`](#ls-experiment-id) | `string` (UUID) | 唯一的实验标识符 - 在实验期间添加 |

## 参数详情

### `ls_provider`

- **类型:** `string`
- **必填:** 是（与 [`ls_model_name`](#ls-model-name) 一起）

**作用:**
标识 LLM 供应商。与 `ls_model_name` 结合，通过匹配 [LangSmith 的模型定价数据库](https://smith.langchain.com/settings/workspaces/models) 来启用自动成本计算。

**常见值:**
- `"openai"`
- `"anthropic"`
- `"azure"`
- `"bedrock"`
- `"google_vertexai"`
- `"google_genai"`
- `"fireworks"`
- `"mistral"`
- `"groq"`
- 或任何自定义字符串

**何时使用:**
当您希望为自定义模型包装器或自托管模型启用[自动成本跟踪](/langsmith/cost-tracking)时。

**示例:**
```python
@traceable(
    run_type="llm",
    metadata={
        "ls_provider": "openai",
        "ls_model_name": "gpt-4o"
    }
)
def my_llm_call(prompt: str):
    return call_api(prompt)
```

**关联关系:**
- **需要** [`ls_model_name`](#ls-model-name) 才能使成本跟踪生效。
- 与令牌使用数据一起工作以计算成本。

### `ls_model_name`

- **类型:** `string`
- **必填:** 是（与 `ls_provider` 一起）

**作用:**
标识特定模型。与 `ls_provider` 结合，匹配定价数据库以进行自动成本计算。

**常见值:**
- OpenAI: `"gpt-4o"`, `"gpt-4o-mini"`, `"gpt-3.5-turbo"`
- Anthropic: `"claude-3-5-sonnet-20241022"`, `"claude-3-opus-20240229"`
- 自定义: 任何模型标识符

**何时使用:**
当您希望在 [UI](https://smith.langchain.com) 中启用自动[成本跟踪](/langsmith/cost-tracking)和模型识别时。

**示例:**
```python
@traceable(
    run_type="llm",
    metadata={
        "ls_provider": "anthropic",
        "ls_model_name": "claude-3-5-sonnet-20241022"
    }
)
def my_claude_call(messages: list):
    return call_claude(messages)
```

**关联关系:**
- **需要** [`ls_provider`](#ls-provider) 才能使成本跟踪生效。
- 与令牌使用数据一起工作以计算成本。

### `ls_temperature`

- **类型:** `number` (可为空)
- **必填:** 否

**作用:**
记录使用的温度设置。这仅用于跟踪，不影响 LangSmith 行为。

**何时使用:**
当您希望为实验或调试跟踪模型配置时。

**示例:**
```python
metadata={
    "ls_provider": "openai",
    "ls_model_name": "gpt-4o",
    "ls_temperature": 0.7
}
```

**关联关系:**
- 独立；仅用于跟踪。
- 与其他配置参数一起用于实验比较很有用。

### `ls_max_tokens`

- **类型:** `number` (可为空)
- **必填:** 否

**作用:**
记录使用的最大令牌设置。这仅用于跟踪，不影响 LangSmith 行为。

**何时使用:**
当您希望为实验或调试跟踪模型配置时。

**示例:**
```python
metadata={
    "ls_provider": "openai",
    "ls_model_name": "gpt-4o",
    "ls_max_tokens": 4096
}
```

**关联关系:**
- 独立；仅用于跟踪。
- 与实际令牌使用情况结合用于成本分析很有用。

### `ls_stop`

- **类型:** `string[]` (可为空)
- **必填:** 否

**作用:**
记录使用的停止序列。这仅用于跟踪，不影响 LangSmith 行为。

**何时使用:**
当您希望为实验或调试跟踪模型配置时。

**示例:**
```python
metadata={
    "ls_provider": "openai",
    "ls_model_name": "gpt-4o",
    "ls_stop": ["END", "STOP", "\n\n"]
}
```

**关联关系:**
- 独立；仅用于跟踪。

### `ls_invocation_params`

- **类型:** `object` (任意键值对)
- **必填:** 否

**作用:**
存储不适合特定 `ls_` 参数的额外模型参数。可以包含供应商特定的设置。

**常见参数:**
`top_p`, `frequency_penalty`, `presence_penalty`, `top_k`, `seed`, 或任何自定义参数

**何时使用:**
当您需要跟踪超出标准参数的额外配置时。

**示例:**
```python
metadata={
    "ls_provider": "openai",
    "ls_model_name": "gpt-4o",
    "ls_invocation_params": {
        "top_p": 0.9,
        "frequency_penalty": 0.5,
        "presence_penalty": 0.3,
        "seed": 12345
    }
}
```

**关联关系:**
- 独立；存储任意配置。

### `ls_run_depth`

- **类型:** `integer`
- **设置者:** LangSmith 后端（自动）
- **无法被覆盖**

**作用:**
指示在追踪树中的深度：
- `0` = 根运行（顶层）
- `1` = 直接子节点
- `2` = 孙节点
- 等等。

**何时使用:**
在追踪摄取期间自动计算。用于过滤（例如，“仅显示根运行”）和 UI 可视化。

**示例查询:**
```
metadata_key = 'ls_run_depth' AND metadata_value = 0
```

**关联关系:**
- 由追踪父子结构决定。
- 无法手动设置。

### `ls_method`

- **类型:** `string`
- **设置者:** SDK（自动）

**作用:**
指示哪个 SDK 方法创建了追踪（对于 `@traceable` 装饰器，通常是 `"traceable"`）。

**何时使用:**
由追踪 SDK 自动设置。用于调试和分析。

**关联关系:**
- 由 SDK 根据创建追踪的方式设置。
- 无法手动设置。

### `ls_example_*`

- **类型:** 任意（取决于示例元数据）
- **模式:** `ls_example_{original_key}`
- **设置者:** LangSmith 实验系统（自动）

**作用:**
当在数据集上[运行实验](/langsmith/evaluation-quickstart)时，来自示例的元数据会自动加上 `ls_example_` 前缀并添加到追踪中。

**特殊参数:**
- `ls_example_dataset_split`: 数据集分割（例如，"train", "test", "validation"）

**何时使用:**
在数据集实验期间。允许按示例特征进行过滤/分组。

**示例:**
如果示例有元数据 `{"category": "technical", "difficulty": "hard"}`，追踪将获得：
```json
{
  "metadata": {
    "ls_example_category": "technical",
    "ls_example_difficulty": "hard",
    "ls_example_dataset_split": "test"
  }
}
```

**关联关系:**
- 自动从示例元数据派生。
- 无法在追踪上手动设置。

### `ls_experiment_id`

- **类型:** `string` (UUID)
- **设置者:** LangSmith 实验系统（自动）

**作用:**
实验运行的唯一标识符。

**何时使用:**
在数据集上运行[实验/评估](/langsmith/evaluation-quickstart)时自动添加。用于分组来自同一实验的所有运行。

**关联关系:**
- 将运行链接到特定实验。
- 无法手动设置。

## 参数关联关系

### 成本跟踪依赖项

为了使 LangSmith 自动计算成本，几个参数必须协同工作。以下是所需内容：

**主要要求:** [`ls_provider`](#ls-provider) + [`ls_model_name`](#ls-model-name)
- 两者都应存在以进行自动成本计算。
- 如果缺少 [`ls_model_name`](#ls-model-name)，系统将回退到检查 [`ls_invocation_params`](#ls-invocation-params) 中的模型名称。
- [`ls_provider`](#ls-provider) 必须匹配[定价数据库](https://smith.langchain.com/settings/workspaces/models)中的供应商（或使用自定义定价）。

**额外要求:**
- 运行必须具有 `run_type="llm"`（或必须启用[任意成本跟踪](/langsmith/cost-tracking#tracking-costs-for-arbitrary-runs)）。
- 追踪中必须存在[令牌使用数据](/langsmith/log-llm-trace#provide-token-and-cost-information)（prompt_tokens, completion_tokens）。
- 模型必须存在于定价数据库中或已[配置自定义定价](/langsmith/cost-tracking#set-up-model-pricing)。

**回退行为:**
如果元数据中没有 [`ls_model_name`](#ls-model-name)，系统会在放弃成本跟踪之前检查 [`ls_invocation_params`](#ls-invocation-params) 中是否有像 `"model"` 这样的模型标识符。

### 配置跟踪组

这些参数帮助您跟踪模型设置，但不影响 LangSmith 的核心功能：

**可选，独立工作:** [`ls_temperature`](#ls-temperature), [`ls_max_tokens`](#ls-max-tokens), [`ls_stop`](#ls-stop)
- 这些用于跟踪/显示。
- 不影响 LangSmith 行为或成本计算。
- 对实验比较和调试很有用。

### 调用参数特殊情况

`ls_invocation_params` 参数具有双重角色，既是跟踪字段又是回退机制：

**[`ls_invocation_params`](#ls-invocation-params)**; 部分独立，具有回退角色：
- 主要存储用于跟踪的任意配置。
- **如果缺少 [`ls_model_name`](#ls-model-name)，可以作为成本跟踪的回退**。
- 当 [`ls_model_name`](#ls-model-name) 存在时，不直接影响成本计算。

### 系统参数

这些参数由 LangSmith 自动生成，无法手动设置：

**用户无法设置:** [`ls_run_depth`](#ls-run-depth), [`ls_method`](#ls-method), [`ls_example_*`](#ls-example-), [`ls_experiment_id`](#ls-experiment-id)
- 由系统自动设置。
- 用于过滤、分析和系统跟踪。

## 按元数据参数过滤追踪

一旦您将 `ls_` 元数据参数添加到追踪中，您就可以通过 [API](https://api.smith.langchain.com/redoc#tag/run/operation/query_runs_api_v1_runs_query_post) 以编程方式或在 [LangSmith UI](https://smith.langchain.com) 中交互式地使用它们来过滤和搜索追踪。这使您可以按模型、供应商、配置设置或追踪深度缩小追踪范围。

### 使用 API

使用 [`Client`](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client) 类及其 [`list_runs()`](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.list_runs) 方法（Python）或 [`listRuns()`](https://docs.smith.langchain.com/reference/js/classes/client.Client#listruns) 方法（TypeScript）来根据元数据值查询追踪。[过滤语法](/langsmith/trace-query-syntax) 支持相等性检查、比较和逻辑运算符。

<CodeGroup>

```python Python
from langsmith import Client

client = Client()

# 按供应商过滤运行
runs = client.list_runs(
    project_name="my-app",
    filter='metadata_key = "ls_provider" AND metadata_value = "openai"'
)

# 按特定模型过滤
runs = client.list_runs(
    project_name="my-app",
    filter='metadata_key = "ls_model_name" AND metadata_value = "gpt-4o"'
)

# 仅过滤根运行（顶层追踪）
runs = client.list_runs(
    project_name="my-app",
    filter='metadata_key = "ls_run_depth" AND metadata_value = 0'
)

# 按温度阈值过滤
runs = client.list_runs(
    project_name="my-app",
    filter='metadata_key = "ls_temperature" AND metadata_value > 0.5'
)
```

```typescript TypeScript
import { Client } from "langsmith";

const client = new Client();

// 按供应商过滤运行
const runsByProvider
