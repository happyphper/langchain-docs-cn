---
title: 智能体中的上下文工程
sidebarTitle: Context engineering
---
## 概述

构建智能体（或任何 LLM 应用）的难点在于使其足够可靠。虽然它们可能在原型阶段工作，但在实际使用场景中常常失败。

### 智能体为何失败？

当智能体失败时，通常是因为其内部的 LLM 调用采取了错误的行动/没有达到我们的预期。LLM 失败的原因通常有两个：

1.  底层 LLM 能力不足
2.  没有将“正确的”上下文传递给 LLM

更多时候，导致智能体不可靠的实际上是第二个原因。

**上下文工程** 是指以正确的格式提供正确的信息和工具，以便 LLM 能够完成任务。这是 AI 工程师的首要工作。缺乏“正确的”上下文是构建更可靠智能体的首要障碍，而 LangChain 的智能体抽象设计独特，旨在促进上下文工程。

<Tip>
不熟悉上下文工程？请从 [概念概述](/oss/concepts/context) 开始，了解不同类型的上下文及其使用时机。
</Tip>

### 智能体循环

典型的智能体循环包含两个主要步骤：

1.  **模型调用** - 使用提示词和可用工具调用 LLM，返回响应或执行工具的请求
2.  **工具执行** - 执行 LLM 请求的工具，返回工具结果

<div style={{ display: "flex", justifyContent: "center" }}>
  <img
    src="/oss/images/core_agent_loop.png"
    alt="核心智能体循环示意图"
    className="rounded-lg"
  />
</div>

此循环持续进行，直到 LLM 决定结束。

### 你可以控制什么

为了构建可靠的智能体，你需要控制智能体循环每一步发生的事情，以及步骤之间发生的事情。

| 上下文类型 | 你可以控制的内容 | 瞬态或持久 |
|--------------|------------------|-------------------------|
| **[模型上下文](#model-context)** | 进入模型调用的内容（指令、消息历史、工具、响应格式） | 瞬态 |
| **[工具上下文](#tool-context)** | 工具可以访问和产生的内容（对状态、存储、运行时上下文的读写） | 持久 |
| **[生命周期上下文](#life-cycle-context)** | 模型调用和工具调用之间发生的事情（摘要、护栏、日志记录等） | 持久 |

<CardGroup>
  <Card title="瞬态上下文" icon="bolt" iconType="duotone">
    LLM 在单次调用中看到的内容。你可以修改消息、工具或提示词，而无需更改保存在状态中的内容。
  </Card>
  <Card title="持久上下文" icon="database" iconType="duotone">
    在多轮对话中保存在状态中的内容。生命周期钩子和工具写入会永久修改此内容。
  </Card>
</CardGroup>

### 数据源

在整个过程中，你的智能体会访问（读取/写入）不同的数据源：

| 数据源 | 亦称 | 作用域 | 示例 |
|-------------|---------------|-------|----------|
| **运行时上下文** | 静态配置 | 会话作用域 | 用户 ID、API 密钥、数据库连接、权限、环境设置 |
| **状态** | 短期记忆 | 会话作用域 | 当前消息、上传的文件、认证状态、工具结果 |
| **存储** | 长期记忆 | 跨会话 | 用户偏好、提取的见解、记忆、历史数据 |

### 工作原理

LangChain 的 [中间件](/oss/langchain/middleware) 是底层机制，它使得使用 LangChain 的开发者能够实际进行上下文工程。

中间件允许你钩入智能体生命周期的任何步骤，并：

*   更新上下文
*   跳转到智能体生命周期中的不同步骤

在本指南中，你将频繁看到使用中间件 API 作为实现上下文工程目标的手段。

## 模型上下文

控制每次模型调用的输入内容——指令、可用工具、使用哪个模型以及输出格式。这些决策直接影响可靠性和成本。

<CardGroup cols={2}>
    <Card title="系统提示词" icon="message-lines" href="#system-prompt">
        开发者提供给 LLM 的基础指令。
    </Card>
    <Card title="消息" icon="comments" href="#messages">
        发送给 LLM 的完整消息列表（对话历史）。
    </Card>
    <Card title="工具" icon="wrench" href="#tools">
        智能体可以访问以执行操作的实用程序。
    </Card>
    <Card title="模型" icon="brain-circuit" href="#model">
        要调用的实际模型（包括配置）。
    </Card>
    <Card title="响应格式" icon="brackets-curly" href="#response-format">
        模型最终响应的模式规范。
    </Card>
</CardGroup>

所有这些类型的模型上下文都可以从 **状态**（短期记忆）、**存储**（长期记忆）或 **运行时上下文**（静态配置）中获取。

### 系统提示词

系统提示词设定了 LLM 的行为和能力。不同的用户、上下文或对话阶段需要不同的指令。成功的智能体会利用记忆、偏好和配置，为对话的当前状态提供正确的指令。

<Tabs>
  <Tab title="状态">
    从状态中访问消息计数或对话上下文：

    :::python

    ```python
    from langchain.agents import create_agent
    from langchain.agents.middleware import dynamic_prompt, ModelRequest

    @dynamic_prompt
    def state_aware_prompt(request: ModelRequest) -> str:
        # request.messages 是 request.state["messages"] 的快捷方式
        message_count = len(request.messages)

        base = "You are a helpful assistant."

        if message_count > 10:
            base += "\nThis is a long conversation - be extra concise."

        return base

    agent = create_agent(
        model="gpt-4o",
        tools=[...],
        middleware=[state_aware_prompt]
    )
    ```
    :::

    :::js
    ```typescript
    import { createAgent } from "langchain";

    const agent = createAgent({
      model: "gpt-4o",
      tools: [...],
      middleware: [
        dynamicSystemPromptMiddleware((state) => {
          // 从状态读取：检查对话长度
          const messageCount = state.messages.length;

          let base = "You are a helpful assistant.";

          if (messageCount > 10) {
            base += "\nThis is a long conversation - be extra concise.";
          }

          return base;
        }),
      ],
    });
    ```
    :::
  </Tab>

  <Tab title="存储">
    从长期记忆中访问用户偏好：

    :::python

    ```python
    from dataclasses import dataclass
    from langchain.agents import create_agent
    from langchain.agents.middleware import dynamic_prompt, ModelRequest
    from langgraph.store.memory import InMemoryStore

    @dataclass
    class Context:
        user_id: str

    @dynamic_prompt
    def store_aware_prompt(request: ModelRequest) -> str:
        user_id = request.runtime.context.user_id

        # 从存储读取：获取用户偏好
        store = request.runtime.store
        user_prefs = store.get(("preferences",), user_id)

        base = "You are a helpful assistant."

        if user_prefs:
            style = user_prefs.value.get("communication_style", "balanced")
            base += f"\nUser prefers {style} responses."

        return base

    agent = create_agent(
        model="gpt-4o",
        tools=[...],
        middleware=[store_aware_prompt],
        context_schema=Context,
        store=InMemoryStore()
    )
    ```
    :::

    :::js
    ```typescript
    import * as z from "zod";
    import { createAgent, dynamicSystemPromptMiddleware } from "langchain";

    const contextSchema = z.object({
      userId: z.string(),
    });

    type Context = z.infer<typeof contextSchema>;

    const agent = createAgent({
      model: "gpt-4o",
      tools: [...],
      contextSchema,
      middleware: [
        dynamicSystemPromptMiddleware<Context>(async (state, runtime) => {
          const userId = runtime.context.userId;

          // 从存储读取：获取用户偏好
          const store = runtime.store;
          const userPrefs = await store.get(["preferences"], userId);

          let base = "You are a helpful assistant.";

          if (userPrefs) {
            const style = userPrefs.value?.communicationStyle || "balanced";
            base += `\nUser prefers ${style} responses.`;
          }

          return base;
        }),
      ],
    });
    ```
    :::
  </Tab>

  <Tab title="运行时上下文">
    从运行时上下文中访问用户 ID 或配置：

    :::python

    ```python
    from dataclasses import dataclass
    from langchain.agents import create_agent
    from langchain.agents.middleware import dynamic_prompt, ModelRequest

    @dataclass
    class Context:
        user_role: str
        deployment_env: str

    @dynamic_prompt
    def context_aware_prompt(request: ModelRequest) -> str:
        # 从运行时上下文读取：用户角色和环境
        user_role = request.runtime.context.user_role
        env = request.runtime.context.deployment_env

        base = "You are a helpful assistant."

        if user_role == "admin":
            base += "\nYou have admin access. You can perform all operations."
        elif user_role == "viewer":
            base += "\nYou have read-only access. Guide users to read operations only."

        if env == "production":
            base += "\nBe extra careful with any data modifications."

        return base

    agent = create_agent(
        model="gpt-4o",
        tools=[...],
        middleware=[context_aware_prompt],
        context_schema=Context
    )
    ```
    :::

    :::js
    ```typescript
    import * as z from "zod";
    import { createAgent, dynamicSystemPromptMiddleware } from "langchain";

    const contextSchema = z.object({
      userRole: z.string(),
      deploymentEnv: z.string(),
    });

    type Context = z.infer<typeof contextSchema>;

    const agent = createAgent({
      model: "gpt-4o",
      tools: [...],
      contextSchema,
      middleware: [
        dynamicSystemPromptMiddleware<Context>((state, runtime) => {
          // 从运行时上下文读取：用户角色和环境
          const userRole = runtime.context.userRole;
          const env = runtime.context.deploymentEnv;

          let base = "You are a helpful assistant.";

          if (userRole === "admin") {
            base += "\nYou have admin access. You can perform all operations.";
          } else if (userRole === "viewer") {
            base += "\nYou have read-only access. Guide users to read operations only.";
          }

          if (env === "production") {
            base += "\nBe extra careful with any data modifications.";
          }

          return base;
        }),
      ],
    });
    ```
    :::
  </Tab>

</Tabs>

### 消息

消息构成了发送给 LLM 的提示词。管理消息内容至关重要，以确保 LLM 拥有正确的信息来做出良好回应。

<Tabs>
  <Tab title="状态">
    当与当前查询相关时，从状态中注入上传的文件上下文：

    :::python

    ```python
    from langchain.agents import create_agent
    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse
    from typing import Callable

    @wrap_model_call
    def inject_file_context(
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse]
    ) -> ModelResponse:
        """注入用户在此会话中上传的文件的相关上下文。"""
        # 从状态读取：获取上传的文件元数据
        uploaded_files = request.state.get("uploaded_files", [])  # [!code highlight]

        if uploaded_files:
            # 构建关于可用文件的上下文
            file_descriptions = []
            for file in uploaded_files:
                file_descriptions.append(
                    f"- {file['name']} ({file['type']}): {file['summary']}"
                )

            file_context = f"""Files you have access to in this conversation:
{chr(10).join(file_descriptions)}

Reference these files when answering questions."""

            # 在最近的消息之前注入文件上下文
            messages = [  # [!code highlight]
                *request.messages,
                {"role": "user", "content": file_context},
            ]
            request = request.override(messages=messages)  # [!code highlight]

        return handler(request)

    agent = create_agent(
        model="gpt-4o",
        tools=[...],
        middleware=[inject_file_context]
    )
    ```
    :::

    :::js
    ```typescript
    import { createMiddleware } from "langchain";

    const injectFileContext = createMiddleware({
      name: "InjectFileContext",
      wrapModelCall: (request, handler) => {
        // request.state 是 request.state.messages 的快捷方式
        const uploadedFiles = request.state.uploadedFiles || [];  // [!code highlight]

        if (uploadedFiles.length > 0) {
          // 构建关于可用文件的上下文
          const fileDescriptions = uploadedFiles.map(file =>
            `- ${file.name} (${file.type}): ${file.summary}`
          );

          const fileContext = `Files you have access to in this conversation:
${fileDescriptions.join("\n")}

Reference these files when answering questions.`;

          // 在最近的消息之前注入文件上下文
          const messages = [  // [!code highlight]
            ...request.messages,  // 对话的其余部分
            { role: "user", content: fileContext }
          ];
          request = request.override({ messages });  // [!code highlight]
        }

        return handler(request);
      },
    });

    const agent = createAgent({
      model: "gpt-4o",
      tools: [...],
      middleware: [injectFileContext],
    });
    ```
    :::
  </Tab>

  <Tab title="存储">
    从存储中注入用户的电子邮件写作风格以指导起草：

    :::python

    ```python
    from dataclasses import dataclass
    from langchain.agents import create_agent
    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse
    from typing import Callable
    from langgraph.store.memory import InMemoryStore

    @dataclass
    class Context:
        user_id: str

    @wrap_model_call
    def inject_writing_style(
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse]
    ) -> ModelResponse:
        """从存储中注入用户的电子邮件写作风格。"""
        user_id = request.runtime.context.user_id  # [!code highlight]

        # 从存储读取：获取用户的写作风格示例
        store = request.runtime.store  # [!code highlight]
        writing_style = store.get(("writing_style",), user_id)  # [!code highlight]

        if writing_style:
            style = writing_style.value
            # 根据存储的示例构建风格指南
            style_context = f"""Your writing style:
- Tone: {style.get('tone', 'professional')}
- Typical greeting: "{style.get('greeting', 'Hi')}"
- Typical sign-off: "{style.get('sign_off', 'Best')}"
- Example email you've written:
{style.get('example_email', '')}"""

            # 在末尾追加 - 模型更关注最后的消息
            messages = [
                *request.messages,
                {"role": "user", "content": style_context}
            ]
            request = request.override(messages=messages)  # [!code highlight]

        return handler(request)

    agent = create_agent(
        model="gpt-4o",
        tools=[...],
        middleware=[inject_writing_style],
        context_schema=Context,
        store=InMemoryStore()
    )
    ```
    :::

    :::js
    ```typescript
    import * as z from "zod";
    import { createMiddleware } from "langchain";

    const contextSchema = z.object({
      userId: z.string(),
    });

    const injectWritingStyle = createMiddleware({
      name: "InjectWritingStyle",
      contextSchema,
      wrapModelCall: async (request, handler) => {
        const userId = request.runtime.context.userId;  // [!code highlight]

        // 从存储读取：获取用户的写作风格示例
        const store = request.runtime.store;  // [!code highlight]
        const writingStyle = await store.get(["writing_style"], userId);  // [!code highlight]

        if (writingStyle) {
          const style = writingStyle.value;
          // 根据存储的示例构建风格指南
          const styleContext = `Your writing style:
- Tone: ${style.tone || 'professional'}
- Typical greeting: "${style.greeting || 'Hi'}"
- Typical sign-off: "${style.signOff || 'Best'}"
- Example email you've written:
${style.exampleEmail || ''}`;

          // 在末尾追加 - 模型更关注最后的消息
          const messages = [
            ...request.messages,
            { role: "user", content: styleContext }
          ];
          request = request.override({ messages });  // [!code highlight]
        }

        return handler(request);
      },
    });
    ```
    :::
  </Tab>

  <Tab title="运行时上下文">
    根据用户的管辖区域，从运行时上下文中注入合规规则：

    :::python

    ```python
    from dataclasses import dataclass
    from langchain.agents import create_agent
    from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse
    from typing import Callable

    @dataclass
    class Context:
        user_jurisdiction: str
        industry: str
        compliance_frameworks: list[str]

    @wrap_model_call
    def inject_compliance_rules(
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse]
    ) -> ModelResponse:
        """从运行时上下文中注入合规约束。"""
        # 从运行时上下文读取：获取合规要求
        jurisdiction = request.runtime.context.user_jurisdiction  # [!code highlight]
        industry = request.runtime.context.industry  # [!code highlight]
        frameworks = request.runtime.context.compliance_frameworks  # [!code highlight]

        # 构建合规约束
        rules = []
        if "GDPR" in frameworks:
            rules.append("- Must obtain explicit consent before processing personal data")
            rules.append("- Users have right to data deletion")
        if "HIPAA"
