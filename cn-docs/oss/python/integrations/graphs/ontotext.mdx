---
title: Ontotext GraphDB
---
>[Ontotext GraphDB](https://graphdb.ontotext.com/) 是一个符合 [RDF](https://www.w3.org/RDF/) 和 [SPARQL](https://www.w3.org/TR/sparql11-query/) 标准的图数据库和知识发现工具。

> 本笔记本展示了如何使用 LLM 为 `Ontotext GraphDB` 提供自然语言查询功能（NLQ 转 SPARQL，也称为 `text2sparql`）。

## GraphDB LLM 功能

`GraphDB` 支持一些 LLM 集成功能，如[此处](https://github.com/w3c/sparql-dev/issues/193)所述：

[gpt-queries](https://graphdb.ontotext.com/documentation/10.5/gpt-queries.html)

*   使用来自知识图谱（KG）的数据，通过魔法谓词向 LLM 请求文本、列表或表格
*   查询解释
*   结果解释、总结、重述、翻译

[retrieval-graphdb-connector](https://graphdb.ontotext.com/documentation/10.5/retrieval-graphdb-connector.html)

*   在向量数据库中索引 KG 实体
*   支持任何文本嵌入算法和向量数据库
*   使用与 GraphDB 用于 Elastic、Solr、Lucene 相同的强大连接器（索引）语言
*   RDF 数据更改自动同步到 KG 实体索引
*   支持嵌套对象（GraphDB 10.5 版本无 UI 支持）
*   将 KG 实体序列化为文本，例如（针对葡萄酒数据集）：

```
Franvino:
- is a RedWine.
- made from grape Merlo.
- made from grape Cabernet Franc.
- has sugar dry.
- has year 2012.
```

[talk-to-graph](https://graphdb.ontotext.com/documentation/10.5/talk-to-graph.html)

*   使用已定义的 KG 实体索引的简单聊天机器人

在本教程中，我们不会使用 GraphDB 的 LLM 集成功能，而是使用从 NLQ 生成 `SPARQL` 的功能。我们将使用 `Star Wars API` (`SWAPI`) 本体和数据集，您可以在[此处](https://github.com/Ontotext-AD/langchain-graphdb-qa-chain-demo/blob/main/starwars-data.trig)查看。

## 环境设置

您需要一个正在运行的 GraphDB 实例。本教程展示了如何使用 [GraphDB Docker 镜像](https://hub.docker.com/r/ontotext/graphdb)在本地运行数据库。它提供了一个 docker compose 设置，该设置会将 Star Wars 数据集加载到 GraphDB 中。所有必要的文件（包括此笔记本）都可以从 [GitHub 仓库 langchain-graphdb-qa-chain-demo](https://github.com/Ontotext-AD/langchain-graphdb-qa-chain-demo) 下载。

*   安装 [Docker](https://docs.docker.com/get-docker/)。本教程使用 Docker 版本 `24.0.7` 创建，该版本捆绑了 [Docker Compose](https://docs.docker.com/compose/)。对于更早的 Docker 版本，您可能需要单独安装 Docker Compose。
*   将 [GitHub 仓库 langchain-graphdb-qa-chain-demo](https://github.com/Ontotext-AD/langchain-graphdb-qa-chain-demo) 克隆到您机器上的本地文件夹中。
*   从同一文件夹执行以下脚本以启动 GraphDB

```
docker build --tag graphdb .
docker compose up -d graphdb
```

  您需要等待几秒钟，数据库将在 `http://localhost:7200/` 上启动。Star Wars 数据集 `starwars-data.trig` 会自动加载到 `langchain` 存储库中。本地 SPARQL 端点 `http://localhost:7200/repositories/langchain` 可用于运行查询。您也可以从您喜欢的网页浏览器打开 GraphDB Workbench `http://localhost:7200/sparql`，在那里可以交互式地进行查询。

*   设置工作环境

如果您使用 `conda`，请创建并激活一个新的 conda 环境，例如：

```
conda create -n graph_ontotext_graphdb_qa python=3.12
conda activate graph_ontotext_graphdb_qa
```

安装以下库：

```
pip install jupyter==1.1.1
pip install rdflib==7.1.1
pip install langchain-community==0.3.4
pip install langchain-openai==0.2.4
```

运行 Jupyter：

```
jupyter notebook
```

## 指定本体

为了使 LLM 能够生成 SPARQL，它需要知道知识图谱的模式（本体）。可以通过 `OntotextGraphDBGraph` 类的以下两个参数之一来提供：

*   `query_ontology`：一个在 SPARQL 端点上执行的 `CONSTRUCT` 查询，用于返回 KG 模式语句。我们建议您将本体存储在其自己的命名图中，这将更容易仅获取相关语句（如下例所示）。不支持 `DESCRIBE` 查询，因为 `DESCRIBE` 返回对称简洁有界描述（SCBD），即也包括传入的类链接。对于拥有数百万实例的大型图来说，这效率不高。请查看 [github.com/eclipse-rdf4j/rdf4j/issues/4857](https://github.com/eclipse-rdf4j/rdf4j/issues/4857)
*   `local_file`：一个本地的 RDF 本体文件。支持的 RDF 格式有 `Turtle`、`RDF/XML`、`JSON-LD`、`N-Triples`、`Notation-3`、`Trig`、`Trix`、`N-Quads`。

无论哪种情况，本体转储都应：

*   包含关于类、属性、属性与类的关联（使用 rdfs:domain、schema:domainIncludes 或 OWL 限制）以及分类法（重要的个体）的足够信息。
*   不包含过于冗长且不相关的定义和示例，这些对 SPARQL 构造没有帮助。

```python
from langchain_community.graphs import OntotextGraphDBGraph

# 使用用户构造查询来提供模式

graph = OntotextGraphDBGraph(
    query_endpoint="http://localhost:7200/repositories/langchain",
    query_ontology="CONSTRUCT {?s ?p ?o} FROM [swapi.co/ontology/](https://swapi.co/ontology/) WHERE {?s ?p ?o}",
)
```

```python
# 使用本地 RDF 文件来提供模式

graph = OntotextGraphDBGraph(
    query_endpoint="http://localhost:7200/repositories/langchain",
    local_file="/path/to/langchain_graphdb_tutorial/starwars-ontology.nt",  # 请更改此处的路径
)
```

无论哪种方式，本体（模式）都以 `Turtle` 格式提供给 LLM，因为带有适当前缀的 `Turtle` 格式最紧凑，也最容易让 LLM 记住。

Star Wars 本体有点不寻常，因为它包含很多关于类的具体三元组，例如物种 `:Aleena` 生活在 `<planet/38>` 上，它们是 `:Reptile` 的子类，具有某些典型特征（平均身高、平均寿命、皮肤颜色），并且特定的个体（角色）是该类的代表：

```
@prefix : [swapi.co/vocabulary/](https://swapi.co/vocabulary/) .
@prefix owl: [www.w3.org/2002/07/owl#](http://www.w3.org/2002/07/owl#) .
@prefix rdfs: [www.w3.org/2000/01/rdf-schema#](http://www.w3.org/2000/01/rdf-schema#) .
@prefix xsd: [www.w3.org/2001/XMLSchema#](http://www.w3.org/2001/XMLSchema#) .

:Aleena a owl:Class, :Species ;
    rdfs:label "Aleena" ;
    rdfs:isDefinedBy [swapi.co/ontology/](https://swapi.co/ontology/) ;
    rdfs:subClassOf :Reptile, :Sentient ;
    :averageHeight 80.0 ;
    :averageLifespan "79" ;
    :character [swapi.co/resource/aleena/47](https://swapi.co/resource/aleena/47) ;
    :film [swapi.co/resource/film/4](https://swapi.co/resource/film/4) ;
    :language "Aleena" ;
    :planet [swapi.co/resource/planet/38](https://swapi.co/resource/planet/38) ;
    :skinColor "blue", "gray" .

    ...

 ```

为了简化本教程，我们使用未加密的 GraphDB。如果 GraphDB 已加密，您应该在初始化 `OntotextGraphDBGraph` 之前设置环境变量 'GRAPHDB_USERNAME' 和 'GRAPHDB_PASSWORD'。

```python
os.environ["GRAPHDB_USERNAME"] = "graphdb-user"
os.environ["GRAPHDB_PASSWORD"] = "graphdb-password"

graph = OntotextGraphDBGraph(
    query_endpoint=...,
    query_ontology=...
)
```

## 针对 StarWars 数据集的问答

现在我们可以使用 `OntotextGraphDBQAChain` 来提问了。

```python
import os

from langchain_classic.chains import OntotextGraphDBQAChain
from langchain_openai import ChatOpenAI

# 我们将使用一个需要 OpenAI API 密钥的 OpenAI 模型。
# 但是，也可以使用其他模型：
# https://python.langchain.com/docs/integrations/chat/

# 将环境变量 `OPENAI_API_KEY` 设置为您的 OpenAI API 密钥
os.environ["OPENAI_API_KEY"] = "sk-***"

# 这里可以使用任何可用的 OpenAI 模型。
# 我们使用 'gpt-4-1106-preview' 是因为其上下文窗口更大。
# 'gpt-4-1106-preview' 这个 model_name 将来会被弃用，并更改为 'gpt-4-turbo' 或类似名称，
# 因此请务必查阅 OpenAI API https://platform.openai.com/docs/models 以获取正确的命名。

chain = OntotextGraphDBQAChain.from_llm(
    ChatOpenAI(temperature=0, model_name="gpt-4-1106-preview"),
    graph=graph,
    verbose=True,
    allow_dangerous_requests=True,
)
```

让我们问一个简单的问题。

```python
chain.invoke({chain.input_key: "What is the climate on Tatooine?"})[chain.output_key]
```

```text


> Entering new OntotextGraphDBQAChain chain...
Generated SPARQL:
PREFIX : [swapi.co/vocabulary/](https://swapi.co/vocabulary/)
PREFIX rdfs: [www.w3.org/2000/01/rdf-schema#](http://www.w3.org/2000/01/rdf-schema#)

SELECT ?climate
WHERE {
  ?planet rdfs:label "Tatooine" ;
          :climate ?climate .
}

> Finished chain.
```

```text
'The climate on Tatooine is arid.'
```

再来一个稍微复杂点的问题。

```python
chain.invoke({chain.input_key: "What is the climate on Luke Skywalker's home planet?"})[
    chain.output_key
]
```

```text
> Entering new OntotextGraphDBQAChain chain...
Generated SPARQL:
PREFIX : [swapi.co/vocabulary/](https://swapi.co/vocabulary/)
PREFIX owl: [www.w3.org/2002/07/owl#](http://www.w3.org/2002/07/owl#)
PREFIX rdfs: [www.w3.org/2000/01/rdf-schema#](http://www.w3.org/2000/01/rdf-schema#)
PREFIX xsd: [www.w3.org/2001/XMLSchema#](http://www.w3.org/2001/XMLSchema#)

SELECT ?climate
WHERE {
  ?character rdfs:label "Luke Skywalker" .
  ?character :homeworld ?planet .
  ?planet :climate ?climate .
}

> Finished chain.
```

```text
"The climate on Luke Skywalker's home planet is arid."
```

我们还可以问更复杂的问题，比如

```python
chain.invoke(
    {
        chain.input_key: "What is the average box office revenue for all the Star Wars movies?"
    }
)[chain.output_key]
```

```text
> Entering new OntotextGraphDBQAChain chain...
Generated SPARQL:
PREFIX : [swapi.co/vocabulary/](https://swapi.co/vocabulary/)
PREFIX xsd: [www.w3.org/2001/XMLSchema#](http://www.w3.org/2001/XMLSchema#)

SELECT (AVG(?boxOffice) AS ?averageBoxOfficeRevenue)
WHERE {
  ?film a :Film .
  ?film :boxOffice ?boxOfficeValue .
  BIND(xsd:decimal(?boxOfficeValue) AS ?boxOffice)
}


> Finished chain.
```

```text
'The average box office revenue for all the Star Wars movies is approximately 754.1 million dollars.'
```

## 链修饰器

Ontotext GraphDB QA 链允许进行提示词优化，以进一步改进您的 QA 链并增强应用程序的整体用户体验。

### "SPARQL 生成" 提示

该提示用于基于用户问题和 KG 模式生成 SPARQL 查询。

*   `sparql_generation_prompt`

    默认值：

  ```python
    GRAPHDB_SPARQL_GENERATION_TEMPLATE = """
    Write a SPARQL SELECT query for querying a graph database.
    The ontology schema delimited by triple backticks in Turtle format is:
    ```
    {schema}
    ```
    Use only the classes and properties provided in the schema to construct the SPARQL query.
    Do not use any classes or properties that are not explicitly provided in the SPARQL query.
    Include all necessary prefixes.
    Do not include any explanations or apologies in your responses.
    Do not wrap the query in backticks.
    Do not include any text except the SPARQL query generated.
    The question delimited by triple backticks is:
    ```
    {prompt}
    ```
    """
    GRAPHDB_SPARQL_GENERATION_PROMPT = PromptTemplate(
        input_variables=["schema", "prompt"],
        template=GRAPHDB_SPARQL_GENERATION_TEMPLATE,
    )
  ```

### "SPARQL 修正" 提示

有时，LLM 可能会生成带有语法错误或缺少前缀等的 SPARQL 查询。该链将通过提示 LLM 在一定次数内进行修正来尝试修正此问题。

*   `sparql_fix_prompt`

    默认值：

  ```python
    GRAPHDB_SPARQL_FIX_TEMPLATE = """
    This following SPARQL query delimited by triple backticks
    ```
    {generated_sparql}
    ```
    is not valid.
    The error delimited by triple backticks is
    ```
    {error_message}
    ```
    Give me a correct version of the SPARQL query.
    Do not change the logic of the query.
    Do not include any explanations or apologies in your responses.
    Do not wrap the query in backticks.
    Do not include any text except the SPARQL query generated.
    The ontology schema delimited by triple backticks in Turtle format is:
    ```
    {schema}
    ```
    """

    GRAPHDB_SPARQL_FIX_PROMPT = PromptTemplate(
        input_variables=["error_message", "generated_sparql", "schema"],
        template=GRAPHDB_SPARQL_FIX_TEMPLATE,
    )
  ```

*   `max_fix_retries`

    默认值：`5`

### "回答" 提示

该提示用于根据从数据库返回的结果和初始用户问题来回答问题。默认情况下，指示 LLM 仅使用返回结果中的信息。如果结果集为空，LLM 应告知无法回答问题。

*   `qa_prompt`

  默认值：

  ```python
    GRAPHDB_QA_TEMPLATE = """Task: Generate a natural language response from the results of a SPARQL query.
    You are an assistant that creates well-written and human understandable answers.
    The information part contains the information provided, which you can use to construct an answer.
    The information provided is authoritative, you must never doubt it or try to use your internal knowledge to correct it.
    Make your response sound like the information is coming from an AI assistant, but don't add any information.
    Don't use internal knowledge to answer the question, just say you don't know if no information is available.
    Information:
    {context}

    Question: {prompt}
    Helpful Answer:"""
    GRAPHDB_QA_PROMPT = PromptTemplate(
        input_variables=["context", "prompt"], template=GRAPHDB_QA_TEMPLATE
    )
  ```

完成 GraphDB 的 QA 操作后，您可以通过在包含 Docker compose 文件的目录中运行
``
docker compose down -v --remove-orphans
``
来关闭 Docker 环境。
