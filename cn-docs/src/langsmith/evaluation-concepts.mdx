---
title: 评估概念
sidebarTitle: Concepts
---
LLM 的输出具有非确定性，这使得评估响应质量变得困难。评估（evals）是一种分解“好”的标准并对其进行衡量的方法。LangSmith 评估提供了一个框架，用于在整个应用程序生命周期中衡量质量，从部署前测试到生产监控。

## 评估什么

在构建评估之前，先确定对您的应用程序而言什么才是重要的。将您的系统分解为其关键组件——LLM 调用、检索步骤、工具调用、输出格式化——并为每个组件确定质量标准。

**从手动整理的示例开始。** 为每个关键组件创建 5-10 个“好”的示例。这些示例作为您的基本事实，并指导您使用哪种评估方法。例如：
- **RAG 系统**：良好检索（相关文档）和良好答案（准确、完整）的示例。
- **智能体**：正确选择工具、参数格式化正确或智能体所采取轨迹的示例。
- **聊天机器人**：有帮助、符合品牌定位、满足用户意图的响应示例。

一旦通过示例定义了“好”，您就可以衡量您的系统产生类似质量输出的频率。

## 离线和在线评估

LangSmith 支持两种类型的评估，它们在您的开发工作流程中服务于不同的目的：

### 离线评估

<Icon icon="flask" iconType="solid" /> 将离线评估用于**部署前测试**：
- **基准测试**：比较多个版本以找到性能最佳者。
- **回归测试**：确保新版本不会降低质量。
- **单元测试**：验证单个组件的正确性。
- **回测**：针对历史数据测试新版本。

离线评估针对来自[_数据集_](#datasets)的[_示例_](#examples)——这些是带有参考输出的精选测试用例，定义了“好”的标准。

### 在线评估

<Icon icon="radar" iconType="solid" /> 将在线评估用于**生产监控**：
- **实时监控**：在实时流量上持续跟踪质量。
- **异常检测**：标记异常模式或边缘情况。
- **生产反馈**：识别问题以添加到离线数据集中。

在线评估针对来自[追踪](/langsmith/observability-quickstart)的[_运行_](#runs)和[_线程_](#threads)——这些是没有参考输出的真实生产追踪记录。

目标的不同决定了您可以评估的内容：离线评估可以根据预期答案检查正确性，而在线评估则侧重于质量模式、安全性和实际行为。

## 评估生命周期

随着您开发和[部署应用程序](/langsmith/deployments)，您的评估策略从部署前测试演进到生产监控。在开发和测试期间，离线评估根据精选数据集验证功能。部署后，在线评估监控生产环境在实时流量上的行为。随着应用程序的成熟，两种评估类型在一个迭代反馈循环中协同工作，以持续改进质量。

```mermaid
graph LR
    A[开发] --> B[测试]
    B --> C[部署]
    C --> D[监控]
    D --> E[迭代]

    A -.-> F[离线]
    B -.-> F
    C -.-> G[在线]
    D -.-> G
    E -.-> H[两者]

    style F fill:#8b5cf6,stroke:#7c3aed,color:#fff
    style G fill:#8b5cf6,stroke:#7c3aed,color:#fff
    style H fill:#8b5cf6,stroke:#7c3aed,color:#fff
```

### 1. 结合离线评估进行开发

在生产部署之前，使用离线评估来验证功能、对不同方法进行基准测试并建立信心。

按照[快速入门](/langsmith/evaluation-quickstart)运行您的第一个离线评估。

### 2. 结合在线评估进行初始部署

部署后，使用在线评估来监控生产质量、检测意外问题并收集真实世界数据。

了解如何为生产监控[配置在线评估](/langsmith/online-evaluations)。

### 3. 持续改进

在迭代反馈循环中同时使用两种评估类型。在线评估发现的问题会成为离线测试用例，离线评估验证修复，而在线评估则确认生产环境的改进。

## 核心评估目标

根据是离线还是在线，评估运行在不同的目标上。

### 离线评估的目标

离线评估在数据集和示例上运行。参考输出的存在使得可以比较预期结果和实际结果。

#### 数据集

数据集是用于评估应用程序的_示例集合_。一个示例是一个测试输入、参考输出对。

![数据集](/langsmith/images/dataset-concept.png)

#### 示例

每个示例包含：

- **输入**：传递给应用程序的输入变量字典。
- **参考输出**（可选）：参考输出字典。这些不会传递给您的应用程序，它们仅用于评估器。
- **元数据**（可选）：附加信息字典，可用于创建数据集的筛选视图。

![示例](/langsmith/images/example-concept.png)

了解更多关于[管理数据集](/langsmith/manage-datasets)的信息。

#### 实验

_实验_ 表示在数据集上评估特定应用程序版本的结果。每个实验捕获数据集中每个示例的输出、评估器分数和执行追踪。

![实验视图](/langsmith/images/experiment-view.png)

通常会在给定数据集上运行多个实验，以测试不同的应用程序配置（例如，不同的提示词或 LLM）。LangSmith 显示与数据集关联的所有实验，并支持[并排比较多个实验](/langsmith/compare-experiment-results)。

![比较视图](/langsmith/images/comparison-view.png)

学习[如何分析实验结果](/langsmith/analyze-an-experiment)。

### 在线评估的目标

在线评估在生产流量的运行和线程上运行。由于没有参考输出，评估器侧重于实时检测问题、异常和质量下降。

#### 运行

_运行_ 是来自您[已部署应用程序](/langsmith/deployments)的单个执行追踪。每个运行包含：
- **输入**：您的应用程序接收到的实际用户输入。
- **输出**：您的应用程序实际返回的内容。
- **中间步骤**：所有子运行（工具调用、LLM 调用等）。
- **元数据**：标签、用户反馈、延迟指标等。

与数据集中的示例不同，运行不包含参考输出。在线评估器必须在不知道“正确”答案应该是什么的情况下评估质量，而是依赖于质量启发式方法、安全检查和无参考评估技术。

了解更多关于[可观测性概念中的运行和追踪](/langsmith/observability-concepts#runs)。

#### 线程

_线程_ 是代表多轮对话的相关运行的集合。在线评估器可以在线程级别运行，以评估整个对话，而不是单个轮次。这使得可以评估对话级别的属性，例如跨轮次的连贯性、主题保持以及整个交互过程中的用户满意度。

## 评估器

_评估器_ 是给应用程序性能打分的函数。它们为离线和在线评估提供度量层，根据可用数据调整其输入。

可以使用 LangSmith SDK（[Python](https://docs.smith.langchain.com/reference/python/reference) 和 [TypeScript](https://docs.smith.langchain.com/reference/js)）、通过 [Prompt Playground](/langsmith/observability-concepts#prompt-playground) 或通过配置[规则](/langsmith/rules)在追踪项目或数据集上自动运行它们来运行评估器。

### 评估器输入

评估器输入根据评估类型而不同：

**离线评估器**接收：
- [示例](#examples)：来自您[数据集](#datasets)的示例，包含输入、参考输出和元数据。
- [运行](/langsmith/observability-concepts#runs)：在示例输入上运行应用程序的实际输出和中间步骤。

**在线评估器**接收：
- [运行](/langsmith/observability-concepts#runs)：包含输入、输出和中间步骤的生产追踪记录（无参考输出可用）。

### 评估器输出

评估器返回**反馈**，即评估的分数。反馈是一个字典或字典列表。每个字典包含：

- `key`：指标名称。
- `score` | `value`：指标值（`score` 用于数值指标，`value` 用于分类指标）。
- `comment`（可选）：对分数的额外推理或解释。

### 评估技术

LangSmith 支持多种评估方法：

- [人工](#human)
- [代码](#code)
- [LLM 作为裁判](#llm-as-judge)
- [成对比较](#pairwise)

#### 人工

_人工评估_ 涉及手动审查应用程序输出和执行追踪。这种方法[通常是评估的有效起点](https://hamel.dev/blog/posts/evals/#looking-at-your-traces)。LangSmith 提供了审查应用程序输出和追踪（所有中间步骤）的工具。

[标注队列](/langsmith/annotation-queues)简化了收集应用程序输出人工反馈的过程。

#### 代码

_代码评估器_ 是确定性的、基于规则的函数。它们适用于检查诸如验证聊天机器人响应结构不为空、生成的代码能否编译或分类是否完全匹配等情况。

#### LLM 作为裁判

_LLM 作为裁判评估器_ 使用 LLM 为应用程序输出打分。评分规则和标准通常编码在 LLM 提示词中。这些评估器可以是：

- **无参考**：检查输出是否包含冒犯性内容或符合特定标准。
- **基于参考**：将输出与参考进行比较（例如，检查相对于参考的事实准确性）。

LLM 作为裁判评估器需要仔细审查分数和提示词调优。少样本评估器（在评分器提示词中包含输入、输出和预期分数的示例）通常能提高性能。

了解[如何定义 LLM 作为裁判评估器](/langsmith/llm-as-judge)。

#### 成对比较

_成对比较评估器_ 使用启发式方法（例如，哪个响应更长）、LLM（带有成对比较提示词）或人工评审员来比较两个应用程序版本的输出。

当成对比较两个输出比直接给单个输出打分更容易时，成对比较评估效果很好。例如，在摘要任务中，选择两个摘要中信息更丰富的那个通常比给单个摘要分配绝对分数更容易。

学习[如何运行成对比较评估](/langsmith/evaluate-pairwise)。

### 无参考与基于参考的评估器

了解评估器是否需要参考输出对于确定其使用时机至关重要。

**无参考评估器** 在不与预期输出比较的情况下评估质量。这些既适用于离线评估也适用于在线评估：
- **安全检查**：毒性检测、PII 检测、内容策略违规
- **格式验证**：JSON 结构、必填字段、模式合规性
- **质量启发式**：响应长度、延迟、特定关键词
- **无参考 LLM 作为裁判**：清晰度、连贯性、帮助性、语气

**基于参考的评估器** 需要参考输出，仅适用于离线评估：
- **正确性**：与参考答案的语义相似性
- **事实准确性**：根据基本事实进行事实核查
- **精确匹配**：带有已知标签的分类任务
- **基于参考的 LLM 作为裁判**：将输出质量与参考进行比较

在设计评估策略时，无参考评估器在离线测试和在线监控中提供了一致性，而基于参考的评估器则在开发期间实现了更精确的正确性检查。

## 评估类型

LangSmith 支持针对开发和部署不同阶段的各种评估方法。了解何时使用每种类型有助于构建全面的评估策略。

离线和在线评估服务于不同的目的：
- **离线评估类型** 在部署前使用带有参考输出的精选数据集进行测试
- **在线评估类型** 在没有参考输出的实时流量上监控生产行为

了解更多关于[评估类型以及何时使用每种类型](/langsmith/evaluation-types)。

## 最佳实践

### 构建数据集

构建数据集有多种策略：

**手动整理的示例**

这是推荐的起点。创建 10-20 个涵盖常见场景和边缘情况的高质量示例。这些示例定义了您的应用程序“好”的标准。

**历史追踪记录**

一旦进入生产环境，将真实的追踪记录转换为示例。对于高流量应用程序：
- **用户反馈**：将收到负面反馈的运行添加到测试中。
- **启发式方法**：识别有趣的运行（例如，长延迟、错误）。
- **LLM 反馈**：使用 LLM 检测值得注意的对话。

**合成数据**

从现有示例生成额外的示例。当以几个高质量的手工示例作为模板开始时效果最好。

### 数据集组织

**划分**

将数据集划分为子集以进行针对性评估。使用划分进行性能优化（较小的划分用于快速迭代）和可解释性（分别评估不同的输入类型）。

学习如何[创建和管理数据集划分](/langsmith/manage-datasets-in-application#create-and-manage-dataset-splits)。

**版本**

当示例更改时，LangSmith 会自动创建数据集[版本](/langsmith/manage-datasets#version-a-dataset)。[标记版本](/langsmith/manage-datasets#tag-a-version)以标记重要里程碑。在 CI 流水线中定位特定版本，以确保数据集更新不会破坏工作流程。

### 人工反馈收集

人工反馈通常提供最有价值的评估，特别是对于主观质量维度。

**标注队列**

[标注队列](/langsmith/annotation-queues)支持结构化收集人工反馈。标记特定运行以供审查，在简化的界面中收集标注，并将标注的运行转移到数据集中以供未来评估。

标注队列通过提供额外功能来补充[内联标注](/langsmith/annotate-traces-inline)：对运行进行分组、指定标准和配置评审员权限。

### 评估与测试

测试和评估是相似但不同的概念。

**评估根据指标衡量性能。** 指标可以是模糊的或主观的，并且在相对意义上更有用。它们通常比较系统之间的性能。

**测试断言正确性。** 只有通过所有测试的系统才能部署。

评估指标可以转换为测试。例如，回归测试可以断言新版本必须在相关指标上优于基线版本。当系统运行成本高昂时，可以一起运行测试和评估以提高效率。

可以使用标准测试工具（如 [pytest](/langsmith/pytest) 或 [Vitest/Jest](/langsmith/vitest-jest)）编写评估。

## 快速参考：离线与在线评估

下表总结了离线和在线评估之间的主要区别：

| | **离线评估** | **在线评估** |
|---|---|---|
| **运行于** | 数据集（示例） | 追踪项目（运行/线程） |
| **数据访问** | 输入、输出、参考输出 | 仅输入、输出 |
| **使用时机** | 部署前、开发期间 | 生产环境、部署后 |
| **主要用例** | 基准测试、单元测试、回归测试、回测 | 实时监控、生产反馈、异常检测 |
| **评估时机** | 对精选测试集进行批处理 | 对实时流量进行实时或近实时处理 |
| **设置位置** | 评估选项卡（SDK、UI、Prompt Playground） | [可观测性选项卡](/langsmith/online-evaluations)（自动化规则） |
| **数据要求** | 需要数据集整理 | 无需数据集，评估实时追踪记录 |
