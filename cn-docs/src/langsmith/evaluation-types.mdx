---
title: 评估类型
sidebarTitle: Evaluation types
---
LangSmith 支持多种评估类型，适用于开发和部署的不同阶段。了解何时使用每种类型有助于构建全面的评估策略。

## 离线评估类型

离线评估在部署前使用精选数据集对应用程序进行测试。通过在具有参考输出的示例上运行评估，团队可以在将变更暴露给用户之前比较版本、验证功能并建立信心。

可以使用 LangSmith SDK（[Python](https://docs.smith.langchain.com/reference/python/reference) 和 [TypeScript](https://docs.smith.langchain.com/reference/js)）在客户端运行离线评估，也可以通过 [Prompt Playground](/langsmith/observability-concepts#prompt-playground) 或 [自动化规则](/langsmith/rules) 在服务端运行。

![离线评估](/langsmith/images/offline.png)

### 基准测试

_基准测试_ 在精选数据集上比较多个应用程序版本，以确定最佳表现者。此过程包括创建代表性输入的数据集、定义性能指标以及测试每个版本。

基准测试需要包含黄金标准参考输出的数据集和精心设计的比较指标。例如：
- **RAG 问答机器人**：包含问题和参考答案的数据集，使用 LLM 作为评判者来检查实际答案与参考答案之间的语义等价性。
- **ReACT 智能体**：包含用户请求和参考工具调用的数据集，使用启发式评估器验证是否进行了所有预期的工具调用。

### 单元测试

_单元测试_ 验证单个系统组件的正确性。在 LLM 上下文中，[单元测试通常是基于规则的断言](https://hamel.dev/blog/posts/evals/#level-1-unit-tests)，针对输入或输出（例如，验证 LLM 生成的代码能否编译、JSON 能否成功加载），以验证基本功能。

单元测试通常期望获得一致的通过结果，因此适合在 CI 流水线中运行。在 CI 中运行时，请配置缓存以最小化 LLM API 调用及相关成本。

### 回归测试

_回归测试_ 衡量应用程序版本随时间推移的性能一致性。它们确保新版本不会在当前版本能正确处理的情况下降低性能，并且理想情况下能展示出相对于基线的改进。这些测试通常在预计会影响用户体验的更新（例如，模型或架构变更）时运行。

LangSmith 的比较视图会突出显示相对于基线的回归（红色）和改进（绿色），从而能够快速识别变更。

![比较视图](/langsmith/images/comparison-view.png)

### 回测

_回测_ 使用历史生产数据评估新的应用程序版本。将生产日志转换为数据集，然后让较新的版本处理这些示例，以评估其在过去、真实的用户输入上的性能。

这种方法通常用于评估新模型版本。例如，当新模型可用时，在最近的生产运行上对其进行测试，并将结果与实际生产结果进行比较。

### 成对评估

_成对评估_ 通过确定相对质量来比较两个版本的输出，而不是分配绝对分数。对于某些任务，[确定“版本 A 优于版本 B”](https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-i/) 比独立为每个版本评分更容易。

这种方法对于主观任务的 LLM 作为评判者评估特别有用。例如，在摘要任务中，确定“哪个摘要更清晰、更简洁？”通常比分配数字清晰度分数更简单。

了解 [如何运行成对评估](/langsmith/evaluate-pairwise)。

## 在线评估类型

在线评估近乎实时地评估生产应用程序的输出。由于没有参考输出，这些评估侧重于检测问题、监控质量趋势以及识别可能影响未来离线测试的边缘情况。

在线评估器通常在服务端运行。LangSmith 提供了内置的 [LLM 作为评判者评估器](/langsmith/llm-as-judge) 供配置，并支持在 LangSmith 内运行的自定义代码评估器。

![在线评估](/langsmith/images/online.png)

### 实时监控

在用户与系统交互时持续监控应用程序质量。在线评估在生产流量上自动运行，为每次交互提供即时反馈。这能够在问题影响大量用户之前，检测到质量下降、异常模式或意外行为。

### 异常检测

识别偏离预期模式的异常值和边缘情况。在线评估器可以标记具有异常特征的运行——例如极长或极短的响应、意外的错误率或未能通过安全检查的输出——以供人工审查，并可能将其添加到离线数据集中。

### 生产反馈循环

利用生产中的洞察来改进离线评估。在线评估会揭示真实世界的问题和使用模式，这些问题和模式可能不会出现在精选数据集中。失败的生产运行会成为数据集示例的候选，从而创建一个迭代循环，使生产经验不断优化测试覆盖范围。
