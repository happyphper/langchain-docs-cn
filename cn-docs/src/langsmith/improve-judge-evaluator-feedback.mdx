---
title: 利用人类反馈改进 LLM 作为评判者的评估效果
sidebarTitle: Improve LLM-as-judge evaluators using human feedback
---
<Check>
在阅读本页内容之前，建议先阅读以下内容：

* [评估概念](/langsmith/evaluation-concepts#evaluators)
* [创建 LLM 作为评判者的评估器](/langsmith/llm-as-judge)
</Check>

可靠的 [_LLM 作为评判者的评估器_](/langsmith/evaluation-concepts#llm-as-judge) 对于就您的 AI 应用（例如，提示词、模型、架构变更）做出明智决策至关重要。正确定义评估器提示词可能很困难，但它直接影响评估的可信度。

本指南描述了如何利用人工反馈来对齐您的 LLM 作为评判者的评估器，以提高评估器的质量，并帮助您构建可靠的 AI 应用。

## 工作原理

LangSmith 的 **对齐评估器** 功能包含一系列步骤，可帮助您根据人类专家反馈来对齐您的 LLM 作为评判者的评估器。您可以使用此功能来对齐用于[离线评估](/langsmith/evaluation-concepts#offline-evaluation)或[在线评估](/langsmith/evaluation-concepts#online-evaluation)的数据集上运行的评估器。无论哪种情况，步骤都类似：

1.  **选择实验或运行**，其中包含来自您应用的输出。
2.  将选定的实验或运行添加到**标注队列**中，供人类专家标注数据。
3.  **针对已标注的示例测试您的 LLM 作为评判者的评估器提示词**。检查您的评估器结果与标注数据不一致的情况。这表明您的评估器提示词需要改进的地方。
4.  **优化并重复**以提高评估器的对齐度。更新您的 LLM 作为评判者的评估器提示词并再次测试。

## 前提条件

在开始本指南进行[离线评估](#离线评估)或[在线评估](#在线评估)之前，您需要满足以下条件：

### 离线评估

*   一个包含至少一个[实验](/langsmith/evaluation-concepts#experiment)的[数据集](/langsmith/evaluation-concepts#datasets)。
*   您需要通过 [SDK](/langsmith/manage-datasets-programmatically#create-a-dataset) 或 [UI](/langsmith/manage-datasets-in-application#set-up-your-dataset) 上传或创建数据集，并通过 [SDK](/langsmith/evaluate-llm-application#run-the-evaluation) 或 [Playground](/langsmith/run-evaluation-from-prompt-playground#5-run-your-evaluation) 运行一个实验。

### 在线评估

*   一个已经向 LangSmith 发送追踪的应用。
*   通过其中一个[追踪集成](/langsmith/observability-concepts#integrations)进行配置以开始。

## 开始使用

您可以在数据集和追踪项目中为新评估器和现有评估器进入对齐流程。

|                                              | 数据集评估器                                                                                                                                                                                                                                                              | 追踪项目评估器                                                                                                                                                                                                                                                              |
| -------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **从头创建对齐的评估器** | 1. 进入 **Datasets & Experiments** 并选择您的数据集<br></br>2. 点击 **+ Evaluator** > **Create from labeled data**<br></br>3. 输入一个描述性的反馈键名称（例如 `correctness`、`hallucination`） | 1. 进入 **Projects** 并选择您的项目<br></br>2. 点击 **+ New** > **Evaluator** > **Create from labeled data**<br></br>3. 输入一个描述性的反馈键名称（例如 `correctness`、`hallucination`） |
| **对齐现有评估器**              | 1. 进入 **Datasets & Experiments** > 选择您的数据集 > **Evaluators** 标签页<br></br>2. 在 **Align Evaluator with experiment data** 框中，点击 **Select Experiments**                              | 1. 进入 **Projects** > 选择您的项目 > **Evaluators** 标签页<br></br>2. 在 **Align Evaluator with experiment data** 框中，点击 **Select Experiments**                                         |

## 1. 选择实验或运行

选择一个或多个实验（或运行）发送给人工标注。这会将运行添加到[标注队列](/langsmith/evaluation-concepts#annotation-queues)。

![添加到评估器队列](/langsmith/images/add-to-evaluator-queue.gif)

要将任何新的实验/运行添加到现有的标注队列，请转到 **Evaluators** 标签页，选择您正在对齐的评估器，然后点击 **Add to Queue。**

<Check>
数据集应能代表您预期在生产环境中看到的输入和输出。

虽然您不需要涵盖所有可能的场景，但重要的是要包含涵盖所有预期用例范围的示例。例如，如果您正在构建一个回答有关棒球、篮球和足球问题的体育机器人，您的数据集应至少包含来自每项运动的已标注示例。
</Check>

## 2. 标注示例

通过添加反馈分数来标注标注队列中的示例。标注完一个示例后，点击 **Add to Reference Dataset**。

<Check>
如果您的实验中有大量示例，您不需要标注每个示例即可开始。我们建议至少从 20 个示例开始，您以后随时可以添加更多。我们建议您标注的示例是多样化的（0 和 1 标签平衡），以确保您构建的评估器提示词是全面的。
</Check>

## 3. 针对已标注的示例测试您的评估器提示词

一旦您有了已标注的示例，下一步就是迭代您的评估器提示词，以尽可能模仿已标注的数据。此迭代在 **Evaluator Playground** 中完成。

要进入评估器 Playground：点击评估器队列右上角的 **View evaluator** 按钮。这将带您到您正在对齐的评估器的详情页面。点击 **Evaluator Playground** 按钮即可访问 Playground。

![评估器 Playground](/langsmith/images/evaluator-pg.gif)

在评估器 Playground 中，您可以创建或编辑您的评估器提示词，然后点击 **Start Alignment** 在第 2 步中创建的已标注示例集上运行它。运行评估器后，您将看到其生成的分数与您的人工标注相比如何。对齐分数是评估器判断与人类专家判断一致的示例所占的百分比。

## 4. 重复以提高评估器对齐度

通过更新您的提示词并再次测试来迭代，以提高评估器的对齐度。

<Check>
对评估器提示词的更新**默认不会保存**。我们建议定期保存您的评估器提示词，尤其是在您看到对齐分数提高之后。

当您在迭代提示词时，评估器 Playground 将显示您最近保存的评估器提示词版本的对齐分数，以供比较。
</Check>

提高评估器的对齐分数并非一门精确的科学，但有一些策略有助于提高对齐分数。

### 提高评估器对齐度的技巧

**1. 调查未对齐的示例**

深入研究未对齐的示例，并尝试将它们归类为常见的失败模式，这是提高评估器对齐度的良好第一步。

一旦确定了常见的失败模式，请向您的评估器提示词添加说明，以便 LLM 了解它们。例如，如果您注意到它不理解特定的首字母缩写词，您可以解释“MFA 代表‘多因素认证’”。或者，如果它在您的评估器上下文中对好/坏的含义感到困惑，您可以告诉它“一个好的响应应始终包含至少 3 个可供预订的酒店”。

**2. 检查 LLM 评分背后的推理过程**

为了理解 LLM 为何以某种方式对示例进行评分，您可以为您的 LLM 作为评判者的评估器启用推理。推理有助于理解 LLM 的思维过程，并可以帮助您识别常见的失败模式，以便将其纳入您的评估器提示词中。

为了在评估器 Playground 中查看推理过程，请将鼠标悬停在 LLM 分数上。

![启用推理](/langsmith/images/enable-reasoning.gif)

这将在评估器 Playground 中显示 LLM 评分背后的推理过程。

**3. 添加更多已标注的示例并验证性能**

为了避免对已标注的示例过拟合，添加更多已标注的示例并测试性能非常重要，特别是如果您开始时只有少量示例。

## 视频指南
<iframe
  className="w-full aspect-video rounded-xl"
  src="https://www.youtube.com/embed/-9o94oj4x0A?si=wfv9cN3L4DalMD2e"
  title="YouTube video player"
  frameBorder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
  allowFullScreen
></iframe>
