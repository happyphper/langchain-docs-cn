---
title: 追踪一个 RAG 应用教程
sidebarTitle: Trace a RAG application
---
import project from '/snippets/langsmith/trace-ingestion-project.mdx';

在本教程中，我们将使用 OpenAI SDK 构建一个简单的 RAG 应用程序。我们将在从原型开发到生产的每个开发阶段，为应用程序添加可观测性。

<CodeGroup>

```python Python
from openai import OpenAI
openai_client = OpenAI()

# 这是我们在 RAG 中要使用的检索器
# 这里模拟了检索过程，但它可以是任何我们想要的形式
def retriever(query: str):
    results = ["Harrison worked at Kensho"]
    return results

# 这是端到端的 RAG 链。
# 它先执行检索步骤，然后调用 OpenAI
def rag(question):
    docs = retriever(question)
    system_message = """Answer the users question using only the provided information below:
        {docs}""".format(docs="\n".join(docs))

    return openai_client.chat.completions.create(
        messages=[
            {"role": "system", "content": system_message},
            {"role": "user", "content": question},
        ],
        model="gpt-4o-mini",
    )
```

```typescript TypeScript
import { OpenAI } from "openai";
const openAIClient = new OpenAI();

// 这是我们在 RAG 中要使用的检索器
// 这里模拟了检索过程，但它可以是任何我们想要的形式
async function retriever(query: string) {
  return ["This is a document"];
}

// 这是端到端的 RAG 链。
// 它先执行检索步骤，然后调用 OpenAI
async function rag(question: string) {
  const docs = await retriever(question);

  const systemMessage =
    "Answer the users question using only the provided information below:\n\n" +
    docs.join("\n");

  return await openAIClient.chat.completions.create({
    messages: [
      { role: "system", content: systemMessage },
      { role: "user", content: question },
    ],
    model: "gpt-4o-mini",
  });
}
```

</CodeGroup>

## 原型开发

从一开始就设置好可观测性，可以帮助你以**远超**通常的速度进行迭代。它让你在快速迭代提示词，或更改所使用的数据和模型时，能够清晰地洞察应用程序的运行情况。在本节中，我们将逐步介绍如何设置可观测性，以便你在原型开发阶段获得最大的可观测性。

### 设置你的环境

首先，通过导航到 [设置页面](https://smith.langchain.com/settings) 创建一个 API 密钥。

接下来，安装 LangSmith SDK：

<CodeGroup>

```bash Python SDK
pip install langsmith
```

```bash TypeScript SDK
npm install langsmith
```

</CodeGroup>

最后，设置相应的环境变量。这将把追踪记录到 `default` 项目（不过你可以轻松更改）。

```bash
export LANGSMITH_TRACING=true
export LANGSMITH_API_KEY=<your-api-key>
export LANGSMITH_WORKSPACE_ID=<your-workspace-id>
export LANGSMITH_PROJECT=default
```

<project />

<Note>
你可能在其他地方看到这些变量被引用为 `LANGCHAIN_*`。它们是等价的，但最佳实践是使用 `LANGSMITH_TRACING`、`LANGSMITH_API_KEY`、`LANGSMITH_PROJECT`。

`LANGSMITH_PROJECT` 标志仅在 JS SDK 版本 >= 0.2.16 中受支持，如果你使用的是旧版本，请改用 `LANGCHAIN_PROJECT`。
</Note>

### 追踪你的 LLM 调用

你可能首先想要追踪的是你所有的 OpenAI 调用。毕竟，这是实际调用 LLM 的地方，所以是最重要的部分！通过引入一个极其简单的 OpenAI 包装器，我们努力让 LangSmith 实现这一点变得尽可能容易。你只需要修改你的代码，使其看起来像这样：

<CodeGroup>

```python Python
from openai import OpenAI
from langsmith.wrappers import wrap_openai
openai_client = wrap_openai(OpenAI())

# 这是我们在 RAG 中要使用的检索器
# 这里模拟了检索过程，但它可以是任何我们想要的形式
def retriever(query: str):
    results = ["Harrison worked at Kensho"]
    return results

# 这是端到端的 RAG 链。
# 它先执行检索步骤，然后调用 OpenAI
def rag(question):
    docs = retriever(question)
    system_message = """Answer the users question using only the provided information below:
        {docs}""".format(docs="\n".join(docs))

    return openai_client.chat.completions.create(
        messages=[
            {"role": "system", "content": system_message},
            {"role": "user", "content": question},
        ],
        model="gpt-4o-mini",
    )
```

```typescript TypeScript
import { OpenAI } from "openai";
import { wrapOpenAI } from "langsmith/wrappers";
const openAIClient = wrapOpenAI(new OpenAI());

// 这是我们在 RAG 中要使用的检索器
// 这里模拟了它的功能，但它可以是任何我们想要的东西
async function retriever(query: string) {
  return ["This is a document"];
}

// 这是端到端的 RAG 链。
// 它先执行检索步骤，然后调用 OpenAI
async function rag(question: string) {
  const docs = await retriever(question);

  const systemMessage =
    "Answer the users question using only the provided information below:\n\n" +
    docs.join("\n");

  return await openAIClient.chat.completions.create({
    messages: [
      { role: "system", content: systemMessage },
      { role: "user", content: question },
    ],
    model: "gpt-4o-mini",
  });
}
```

</CodeGroup>

请注意我们如何导入 `from langsmith.wrappers import wrap_openai` 并使用它来包装 OpenAI 客户端 (`openai_client = wrap_openai(OpenAI())`)。

如果你以下面的方式调用它会发生什么？

```python
rag("where did harrison work")
```

这将生成仅包含 OpenAI 调用的追踪记录 - 它应该看起来像[这样](https://smith.langchain.com/public/e7b7d256-10fe-4d49-a8d5-36ca8e5af0d2/r)

![Tracing tutorial openai](/langsmith/images/tracing-tutorial-openai.png)

### 追踪整个链

很好 - 我们已经追踪了 LLM 调用。但追踪更多内容通常非常有价值。LangSmith **专为**追踪整个 LLM 流水线而构建 - 所以让我们开始吧！我们可以通过修改代码来实现，使其现在看起来像这样：

<CodeGroup>

```python Python
from openai import OpenAI
from langsmith import traceable
from langsmith.wrappers import wrap_openai
openai_client = wrap_openai(OpenAI())

def retriever(query: str):
    results = ["Harrison worked at Kensho"]
    return results

@traceable
def rag(question):
    docs = retriever(question)
    system_message = """Answer the users question using only the provided information below:
        {docs}""".format(docs="\n".join(docs))

    return openai_client.chat.completions.create(
        messages=[
            {"role": "system", "content": system_message},
            {"role": "user", "content": question},
        ],
        model="gpt-4o-mini",
    )
```

```typescript TypeScript
import { OpenAI } from "openai";
import { traceable } from "langsmith/traceable";
import { wrapOpenAI } from "langsmith/wrappers";
const openAIClient = wrapOpenAI(new OpenAI());

async function retriever(query: string) {
  return ["This is a document"];
}

const rag = traceable(async function rag(question: string) {
  const docs = await retriever(question);

  const systemMessage =
    "Answer the users question using only the provided information below:\n\n" +
    docs.join("\n");

  return await openAIClient.chat.completions.create({
    messages: [
      { role: "system", content: systemMessage },
      { role: "user", content: question },
    ],
    model: "gpt-4o-mini",
  });
});
```

</CodeGroup>

请注意我们如何导入 `from langsmith import traceable` 并使用它来装饰整个函数 (`@traceable`)。

如果你以下面的方式调用它会发生什么？

```python
rag("where did harrison work")
```

这将生成整个 RAG 流水线的追踪记录 - 它应该看起来像[这样](https://smith.langchain.com/public/8cafba6a-1a6d-4a73-8565-483186f31c29/r)

![Tracing tutorial chain](/langsmith/images/tracing-tutorial-chain.png)

## Beta 测试

LLM应用开发的下一阶段是对你的应用进行Beta测试。这是你将其发布给少数初始用户的时候。在此阶段建立良好的可观测性至关重要，因为你通常无法确切知道用户将如何实际使用你的应用，而这能让你深入了解他们的使用方式。这也意味着你可能需要对追踪设置进行一些调整，以便更好地实现这一目标。这扩展了你在上一节中建立的可观测性。

### 收集反馈

在Beta测试期间，良好可观测性的一个重要部分是收集反馈。收集什么反馈通常因应用而异——但至少一个简单的点赞/点踩是一个好的开始。记录反馈后，你需要能够轻松地将其与引发该反馈的运行关联起来。幸运的是，LangSmith让这变得很容易。

首先，你需要从应用中记录反馈。一个简单的方法是跟踪每次运行的运行ID，然后使用它来记录反馈。跟踪运行ID看起来像这样：

```python
from langsmith import uuid7

run_id = str(uuid7())
rag(
    "where did harrison work",
    langsmith_extra={"run_id": run_id}
)
```

将反馈与该运行关联起来看起来像这样：

```python
from langsmith import Client
ls_client = Client()
ls_client.create_feedback(
    run_id,
    key="user-score",
    score=1.0,
)
```

一旦反馈被记录，你就可以在检查运行时点击`Metadata`选项卡，看到与每次运行关联的反馈。它应该看起来像[这样](https://smith.langchain.com/public/8cafba6a-1a6d-4a73-8565-483186f31c29/r)

![追踪教程反馈](/langsmith/images/tracing-tutorial-feedback.png)

你也可以通过使用运行表中的过滤逻辑，查询所有获得正面（或负面）反馈的运行。你可以通过创建如下过滤器来实现：

![追踪教程过滤](/langsmith/images/tracing-tutorial-filtering.png)

### 记录元数据

开始记录元数据也是一个好主意。这允许你开始跟踪应用的不同属性。这对于让你了解产生特定结果时使用的是哪个版本或变体的应用非常重要。

对于这个例子，我们将记录使用的LLM。通常你可能正在尝试不同的LLM，因此将这些信息作为元数据对于过滤很有用。为了做到这一点，我们可以这样添加：

```python
from openai import OpenAI
from langsmith import traceable
from langsmith.wrappers import wrap_openai
openai_client = wrap_openai(OpenAI())

@traceable(run_type="retriever")
def retriever(query: str):
    results = ["Harrison worked at Kensho"]
    return results

@traceable(metadata={"llm": "gpt-4o-mini"})
def rag(question):
    docs = retriever(question)
    system_message = """Answer the users question using only the provided information below:
    {docs}""".format(docs='\n'.join(docs))
    return openai_client.chat.completions.create(messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": question},
    ], model="gpt-4o-mini")
```

注意，我们在`rag`函数中添加了`@traceable(metadata={"llm": "gpt-4o-mini"})`。

以这种方式跟踪元数据假设它是预先已知的。这对于LLM类型来说没问题，但对于其他类型的信息——比如用户ID——就不太理想了。为了记录这类信息，我们可以在运行时通过运行ID传入。

```python
from langsmith import uuid7

run_id = str(uuid7())
rag(
    "where did harrison work",
    langsmith_extra={"run_id": run_id, "metadata": {"user_id": "harrison"}}
)
```

现在我们已经记录了这两条元数据，我们应该能够在UI中看到它们都显示出来，点击[这里](https://smith.langchain.com/public/37adf7e5-97aa-42d0-9850-99c0199bddf6/r)。

![追踪教程元数据](/langsmith/images/tracing-tutorial-metadata.png)

我们可以通过构建如下过滤器来筛选这些信息片段：

![追踪教程元数据过滤](/langsmith/images/tracing-tutorial-metadata-filtering.png)

## 生产环境

很好——你已经利用这种新发现的观测能力快速迭代，并对应用性能充满信心。是时候将其部署到生产环境了！你需要添加哪些新的观测功能呢？

首先，请注意你已经添加的相同观测功能将在生产环境中持续提供价值。你仍将能够深入查看特定的运行记录。

在生产环境中，你可能会有更多的流量。因此，你肯定不希望被困在逐个查看数据点中。幸运的是，LangSmith 提供了一套工具来帮助实现生产环境中的观测。

### 监控

如果你在项目中点击 `Monitor` 标签，你将看到一系列监控图表。这里我们跟踪许多 LLM 特定的统计数据——追踪数量、反馈、首令牌时间等。你可以跨几个不同的时间区间查看这些数据随时间的变化。

![追踪教程监控](/langsmith/images/tracing-tutorial-monitor.png)

### A/B 测试

<Note>
A/B 测试的分组功能要求给定元数据键至少存在 2 个不同的值。
</Note>

你还可以使用此标签执行一种 A/B 测试。在之前的教程中，我们开始跟踪几个不同的元数据属性——其中之一是 `llm`。我们可以按**任何**元数据属性对监控图表进行分组，并立即获得按时间分组的图表。这使我们能够试验不同的 LLM（或提示词，或其他），并跟踪它们随时间变化的性能。

为此，我们只需点击顶部的 `Metadata` 按钮。这将提供一个下拉选项列表供我们选择分组依据：

![追踪教程监控元数据](/langsmith/images/tracing-tutorial-monitor-metadata.png)

一旦我们选择此项，我们将开始看到按此属性分组的图表：

![追踪教程监控分组](/langsmith/images/tracing-tutorial-monitor-grouped.png)

### 深入分析

LangSmith 提供的一项强大能力是，当你在查看监控图表时，能够轻松深入分析你识别为有问题的数据点。为此，你只需将鼠标悬停在监控图表中的一个数据点上。当你这样做时，你将能够点击该数据点。这将带你回到运行记录表，并带有过滤视图：

![追踪教程监控深入分析](/langsmith/images/tracing-tutorial-monitor-drilldown.png)

## 结论

在本教程中，你看到了如何为你的 LLM 应用设置一流的观测能力。无论你的应用处于哪个阶段，你都将从观测中受益。

如果你对观测有更深入的问题，请查看[操作指南部分](/langsmith/observability-concepts)，了解关于测试、提示词管理等主题的指南。
