---
title: 使用 Vercel AI SDK 进行追踪（仅限 JS/TS）
sidebarTitle: Vercel AI SDK
---
您可以使用 LangSmith 来追踪 Vercel AI SDK 的运行。本指南将通过一个示例进行说明。

## 安装

<Note>
此包装器需要 AI SDK v5 和 `langsmith>=0.3.63`。如果您使用的是旧版本的 AI SDK 或 `langsmith`，请参阅[此页面](/langsmith/legacy-trace-with-vercel-ai-sdk)上基于 OpenTelemetry (OTEL) 的方法。
</Note>

安装 Vercel AI SDK。本指南在下面的代码片段中使用 Vercel 的 OpenAI 集成，但您也可以使用他们的任何其他选项。

<CodeGroup>

```bash npm
npm install ai @ai-sdk/openai zod
```

```bash yarn
yarn add ai @ai-sdk/openai zod
```

```bash pnpm
pnpm add ai @ai-sdk/openai zod
```

</CodeGroup>

## 环境配置

<CodeGroup>

```bash Shell
export LANGSMITH_TRACING=true
export LANGSMITH_API_KEY=<your-api-key>

# 示例使用 OpenAI，但您可以选择任何 LLM 提供商
export OPENAI_API_KEY=<your-openai-api-key>

# 对于链接到多个工作区的 LangSmith API 密钥，设置 LANGSMITH_WORKSPACE_ID 环境变量以指定要使用的工作区。
export LANGSMITH_WORKSPACE_ID=<your-workspace-id>
```

</CodeGroup>

## 基本设置

导入并包装 AI SDK 方法，然后像往常一样使用它们：

```typescript
import { openai } from "@ai-sdk/openai";
import * as ai from "ai";

import { wrapAISDK } from "langsmith/experimental/vercel";

const { generateText, streamText, generateObject, streamObject } =
  wrapAISDK(ai);

await generateText({
  model: openai("gpt-5-nano"),
  prompt: "Write a vegetarian lasagna recipe for 4 people.",
});
```

您应该在 LangSmith 仪表板中看到一个追踪记录，[类似于此](https://smith.langchain.com/public/4f0e689e-c801-44d3-8857-93b47ab100cc/r)。

您还可以追踪包含工具调用的运行：

```typescript
import * as ai from "ai";
import { tool, stepCountIs } from "ai";
import { openai } from "@ai-sdk/openai";
import { z } from "zod";

import { wrapAISDK } from "langsmith/experimental/vercel";

const { generateText, streamText, generateObject, streamObject } =
  wrapAISDK(ai);

await generateText({
  model: openai("gpt-5-nano"),
  messages: [
    {
      role: "user",
      content: "What are my orders and where are they? My user ID is 123",
    },
  ],
  tools: {
    listOrders: tool({
      description: "list all orders",
      inputSchema: z.object({ userId: z.string() }),
      execute: async ({ userId }) =>
        `User ${userId} has the following orders: 1`,
    }),
    viewTrackingInformation: tool({
      description: "view tracking information for a specific order",
      inputSchema: z.object({ orderId: z.string() }),
      execute: async ({ orderId }) =>
        `Here is the tracking information for ${orderId}`,
    }),
  },
  stopWhen: stepCountIs(5),
});
```

这将产生一个追踪记录，[类似于此](https://smith.langchain.com/public/6075fa2c-d255-4885-a66a-4fc798afaa9f/r)。

您可以像往常一样使用其他 AI SDK 方法。

### 使用 `traceable`

您可以在 AI SDK 调用周围或 AI SDK 工具调用内部包装 `traceable` 调用。如果您想在 LangSmith 中将运行分组在一起，这很有用：

```typescript
import * as ai from "ai";
import { tool, stepCountIs } from "ai";
import { openai } from "@ai-sdk/openai";
import { z } from "zod";

import { traceable } from "langsmith/traceable";
import { wrapAISDK } from "langsmith/experimental/vercel";

const { generateText, streamText, generateObject, streamObject } =
  wrapAISDK(ai);
```

```typescript
const wrapper = traceable(async (input: string) => {
  const { text } = await generateText({
    model: openai("gpt-5-nano"),
    messages: [
      {
        role: "user",
        content: input,
      },
    ],
    tools: {
      listOrders: tool({
        description: "list all orders",
        inputSchema: z.object({ userId: z.string() }),
        execute: async ({ userId }) =>
          `User ${userId} has the following orders: 1`,
      }),
      viewTrackingInformation: tool({
        description: "view tracking information for a specific order",
        inputSchema: z.object({ orderId: z.string() }),
        execute: async ({ orderId }) =>
          `Here is the tracking information for ${orderId}`,
      }),
    },
    stopWhen: stepCountIs(5),
  });
  return text;
}, {
  name: "wrapper",
});

await wrapper("What are my orders and where are they? My user ID is 123.");
```

生成的追踪记录将[如图所示](https://smith.langchain.com/public/ff25bc26-9389-4798-8b91-2bdcc95d4a8e/r)。

## 在无服务器环境中进行追踪

在无服务器环境中进行追踪时，您必须在环境关闭前等待所有运行记录完成刷新。为此，您可以在包装 AI SDK 方法时传入一个 LangSmith [`Client`](https://docs.smith.langchain.com/reference/js/classes/client.Client) 实例，然后调用 `await client.awaitPendingTraceBatches()`。
请确保也将其传入您创建的任何 `traceable` 包装器：

```typescript
import * as ai from "ai";
import { tool, stepCountIs } from "ai";
import { openai } from "@ai-sdk/openai";
import { z } from "zod";

import { Client } from "langsmith";
import { traceable } from "langsmith/traceable";
import { wrapAISDK } from "langsmith/experimental/vercel";

const client = new Client();

const { generateText, streamText, generateObject, streamObject } =
  wrapAISDK(ai, { client });

const wrapper = traceable(async (input: string) => {
  const { text } = await generateText({
    model: openai("gpt-5-nano"),
    messages: [
      {
        role: "user",
        content: input,
      },
    ],
    tools: {
      listOrders: tool({
        description: "list all orders",
        inputSchema: z.object({ userId: z.string() }),
        execute: async ({ userId }) =>
          `User ${userId} has the following orders: 1`,
      }),
      viewTrackingInformation: tool({
        description: "view tracking information for a specific order",
        inputSchema: z.object({ orderId: z.string() }),
        execute: async ({ orderId }) =>
          `Here is the tracking information for ${orderId}`,
      }),
    },
    stopWhen: stepCountIs(5),
  });
  return text;
}, {
  name: "wrapper",
  client,
});

try {
  await wrapper("What are my orders and where are they? My user ID is 123.");
} finally {
  await client.awaitPendingTraceBatches();
}
```

如果您使用的是 `Next.js`，有一个方便的 [`after`](https://nextjs.org/docs/app/api-reference/functions/after) 钩子，您可以将此逻辑放在其中：

```typescript
import { after } from "next/server"
import { Client } from "langsmith";


export async function POST(request: Request) {
  const client = new Client();

  ...

  after(async () => {
    await client.awaitPendingTraceBatches();
  });

  return new Response(JSON.stringify({ ... }), {
    status: 200,
    headers: { "Content-Type": "application/json" },
  });
};
```

有关更多详细信息，包括在无服务器环境中管理速率限制的信息，请参阅[此页面](/langsmith/serverless-environments)。

## 传递 LangSmith 配置

您可以在最初包装 AI SDK 方法时以及通过 `providerOptions.langsmith` 运行它们时，将 LangSmith 特定的配置传递给您的包装器。
这包括元数据（您稍后可以在 LangSmith 中用于筛选运行记录）、顶层运行名称、标签、自定义客户端实例等。

包装时传递的配置将应用于您使用包装方法进行的所有未来调用：

```typescript
import { openai } from "@ai-sdk/openai";
import * as ai from "ai";

import { wrapAISDK } from "langsmith/experimental/vercel";

const { generateText, streamText, generateObject, streamObject } =
  wrapAISDK(ai, {
    metadata: {
      key_for_all_runs: "value",
    },
    tags: ["myrun"],
  });

await generateText({
  model: openai("gpt-5-nano"),
  prompt: "Write a vegetarian lasagna recipe for 4 people.",
});
```

通过 `providerOptions.langsmith` 在运行时传递配置将仅应用于该次运行。
我们建议导入并使用 `createLangSmithProviderOptions` 包装您的配置，以确保正确的类型：

```typescript
import { openai } from "@ai-sdk/openai";
import * as ai from "ai";

import {
  wrapAISDK,
  createLangSmithProviderOptions,
} from "langsmith/experimental/vercel";

const { generateText, streamText, generateObject, streamObject } =
  wrapAISDK(ai);

const lsConfig = createLangSmithProviderOptions({
  metadata: {
    individual_key: "value",
  },
  name: "my_individual_run",
});

await generateText({
  model: openai("gpt-5-nano"),
  prompt: "Write a vegetarian lasagna recipe for 4 people.",
  providerOptions: {
    langsmith: lsConfig,
  },
});
```

## 数据脱敏

您可以通过指定自定义的输入/输出处理函数，来定制 AI SDK 发送给 LangSmith 的输入和输出内容。如果您正在处理敏感数据并希望避免将其发送到 LangSmith，这将非常有用。

由于输出格式会根据您使用的 AI SDK 方法而有所不同，我们建议单独定义配置并将其传递到包装的方法中。您还需要为 AI SDK 调用内部的子 LLM 运行提供单独的函数，因为在顶层调用 `generateText` 会在内部调用 LLM，并且可能多次调用。

我们还建议向 `createLangSmithProviderOptions` 传递一个泛型参数，以便为输入和输出获取正确的类型。
以下是 `generateText` 的一个示例：

```typescript
import {
  wrapAISDK,
  createLangSmithProviderOptions,
} from "langsmith/experimental/vercel";
import * as ai from "ai";
import { openai } from "@ai-sdk/openai";

const { generateText } = wrapAISDK(ai);

const lsConfig = createLangSmithProviderOptions<typeof generateText>({
  processInputs: (inputs) => {
    const { messages } = inputs;
    return {
      messages: messages?.map((message) => ({
        providerMetadata: message.providerOptions,
        role: "assistant",
        content: "REDACTED",
      })),
      prompt: "REDACTED",
    };
  },
  processOutputs: (outputs) => {
    return {
      providerMetadata: outputs.providerMetadata,
      role: "assistant",
      content: "REDACTED",
    };
  },
  processChildLLMRunInputs: (inputs) => {
    const { prompt } = inputs;
    return {
      messages: prompt.map((message) => ({
        ...message,
        content: "REDACTED CHILD INPUTS",
      })),
    };
  },
  processChildLLMRunOutputs: (outputs) => {
    return {
      providerMetadata: outputs.providerMetadata,
      content: "REDACTED CHILD OUTPUTS",
      role: "assistant",
    };
  },
});

const { text } = await generateText({
  model: openai("gpt-5-nano"),
  prompt: "What is the capital of France?",
  providerOptions: {
    langsmith: lsConfig,
  },
});

// Paris.
console.log(text);
```

实际的返回值将包含原始的、未脱敏的结果，但 LangSmith 中的追踪记录将是脱敏的。[这里有一个示例](https://smith.langchain.com/public/b4c69c8e-285b-4c0c-8492-e571e2cf562f/r)。

要对工具输入/输出进行脱敏，请像这样将您的 `execute` 方法包装在 `traceable` 中：

```typescript
import * as ai from "ai";
import { tool, stepCountIs } from "ai";
import { openai } from "@ai-sdk/openai";
import { z } from "zod";

import { Client } from "langsmith";
import { traceable } from "langsmith/traceable";
import { wrapAISDK } from "langsmith/experimental/vercel";

const client = new Client();
```

const { generateText, streamText, generateObject, streamObject } =
  wrapAISDK(ai, { client });

const { text } = await generateText({
  model: openai("gpt-5-nano"),
  messages: [
    {
      role: "user",
      content: "我的订单是什么？我的用户 ID 是 123。",
    },
  ],
  tools: {
    listOrders: tool({
      description: "列出所有订单",
      inputSchema: z.object({ userId: z.string() }),
      execute: traceable(
        async ({ userId }) => {
          return `用户 ${userId} 拥有以下订单：1`;
        },
        {
          processInputs: (input) => ({ text: "已编辑" }),
          processOutputs: (outputs) => ({ text: "已编辑" }),
          run_type: "tool",
          name: "listOrders",
        }
      ) as (input: { userId: string }) => Promise<string>,
    }),
  },
  stopWhen: stepCountIs(5),
});
```

`traceable` 的返回类型很复杂，这使得类型转换成为必要。如果您希望避免类型转换，也可以省略 AI SDK 的 `tool` 包装函数。
