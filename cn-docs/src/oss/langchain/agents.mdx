---
title: 智能体 (Agents)
---

智能体 (Agents) 将语言模型与 [工具 (tools)](/oss/langchain/tools) 相结合，构建出能够对任务进行推理、决定使用哪些工具并迭代寻找解决方案的系统。

:::python
@[`create_agent`] 提供了一个生产级的智能体实现。
:::
:::js
`createAgent()` 提供了一个生产级的智能体实现。
:::

[LLM 智能体通过循环运行工具来实现目标](https://simonwillison.net/2025/Sep/18/agents/)。
智能体会持续运行，直到满足停止条件——即模型产生最终输出或达到迭代限制。

```mermaid
%%{
  init: {
    "fontFamily": "monospace",
    "flowchart": {
      "curve": "curve"
    },
    "themeVariables": {"edgeLabelBackground": "transparent"}
  }
}%%
graph TD
  %% Outside the agent
  QUERY([输入])
  LLM{模型}
  TOOL(工具)
  ANSWER([输出])

  %% Main flows (no inline labels)
  QUERY --> LLM
  LLM --"执行动作"--> TOOL
  TOOL --"观察结果"--> LLM
  LLM --"完成任务"--> ANSWER

  classDef blueHighlight fill:#0a1c25,stroke:#0a455f,color:#bae6fd;
  classDef greenHighlight fill:#0b1e1a,stroke:#0c4c39,color:#9ce4c4;
  class QUERY blueHighlight;
  class ANSWER blueHighlight;
```

<Info>

:::python
@[`create_agent`] 使用 [LangGraph](/oss/langgraph/overview) 构建了一个基于 **图 (graph)** 的智能体运行时。图由节点 (nodes，即步骤) 和边 (edges，即连接) 组成，定义了智能体处理信息的方式。智能体在这个图中移动，执行诸如模型节点 (调用模型)、工具节点 (执行工具) 或中间件等节点。
:::
:::js
`createAgent()` 使用 [LangGraph](/oss/langgraph/overview) 构建了一个基于 **图 (graph)** 的智能体运行时。图由节点 (nodes，即步骤) 和边 (edges，即连接) 组成，定义了智能体处理信息的方式。智能体在这个图中移动，执行诸如模型节点 (调用模型)、工具节点 (执行工具) 或中间件等节点。
:::

了解更多关于 [Graph API](/oss/langgraph/graph-api) 的信息。

</Info>

## 核心组件 (Core components)

### 模型 (Model)

[模型 (model)](/oss/langchain/models) 是智能体的推理引擎。它可以通过多种方式指定，支持静态和动态模型选择。

#### 静态模型 (Static model)

静态模型在创建智能体时配置一次，并在整个执行过程中保持不变。这是最常见且最直接的方法。

要从 <Tooltip tip="遵循 `provider:model` 格式的字符串 (例如 openai:gpt-5)" cta="查看映射" href="https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model(model)">模型标识符字符串 (model identifier string)</Tooltip> 初始化静态模型：

:::python
```python wrap
from langchain.agents import create_agent

agent = create_agent("openai:gpt-5", tools=tools)
```
:::
:::js
```ts wrap
import { createAgent } from "langchain";

const agent = createAgent({
  model: "openai:gpt-5",
  tools: []
});
```
:::

:::python
<Tip>
    模型标识符字符串支持自动推断 (例如，`"gpt-5"` 将被推断为 `"openai:gpt-5"`)。请参阅 @[reference][init_chat_model(model)] 以查看完整的模型标识符字符串映射列表。
</Tip>

如果需要对模型配置进行更多控制，可以直接使用提供者包 (provider package) 初始化模型实例。在本示例中，我们使用 @[`ChatOpenAI`]。有关其他可用的聊天模型类，请参阅 [聊天模型 (Chat models)](/oss/integrations/chat)。

```python wrap
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI

model = ChatOpenAI(
    model="gpt-5",
    temperature=0.1,
    max_tokens=1000,
    timeout=30
    # ... (其他参数)
)
agent = create_agent(model, tools=tools)
```

模型实例让您可以完全控制配置。当您需要设置特定的 [参数 (parameters)](/oss/langchain/models#parameters)（如 `temperature`、`max_tokens`、`timeouts`、`base_url` 以及其他提供者特定的设置）时，请使用它们。请参阅 [参考文档](/oss/integrations/providers/all_providers) 以查看模型上可用的参数和方法。
:::
:::js
模型标识符字符串使用 `provider:model` 格式 (例如 `"openai:gpt-5"`)。您可能希望对模型配置有更多控制，在这种情况下，可以直接使用提供者包初始化模型实例：

```ts wrap
import { createAgent } from "langchain";
import { ChatOpenAI } from "@langchain/openai";

const model = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0.1,
  maxTokens: 1000,
  timeout: 30
});

const agent = createAgent({
  model,
  tools: []
});
```

模型实例让您可以完全控制配置。当您需要设置特定参数，如 `temperature`、`max_tokens`、`timeouts`，或配置 API 密钥、`base_url` 以及其他提供者特定的设置时，请使用它们。请参阅 [API 参考](/oss/integrations/providers/) 以查看模型上可用的参数和方法。
:::

#### 动态模型 (Dynamic model)

动态模型在 <Tooltip tip="智能体的执行环境，包含在整个智能体执行过程中持久存在的不可变配置和上下文数据 (例如：用户 ID、会话详情或应用程序特定的配置)。">运行时 (runtime)</Tooltip> 根据当前的 <Tooltip tip="流经智能体执行的数据，包括消息、自定义字段以及在处理过程中需要跟踪并可能修改的任何信息 (例如：用户偏好或工具使用统计)。">状态 (state)</Tooltip> 和上下文进行选择。这可以实现复杂的路由逻辑和成本优化。

:::python

要使用动态模型，请使用 @[`@wrap_model_call`] 装饰器创建中间件 (middleware)，以修改请求中的模型：

```python
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent
from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse


basic_model = ChatOpenAI(model="gpt-4o-mini")
advanced_model = ChatOpenAI(model="gpt-4o")

@wrap_model_call
def dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:
    """根据对话复杂度选择模型。"""
    message_count = len(request.state["messages"])

    if message_count > 10:
        # 对较长的对话使用高级模型
        model = advanced_model
    else:
        model = basic_model

    return handler(request.override(model=model))

agent = create_agent(
    model=basic_model,  # 默认模型
    tools=tools,
    middleware=[dynamic_model_selection]
)
```

<Warning>
在使用结构化输出 (structured output) 时，不支持预绑定模型 (即已经调用过 @[`bind_tools`][BaseChatModel.bind_tools] 的模型)。如果您需要结合结构化输出进行动态模型选择，请确保传递给中间件的模型是未进行预绑定的。
</Warning>

:::
:::js

要使用动态模型，请使用 `wrapModelCall` 创建中间件，以修改请求中的模型：

```ts
import { ChatOpenAI } from "@langchain/openai";
import { createAgent, createMiddleware } from "langchain";

const basicModel = new ChatOpenAI({ model: "gpt-4o-mini" });
const advancedModel = new ChatOpenAI({ model: "gpt-4o" });

const dynamicModelSelection = createMiddleware({
  name: "DynamicModelSelection",
  wrapModelCall: (request, handler) => {
    // 根据对话复杂度选择模型
    const messageCount = request.messages.length;

    return handler({
        ...request,
        model: messageCount > 10 ? advancedModel : basicModel,
    });
  },
});

const agent = createAgent({
  model: "gpt-4o-mini", // 基础模型 (当 messageCount ≤ 10 时使用)
  tools,
  middleware: [dynamicModelSelection],
});
```

有关中间件和高级模式的更多详细信息，请参阅 [中间件文档](/oss/langchain/middleware)。
:::

<Tip>
有关模型配置的详细信息，请参阅 [模型 (Models)](/oss/langchain/models)。有关动态模型选择模式，请参阅 [中间件中的动态模型](/oss/langchain/middleware#dynamic-model)。
</Tip>

### 工具 (Tools)

工具赋予了智能体执行动作的能力。智能体超越了简单的仅限模型的工具绑定，它可以实现：

- 按顺序执行多次工具调用 (由单次提示触发)
- 在适当时进行并行工具调用
- 基于先前结果的动态工具选择
- 工具重试逻辑和错误处理
- 跨工具调用的状态持久化

更多信息，请参阅 [工具 (Tools)](/oss/langchain/tools)。

#### 定义工具 (Defining tools)

将工具列表传递给智能体。

:::python

<Tip>
工具可以指定为普通的 Python 函数或 <Tooltip tip="一种可以暂停执行并在稍后恢复的方法">协程 (coroutines)</Tooltip>。

可以使用 [tool 装饰器](/oss/langchain/tools#create-tools) 来自定义工具名称、描述、参数模式和其他属性。
</Tip>

```python wrap
from langchain.tools import tool
from langchain.agents import create_agent


@tool
def search(query: str) -> str:
    """搜索信息。"""
    return f"关于 {query} 的搜索结果"

@tool
def get_weather(location: str) -> str:
    """获取指定位置的天气信息。"""
    return f"{location} 的天气：晴，72°F"

agent = create_agent(model, tools=[search, get_weather])
```
:::
:::js
```ts wrap
import * as z from "zod";
import { createAgent, tool } from "langchain";

const search = tool(
  ({ query }) => `关于 ${query} 的搜索结果`,
  {
    name: "search",
    description: "搜索信息",
    schema: z.object({
      query: z.string().describe("要搜索的查询内容"),
    }),
  }
);

const getWeather = tool(
  ({ location }) => `${location} 的天气：晴，72°F`,
  {
    name: "get_weather",
    description: "获取指定位置的天气信息",
    schema: z.object({
      location: z.string().describe("要获取天气的地点"),
    }),
  }
);

const agent = createAgent({
  model: "gpt-4o",
  tools: [search, getWeather],
});
```
:::

如果提供空的工具列表，智能体将仅由单个 LLM 节点组成，不具备工具调用能力。

#### 工具错误处理 (Tool error handling)

:::python

要自定义工具错误的处理方式，请使用 @[`@wrap_tool_call`] 装饰器创建中间件：

```python wrap
from langchain.agents import create_agent
from langchain.agents.middleware import wrap_tool_call
from langchain.messages import ToolMessage


@wrap_tool_call
def handle_tool_errors(request, handler):
    """使用自定义消息处理工具执行错误。"""
    try:
        return handler(request)
    except Exception as e:
        # 向模型返回自定义错误消息
        return ToolMessage(
            content=f"工具错误：请检查输入并重试。({str(e)})",
            tool_call_id=request.tool_call["id"]
        )

agent = create_agent(
    model="gpt-4o",
    tools=[search, get_weather],
    middleware=[handle_tool_errors]
)
```

当工具运行失败时，智能体会返回一个带有自定义错误消息的 @[`ToolMessage`]：

```python
[
    ...
    ToolMessage(
        content="工具错误：请检查输入并重试。(除以零错误)",
        tool_call_id="..."
    ),
    ...
]
```

:::
:::js

要自定义工具错误的处理方式，请在自定义中间件中使用 `wrapToolCall` 钩子 (hook)：

```ts wrap
import { createAgent, createMiddleware, ToolMessage } from "langchain";

const handleToolErrors = createMiddleware({
  name: "HandleToolErrors",
  wrapToolCall: async (request, handler) => {
    try {
      return await handler(request);
    } catch (error) {
      // 向模型返回自定义错误消息
      return new ToolMessage({
        content: `工具错误：请检查输入并重试。(${error})`,
        tool_call_id: request.toolCall.id!,
      });
    }
  },
});

const agent = createAgent({
  model: "gpt-4o",
  tools: [
    /* ... */
  ],
  middleware: [handleToolErrors],
});
```

当工具运行失败时，智能体会返回一个带有自定义错误消息的 @[`ToolMessage`]。
:::

#### ReAct 循环中的工具使用 (Tool use in the ReAct loop)

智能体遵循 ReAct ("推理 + 行动") 模式，在简短的推理步骤与针对性的工具调用之间交替进行，并将得到的观察结果反馈到后续决策中，直到能够给出最终答案。

<Accordion title="ReAct 循环示例">
**提示词 (Prompt)：** 识别当前最受欢迎的无线耳机并验证其库存情况。

```
================================ Human Message =================================

找出目前最受欢迎的无线耳机，并检查它们是否有货。
```

* **推理 (Reasoning)**：“流行度是随时变化的，我需要使用提供的搜索工具。”
* **行动 (Acting)**：调用 `search_products("wireless headphones")`

```
================================== Ai Message ==================================
工具调用：
  search_products (探索 ID: call_abc123)
  参数：
    query: wireless headphones
```
```
================================= Tool Message =================================

找到 5 款匹配“无线耳机”的产品。排名前 5 的结果：WH-1000XM5, ...
```

* **推理 (Reasoning)**：“在回答之前，我需要确认排名第一的产品的库存情况。”
* **行动 (Acting)**：调用 `check_inventory("WH-1000XM5")`

```
================================== Ai Message ==================================
工具调用：
  check_inventory (探索 ID: call_def456)
  参数：
    product_id: WH-1000XM5
```
```
================================= Tool Message =================================

产品 WH-1000XM5：库存 10 件
```

* **推理 (Reasoning)**：“我已经掌握了最受欢迎的型号及其库存状态。现在可以回答用户的问题了。”
* **行动 (Acting)**：输出最终答案。

```
================================== Ai Message ==================================

我找到了最受欢迎的无线耳机（型号 WH-1000XM5），目前库存充足（剩余 10 件）...
```
</Accordion>

<Tip>
要了解更多关于工具的信息，请参阅 [工具 (Tools)](/oss/langchain/tools)。
</Tip>

### 系统提示 (System prompt)

:::python
您可以通过提供提示词来塑造智能体处理任务的方式。@[`system_prompt`] 参数可以以字符串形式提供：
:::

:::js
您可以通过提供提示词来塑造智能体处理任务的方式。`systemPrompt` 参数可以以字符串形式提供：
:::

:::python
```python wrap
agent = create_agent(
    model,
    tools,
    system_prompt="你是一个得力的助手。请确保回答简洁准确。"
)
```
:::
:::js
```ts wrap
const agent = createAgent({
  model,
  tools,
  systemPrompt: "你是一个得力的助手。请确保回答简洁准确。",
});
```
:::

:::python
当未提供 @[`system_prompt`] 时，智能体将直接从消息中推断其任务。

@[`system_prompt`] 参数接受 `str` 或 @[`SystemMessage`]。使用 `SystemMessage` 可以让您更好地控制提示词结构，这对于特定提供者的功能（如 [Anthropic 的提示词缓存](/oss/integrations/chat/anthropic#prompt-caching)）非常有用：

```python wrap
from langchain.agents import create_agent
from langchain.messages import SystemMessage, HumanMessage

literary_agent = create_agent(
    model="anthropic:claude-sonnet-4-5",
    system_prompt=SystemMessage(
        content=[
            {
                "type": "text",
                "text": "你是一个负责分析文学作品的 AI 助手。",
            },
            {
                "type": "text",
                "text": "<《傲慢与偏见》的全文本内容>",
                "cache_control": {"type": "ephemeral"}
            }
        ]
    )
)

result = literary_agent.invoke(
    {"messages": [HumanMessage("分析《傲慢与偏见》的主要主题。")]}
)
```

带有 `{"type": "ephemeral"}` 的 `cache_control` 字段告知 Anthropic 缓存该内容块，从而减少使用相同系统提示词的重复请求的延迟和成本。
:::

:::js
当未提供 `systemPrompt` 时，智能体将直接从消息中推断其任务。

`systemPrompt` 参数接受 `string` 或 `SystemMessage`。使用 `SystemMessage` 可以让您更好地控制提示词结构，这对于特定提供者的功能（如 [Anthropic 的提示词缓存](/oss/integrations/chat/anthropic#prompt-caching)）非常有用：

```ts wrap
import { createAgent } from "langchain";
import { SystemMessage, HumanMessage } from "@langchain/core/messages";

const literaryAgent = createAgent({
  model: "anthropic:claude-sonnet-4-5",
  systemPrompt: new SystemMessage({
    content: [
      {
        type: "text",
        text: "你是一个负责分析文学作品的 AI 助手。",
      },
      {
        type: "text",
        text: "<《傲慢与偏见》的全文本内容>",
        cache_control: { type: "ephemeral" }
      }
    ]
  })
});

const result = await literaryAgent.invoke({
  messages: [new HumanMessage("分析《傲慢与偏见》的主要主题。")]
});
```

带有 `{ type: "ephemeral" }` 的 `cache_control` 字段告知 Anthropic 缓存该内容块，从而减少使用相同系统提示词的重复请求的延迟和成本。
:::

#### 动态系统提示 (Dynamic system prompt)

对于需要根据运行时上下文或智能体状态修改系统提示词的高级用例，您可以使用 [中间件 (middleware)](/oss/langchain/middleware)。

:::python

@[`@dynamic_prompt`] 装饰器可以创建根据模型请求生成系统提示词的中间件：

```python wrap
from typing import TypedDict

from langchain.agents import create_agent
from langchain.agents.middleware import dynamic_prompt, ModelRequest


class Context(TypedDict):
    user_role: str

@dynamic_prompt
def user_role_prompt(request: ModelRequest) -> str:
    """根据用户角色生成系统提示词。"""
    user_role = request.runtime.context.get("user_role", "user")
    base_prompt = "你是一个得力的助手。"

    if user_role == "expert":
        return f"{base_prompt} 请提供详细的技术性回答。"
    elif user_role == "beginner":
        return f"{base_prompt} 请用通俗易懂的方式解释概念，避免使用专业术语。"

    return base_prompt

agent = create_agent(
    model="gpt-4o",
    tools=[web_search],
    middleware=[user_role_prompt],
    context_schema=Context
)

# 系统提示词将根据上下文动态设置
result = agent.invoke(
    {"messages": [{"role": "user", "content": "解释机器学习"}]},
    context={"user_role": "expert"}
)
```
:::

:::js
```typescript wrap
import * as z from "zod";
import { createAgent, dynamicSystemPromptMiddleware } from "langchain";

const contextSchema = z.object({
  userRole: z.enum(["expert", "beginner"]),
});

const agent = createAgent({
  model: "gpt-4o",
  tools: [/* ... */],
  contextSchema,
  middleware: [
    dynamicSystemPromptMiddleware<z.infer<typeof contextSchema>>((state, runtime) => {
      const userRole = runtime.context.userRole || "user";
      const basePrompt = "你是一个得力的助手。";

      if (userRole === "expert") {
        return `${basePrompt} 请提供详细的技术性回答。`;
      } else if (userRole === "beginner") {
        return `${basePrompt} 请用通俗易懂的方式解释概念，避免使用专业术语。`;
      }
      return basePrompt;
    }),
  ],
});

// 系统提示词将根据上下文动态设置
const result = await agent.invoke(
  { messages: [{ role: "user", content: "解释机器学习" }] },
  { context: { userRole: "expert" } }
);
```
:::

<Tip>
有关消息类型和格式的更多详细信息，请参阅 [消息 (Messages)](/oss/langchain/messages)。有关完整的中间件文档，请参阅 [中间件 (Middleware)](/oss/langchain/middleware)。
</Tip>

## 调用 (Invocation)

您可以通过向智能体的 [`状态 (State)`](/oss/langgraph/graph-api#state) 传递更新来调用它。所有智能体的状态都包含一个 [消息序列 (sequence of messages)](/oss/langgraph/use-graph-api#messagesstate)；要调用智能体，只需传递一条新消息：

:::python
```python
result = agent.invoke(
    {"messages": [{"role": "user", "content": "旧金山的天气怎么样？"}]}
)
```
:::
:::js
```typescript
await agent.invoke({
  messages: [{ role: "user", content: "旧金山的天气怎么样？" }],
})
```
:::

要从智能体流式获取步骤和/或 Token，请参阅 [流式传输 (streaming)](/oss/langchain/streaming) 指南。

此外，智能体遵循 LangGraph 的 [Graph API](/oss/langgraph/use-graph-api)，并支持所有相关方法，如 `stream` 和 `invoke`。

## 高级概念 (Advanced concepts)

### 结构化输出 (Structured output)

:::python

在某些情况下，您可能希望智能体以特定的格式返回输出。LangChain 通过 @[`response_format`][create_agent(response_format)] 参数提供结构化输出策略。

#### ToolStrategy

`ToolStrategy` 利用人工工具调用来生成结构化输出。这适用于任何支持工具调用的模型。当提供者原生的结构化输出（通过 [`ProviderStrategy`](#ProviderStrategy)）不可用或不可靠时，应使用 `ToolStrategy`。

```python wrap
from pydantic import BaseModel
from langchain.agents import create_agent
from langchain.agents.structured_output import ToolStrategy


class ContactInfo(BaseModel):
    name: str
    email: str
    phone: str

agent = create_agent(
    model="gpt-4o-mini",
    tools=[search_tool],
    response_format=ToolStrategy(ContactInfo)
)

result = agent.invoke({
    "messages": [{"role": "user", "content": "从以下内容提取联系信息：张三, zhangsan@example.com, (555) 123-4567"}]
})

result["structured_response"]
# ContactInfo(name='张三', email='zhangsan@example.com', phone='(555) 123-4567')
```

#### ProviderStrategy

`ProviderStrategy` 使用模型提供者原生的结构化输出生成功能。这种方式更可靠，但仅适用于支持原生结构化输出的提供者：

```python wrap
from langchain.agents.structured_output import ProviderStrategy

agent = create_agent(
    model="gpt-4o",
    response_format=ProviderStrategy(ContactInfo)
)
```

<Note>
自 `langchain 1.0` 起，不再支持直接传递模式（例如 `response_format=ContactInfo`）。您必须明确使用 `ToolStrategy` 或 `ProviderStrategy`。
</Note>

:::
:::js
在某些情况下，您可能希望智能体以特定的格式返回输出。LangChain 提供了一种简单且通用的方式，通过 `responseFormat` 参数来实现这一点。

```ts wrap
import * as z from "zod";
import { createAgent } from "langchain";

const ContactInfo = z.object({
  name: z.string(),
  email: z.string(),
  phone: z.string(),
});

const agent = createAgent({
  model: "gpt-4o",
  responseFormat: ContactInfo,
});

const result = await agent.invoke({
  messages: [
    {
      role: "user",
      content: "从以下内容提取联系信息：张三, zhangsan@example.com, (555) 123-4567",
    },
  ],
});

console.log(result.structuredResponse);
// {
//   name: '张三',
//   email: 'zhangsan@example.com',
//   phone: '(555) 123-4567'
// }
```
:::
<Tip>
    有关结构化输出的学习，请参阅 [结构化输出 (Structured output)](/oss/langchain/structured-output)。
</Tip>

### 记忆 (Memory)

智能体通过消息状态自动维护对话历史记录。您还可以配置智能体使用自定义状态模式 (state schema)，以便在对话期间记住额外的信息。

存储在状态中的信息可以被视为智能体的 [短时记忆 (short-term memory)](/oss/langchain/short-term-memory)：

:::python

自定义状态模式必须作为 `TypedDict` 扩展 @[`AgentState`]。

定义自定义状态有两种方式：
1. 通过 [中间件 (middleware)](/oss/langchain/middleware)（推荐）
2. 通过 @[`create_agent`] 上的 @[`state_schema`]

#### 通过中间件定义状态

当您的自定义状态需要被连接到该中间件的特定中间件钩子和工具访问时，请使用中间件来定义它。

```python
from langchain.agents import AgentState
from langchain.agents.middleware import AgentMiddleware
from typing import Any


class CustomState(AgentState):
    user_preferences: dict

class CustomMiddleware(AgentMiddleware):
    state_schema = CustomState
    tools = [tool1, tool2]

    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:
        ...

agent = create_agent(
    model,
    tools=tools,
    middleware=[CustomMiddleware()]
)

# 智能体现在可以跟踪消息之外的额外状态
result = agent.invoke({
    "messages": [{"role": "user", "content": "我更喜欢技术性的解释"}],
    "user_preferences": {"style": "technical", "verbosity": "detailed"},
})
```

#### 通过 `state_schema` 定义状态

使用 @[`state_schema`] 参数作为快捷方式来定义仅在工具中使用的自定义状态。

```python
from langchain.agents import AgentState


class CustomState(AgentState):
    user_preferences: dict

agent = create_agent(
    model,
    tools=[tool1, tool2],
    state_schema=CustomState
)
# 智能体现在可以跟踪消息之外的额外状态
result = agent.invoke({
    "messages": [{"role": "user", "content": "我更喜欢技术性的解释"}],
    "user_preferences": {"style": "technical", "verbosity": "detailed"},
})
```

<Note>
自 `langchain 1.0` 起，自定义状态模式 **必须** 是 `TypedDict` 类型。不再支持 Pydantic 模型和数据类。有关更多详细信息，请参阅 [v1 迁移指南](/oss/migrate/langchain-v1#state-type-restrictions)。
</Note>

:::
:::js
```ts wrap
import * as z from "zod";
import { MessagesZodState } from "@langchain/langgraph";
import { createAgent } from "langchain";
import { type BaseMessage } from "@langchain/core/messages";

const customAgentState = z.object({
  messages: MessagesZodState.shape.messages,
  userPreferences: z.record(z.string(), z.string()),
});

const CustomAgentState = createAgent({
  model: "gpt-4o",
  tools: [],
  stateSchema: customAgentState,
});
```
:::

:::python
<Note>
    通过中间件定义自定义状态优于直接在 @[`create_agent`] 上通过 @[`state_schema`] 定义，因为它允许您让状态扩展根据相关的中间件和工具在概念上保持作用域明确。

    为了向后兼容，@[`create_agent`] 仍支持 @[`state_schema`]。
</Note>
:::

<Tip>
    要了解有关记忆的更多信息，请参阅 [记忆 (Memory)](/oss/concepts/memory)。有关实现跨会话持久化的长时记忆的信息，请参阅 [长时记忆 (Long-term memory)](/oss/langchain/long-term-memory)。
</Tip>

### 流式传输 (Streaming)

我们已经了解了如何通过 `invoke` 调用智能体以获取最终回答。如果智能体执行多个步骤，这可能需要一段时间。为了展示中间进展，我们可以流式传回产生的消息。

:::python
```python
for chunk in agent.stream({
    "messages": [{"role": "user", "content": "搜索 AI 新闻并总结发现"}]
}, stream_mode="values"):
    # 每个 chunk 包含该时间点的完整状态
    latest_message = chunk["messages"][-1]
    if latest_message.content:
        print(f"智能体：{latest_message.content}")
    elif latest_message.tool_calls:
        print(f"正在调用工具：{[tc['name'] for tc in latest_message.tool_calls]}")
```
:::
:::js
```ts
const stream = await agent.stream(
  {
    messages: [{
      role: "user",
      content: "搜索 AI 新闻并总结发现"
    }],
  },
  { streamMode: "values" }
);

for await (const chunk of stream) {
  // 每个 chunk 包含该时间点的完整状态
  const latestMessage = chunk.messages.at(-1);
  if (latestMessage?.content) {
    console.log(`智能体：${latestMessage.content}`);
  } else if (latestMessage?.tool_calls) {
    const toolCallNames = latestMessage.tool_calls.map((tc) => tc.name);
    console.log(`正在调用工具：${toolCallNames.join(", ")}`);
  }
}
```
:::

<Tip>
有关流式传输的更多详细信息，请参阅 [流式传输 (Streaming)](/oss/langchain/streaming)。
</Tip>

### 中间件 (Middleware)

[中间件 (Middleware)](/oss/langchain/middleware) 提供了强大的可扩展性，用于在执行的不同阶段自定义智能体行为。您可以使用中间件来：

- 在调用模型之前处理状态（例如：消息修剪、上下文注入）
- 修改或验证模型的响应（例如：护栏设置、内容过滤）
- 使用自定义逻辑处理工具执行错误
- 根据状态或上下文实现动态模型选择
- 添加自定义日志、监控或分析

中间件无缝集成到智能体的执行中，允许您在关键点拦截并修改数据流，而无需更改核心智能体逻辑。

:::python
<Tip>
有关包含 @[`@before_model`]、@[`@after_model`] 和 @[`@wrap_tool_call`] 等装饰器的完整中间件文档，请参阅 [中间件 (Middleware)](/oss/langchain/middleware)。
</Tip>
:::

:::js
<Tip>
有关包含 `beforeModel`、`afterModel` 和 `wrapToolCall` 等钩子的完整中间件文档，请参阅 [中间件 (Middleware)](/oss/langchain/middleware)。
</Tip>
:::
