---
title: æ¦‚è¿°
description: å®æ—¶æµå¼ä¼ è¾“æ™ºèƒ½ä½“è¿è¡Œæ›´æ–°
---
LangChain å®ç°äº†ä¸€å¥—æµå¼ä¼ è¾“ç³»ç»Ÿï¼Œç”¨äºå®æ—¶å‘ˆç°æ›´æ–°ã€‚

æµå¼ä¼ è¾“å¯¹äºæå‡åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ„å»ºçš„åº”ç”¨ç¨‹åºçš„å“åº”èƒ½åŠ›è‡³å…³é‡è¦ã€‚é€šè¿‡é€æ­¥æ˜¾ç¤ºè¾“å‡ºï¼Œç”šè‡³åœ¨å®Œæ•´å“åº”å‡†å¤‡å°±ç»ªä¹‹å‰ï¼Œæµå¼ä¼ è¾“èƒ½æ˜¾è‘—æ”¹å–„ç”¨æˆ·ä½“éªŒï¼ˆUXï¼‰ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤§è¯­è¨€æ¨¡å‹çš„å»¶è¿Ÿæ—¶ã€‚

## æ¦‚è¿°

LangChain çš„æµå¼ä¼ è¾“ç³»ç»Ÿå…è®¸æ‚¨å°†æ™ºèƒ½ä½“ï¼ˆagentï¼‰è¿è¡Œè¿‡ç¨‹ä¸­çš„å®æ—¶åé¦ˆå‘ˆç°ç»™æ‚¨çš„åº”ç”¨ç¨‹åºã€‚

ä½¿ç”¨ LangChain æµå¼ä¼ è¾“å¯ä»¥å®ç°çš„åŠŸèƒ½ï¼š

* <Icon icon="brain" size={16} /> [**æµå¼ä¼ è¾“æ™ºèƒ½ä½“è¿›åº¦**](#agent-progress) â€” åœ¨æ¯æ¬¡æ™ºèƒ½ä½“æ­¥éª¤åè·å–çŠ¶æ€æ›´æ–°ã€‚
* <Icon icon="square-binary" size={16} /> [**æµå¼ä¼ è¾“å¤§è¯­è¨€æ¨¡å‹è¯å…ƒ**](#llm-tokens) â€” åœ¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆè¯å…ƒæ—¶è¿›è¡Œæµå¼ä¼ è¾“ã€‚
* <Icon icon="table" size={16} /> [**æµå¼ä¼ è¾“è‡ªå®šä¹‰æ›´æ–°**](#custom-updates) â€” å‘å‡ºç”¨æˆ·å®šä¹‰çš„ä¿¡å·ï¼ˆä¾‹å¦‚ï¼Œ`"å·²è·å– 10/100 æ¡è®°å½•"`ï¼‰ã€‚
* <Icon icon="layer-plus" size={16} /> [**æµå¼ä¼ è¾“å¤šç§æ¨¡å¼**](#stream-multiple-modes) â€” ä» `updates`ï¼ˆæ™ºèƒ½ä½“è¿›åº¦ï¼‰ã€`messages`ï¼ˆå¤§è¯­è¨€æ¨¡å‹è¯å…ƒ + å…ƒæ•°æ®ï¼‰æˆ– `custom`ï¼ˆä»»æ„ç”¨æˆ·æ•°æ®ï¼‰ä¸­é€‰æ‹©ã€‚

æ›´å¤šç«¯åˆ°ç«¯ç¤ºä¾‹ï¼Œè¯·å‚é˜…ä¸‹é¢çš„[å¸¸è§æ¨¡å¼](#common-patterns)éƒ¨åˆ†ã€‚

## æ”¯æŒçš„æµæ¨¡å¼

:::python
å°†ä¸€ä¸ªæˆ–å¤šä¸ªä»¥ä¸‹æµæ¨¡å¼ä½œä¸ºåˆ—è¡¨ä¼ é€’ç»™ @[`stream`][CompiledStateGraph.stream] æˆ– @[`astream`][CompiledStateGraph.astream] æ–¹æ³•ï¼š
:::

:::js
å°†ä¸€ä¸ªæˆ–å¤šä¸ªä»¥ä¸‹æµæ¨¡å¼ä½œä¸ºåˆ—è¡¨ä¼ é€’ç»™ @[`stream`][CompiledStateGraph.stream] æ–¹æ³•ï¼š
:::

| æ¨¡å¼        | æè¿°                                                                                                 |
| ----------- | ---------------------------------------------------------------------------------------------------- |
| `updates`   | åœ¨æ¯æ¬¡æ™ºèƒ½ä½“æ­¥éª¤åæµå¼ä¼ è¾“çŠ¶æ€æ›´æ–°ã€‚å¦‚æœåœ¨åŒä¸€æ­¥éª¤ä¸­è¿›è¡Œäº†å¤šæ¬¡æ›´æ–°ï¼ˆä¾‹å¦‚ï¼Œè¿è¡Œäº†å¤šä¸ªèŠ‚ç‚¹ï¼‰ï¼Œè¿™äº›æ›´æ–°å°†åˆ†åˆ«æµå¼ä¼ è¾“ã€‚ |
| `messages`  | ä»ä»»ä½•è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹çš„å›¾èŠ‚ç‚¹æµå¼ä¼ è¾“ `(token, metadata)` å…ƒç»„ã€‚ |
| `custom`    | ä½¿ç”¨æµå†™å…¥å™¨ä»å›¾èŠ‚ç‚¹å†…éƒ¨æµå¼ä¼ è¾“è‡ªå®šä¹‰æ•°æ®ã€‚ |

## æ™ºèƒ½ä½“è¿›åº¦

:::python
è¦æµå¼ä¼ è¾“æ™ºèƒ½ä½“è¿›åº¦ï¼Œè¯·ä½¿ç”¨ @[`stream`][CompiledStateGraph.stream] æˆ– @[`astream`][CompiledStateGraph.astream] æ–¹æ³•å¹¶è®¾ç½® `stream_mode="updates"`ã€‚è¿™ä¼šåœ¨æ¯æ¬¡æ™ºèƒ½ä½“æ­¥éª¤åå‘å‡ºä¸€ä¸ªäº‹ä»¶ã€‚
:::

:::js
è¦æµå¼ä¼ è¾“æ™ºèƒ½ä½“è¿›åº¦ï¼Œè¯·ä½¿ç”¨ @[`stream`][CompiledStateGraph.stream] æ–¹æ³•å¹¶è®¾ç½® `streamMode: "updates"`ã€‚è¿™ä¼šåœ¨æ¯æ¬¡æ™ºèƒ½ä½“æ­¥éª¤åå‘å‡ºä¸€ä¸ªäº‹ä»¶ã€‚
:::

ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æœ‰ä¸€ä¸ªè°ƒç”¨ä¸€æ¬¡å·¥å…·çš„æ™ºèƒ½ä½“ï¼Œæ‚¨åº”è¯¥ä¼šçœ‹åˆ°ä»¥ä¸‹æ›´æ–°ï¼š

* **å¤§è¯­è¨€æ¨¡å‹èŠ‚ç‚¹**ï¼šåŒ…å«å·¥å…·è°ƒç”¨è¯·æ±‚çš„ @[`AIMessage`]
* **å·¥å…·èŠ‚ç‚¹**ï¼šåŒ…å«æ‰§è¡Œç»“æœçš„ @[`ToolMessage`]
* **å¤§è¯­è¨€æ¨¡å‹èŠ‚ç‚¹**ï¼šæœ€ç»ˆçš„äººå·¥æ™ºèƒ½å“åº”

:::python

```python title="æµå¼ä¼ è¾“æ™ºèƒ½ä½“è¿›åº¦"
from langchain.agents import create_agent


def get_weather(city: str) -> str:
    """è·å–ç»™å®šåŸå¸‚çš„å¤©æ°”ã€‚"""

    return f"It's always sunny in {city}!"

agent = create_agent(
    model="gpt-5-nano",
    tools=[get_weather],
)
for chunk in agent.stream(  # [!code highlight]
    {"messages": [{"role": "user", "content": "What is the weather in SF?"}]},
    stream_mode="updates",
):
    for step, data in chunk.items():
        print(f"step: {step}")
        print(f"content: {data['messages'][-1].content_blocks}")
```

```shell title="è¾“å‡º"
step: model
content: [{'type': 'tool_call', 'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_OW2NYNsNSKhRZpjW0wm2Aszd'}]

step: tools
content: [{'type': 'text', 'text': "It's always sunny in San Francisco!"}]

step: model
content: [{'type': 'text', 'text': 'It's always sunny in San Francisco!'}]
```
:::

:::js
```typescript
import z from "zod";
import { createAgent, tool } from "langchain";

const getWeather = tool(
    async ({ city }) => {
        return `The weather in ${city} is always sunny!`;
    },
    {
        name: "get_weather",
        description: "è·å–ç»™å®šåŸå¸‚çš„å¤©æ°”ã€‚",
        schema: z.object({
        city: z.string(),
        }),
    }
);

const agent = createAgent({
    model: "gpt-5-nano",
    tools: [getWeather],
});

for await (const chunk of await agent.stream(
    { messages: [{ role: "user", content: "what is the weather in sf" }] },
    { streamMode: "updates" }
)) {
    const [step, content] = Object.entries(chunk)[0];
    console.log(`step: ${step}`);
    console.log(`content: ${JSON.stringify(content, null, 2)}`);
}
/**
 * step: model
 * content: {
 *   "messages": [
 *     {
 *       "kwargs": {
 *         // ...
 *         "tool_calls": [
 *           {
 *             "name": "get_weather",
 *             "args": {
 *               "city": "San Francisco"
 *             },
 *             "type": "tool_call",
 *             "id": "call_0qLS2Jp3MCmaKJ5MAYtr4jJd"
 *           }
 *         ],
 *         // ...
 *       }
 *     }
 *   ]
 * }
 * step: tools
 * content: {
 *   "messages": [
 *     {
 *       "kwargs": {
 *         "content": "The weather in San Francisco is always sunny!",
 *         "name": "get_weather",
 *         // ...
 *       }
 *     }
 *   ]
 * }
 * step: model
 * content: {
 *   "messages": [
 *     {
 *       "kwargs": {
 *         "content": "The latest update says: The weather in San Francisco is always sunny!\n\nIf you'd like real-time details (current temperature, humidity, wind, and today's forecast), I can pull the latest data for you. Want me to fetch that?",
 *         // ...
 *       }
 *     }
 *   ]
 * }
 */
```
:::

## LLM ä»¤ç‰Œ

:::python
è¦æµå¼ä¼ è¾“ç”± LLM ç”Ÿæˆçš„ä»¤ç‰Œï¼Œè¯·ä½¿ç”¨ `stream_mode="messages"`ã€‚ä¸‹é¢æ‚¨å¯ä»¥çœ‹åˆ°æ™ºèƒ½ä½“æµå¼ä¼ è¾“å·¥å…·è°ƒç”¨å’Œæœ€ç»ˆå“åº”çš„è¾“å‡ºã€‚

```python title="æµå¼ä¼ è¾“ LLM ä»¤ç‰Œ"
from langchain.agents import create_agent


def get_weather(city: str) -> str:
    """è·å–ç»™å®šåŸå¸‚çš„å¤©æ°”ã€‚"""

    return f"It's always sunny in {city}!"

agent = create_agent(
    model="gpt-5-nano",
    tools=[get_weather],
)
for token, metadata in agent.stream(  # [!code highlight]
    {"messages": [{"role": "user", "content": "What is the weather in SF?"}]},
    stream_mode="messages",
):
    print(f"node: {metadata['langgraph_node']}")
    print(f"content: {token.content_blocks}")
    print("\n")
```

```shell title="è¾“å‡º" expandable
node: model
content: [{'type': 'tool_call_chunk', 'id': 'call_vbCyBcP8VuneUzyYlSBZZsVa', 'name': 'get_weather', 'args': '', 'index': 0}]


node: model
content: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': '{"', 'index': 0}]


node: model
content: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': 'city', 'index': 0}]


node: model
content: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': '":"', 'index': 0}]


node: model
content: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': 'San', 'index': 0}]


node: model
content: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': ' Francisco', 'index': 0}]


node: model
content: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': '"}', 'index': 0}]


node: model
content: []


node: tools
content: [{'type': 'text', 'text': "It's always sunny in San Francisco!"}]


node: model
content: []


node: model
content: [{'type': 'text', 'text': 'Here'}]


node: model
content: [{'type': 'text', 'text': ''s'}]


node: model
content: [{'type': 'text', 'text': ' what'}]


node: model
content: [{'type': 'text', 'text': ' I'}]


node: model
content: [{'type': 'text', 'text': ' got'}]


node: model
content: [{'type': 'text', 'text': ':'}]


node: model
content: [{'type': 'text', 'text': ' "'}]


node: model
content: [{'type': 'text', 'text': "It's"}]


node: model
content: [{'type': 'text', 'text': ' always'}]
```

èŠ‚ç‚¹ï¼šæ¨¡å‹
å†…å®¹ï¼š[{'type': 'text', 'text': ' sunny'}]

èŠ‚ç‚¹ï¼šæ¨¡å‹
å†…å®¹ï¼š[{'type': 'text', 'text': ' in'}]

èŠ‚ç‚¹ï¼šæ¨¡å‹
å†…å®¹ï¼š[{'type': 'text', 'text': ' San'}]

èŠ‚ç‚¹ï¼šæ¨¡å‹
å†…å®¹ï¼š[{'type': 'text', 'text': ' Francisco'}]

èŠ‚ç‚¹ï¼šæ¨¡å‹
å†…å®¹ï¼š[{'type': 'text', 'text': '!"\n\n'}]
```
:::

:::js
è¦åœ¨ LLM ç”Ÿæˆä»¤ç‰Œæ—¶æµå¼ä¼ è¾“å®ƒä»¬ï¼Œè¯·ä½¿ç”¨ `streamMode: "messages"`ï¼š

```typescript
import z from "zod";
import { createAgent, tool } from "langchain";

const getWeather = tool(
    async ({ city }) => {
        return `The weather in ${city} is always sunny!`;
    },
    {
        name: "get_weather",
        description: "Get weather for a given city.",
        schema: z.object({
        city: z.string(),
        }),
    }
);

const agent = createAgent({
    model: "gpt-4o-mini",
    tools: [getWeather],
});

for await (const [token, metadata] of await agent.stream(
    { messages: [{ role: "user", content: "what is the weather in sf" }] },
    { streamMode: "messages" }
)) {
    console.log(`node: ${metadata.langgraph_node}`);
    console.log(`content: ${JSON.stringify(token.contentBlocks, null, 2)}`);
}
```
:::

## è‡ªå®šä¹‰æ›´æ–°

:::python
è¦åœ¨å·¥å…·æ‰§è¡Œæ—¶æµå¼ä¼ è¾“æ›´æ–°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ @[`get_stream_writer`]ã€‚

```python title="æµå¼ä¼ è¾“è‡ªå®šä¹‰æ›´æ–°"
from langchain.agents import create_agent
from langgraph.config import get_stream_writer  # [!code highlight]


def get_weather(city: str) -> str:
    """Get weather for a given city."""
    writer = get_stream_writer()  # [!code highlight]
    # æµå¼ä¼ è¾“ä»»æ„æ•°æ®
    writer(f"Looking up data for city: {city}")
    writer(f"Acquired data for city: {city}")
    return f"It's always sunny in {city}!"

agent = create_agent(
    model="claude-sonnet-4-5-20250929",
    tools=[get_weather],
)

for chunk in agent.stream(
    {"messages": [{"role": "user", "content": "What is the weather in SF?"}]},
    stream_mode="custom"  # [!code highlight]
):
    print(chunk)
```

```shell title="è¾“å‡º"
Looking up data for city: San Francisco
Acquired data for city: San Francisco
```

<Note>
    å¦‚æœæ‚¨åœ¨å·¥å…·å†…éƒ¨æ·»åŠ  @[`get_stream_writer`]ï¼Œæ‚¨å°†æ— æ³•åœ¨ LangGraph æ‰§è¡Œä¸Šä¸‹æ–‡ä¹‹å¤–è°ƒç”¨è¯¥å·¥å…·ã€‚
</Note>
:::

:::js
è¦åœ¨å·¥å…·æ‰§è¡Œæ—¶æµå¼ä¼ è¾“æ›´æ–°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨é…ç½®ä¸­çš„ `writer` å‚æ•°ã€‚

```typescript
import z from "zod";
import { tool, createAgent } from "langchain";
import { LangGraphRunnableConfig } from "@langchain/langgraph";

const getWeather = tool(
    async (input, config: LangGraphRunnableConfig) => {
        // æµå¼ä¼ è¾“ä»»æ„æ•°æ®
        config.writer?.(`Looking up data for city: ${input.city}`);
        // ... è·å–åŸå¸‚æ•°æ®
        config.writer?.(`Acquired data for city: ${input.city}`);
        return `It's always sunny in ${input.city}!`;
    },
    {
        name: "get_weather",
        description: "Get weather for a given city.",
        schema: z.object({
        city: z.string().describe("The city to get weather for."),
        }),
    }
);

const agent = createAgent({
    model: "gpt-4o-mini",
    tools: [getWeather],
});

for await (const chunk of await agent.stream(
    { messages: [{ role: "user", content: "what is the weather in sf" }] },
    { streamMode: "custom" }
)) {
    console.log(chunk);
}
```

```shell title="è¾“å‡º"
Looking up data for city: San Francisco
Acquired data for city: San Francisco
```

<Note>
    å¦‚æœæ‚¨å°† `writer` å‚æ•°æ·»åŠ åˆ°æ‚¨çš„å·¥å…·ä¸­ï¼Œåœ¨æ²¡æœ‰æä¾› writer å‡½æ•°çš„æƒ…å†µä¸‹ï¼Œæ‚¨å°†æ— æ³•åœ¨ LangGraph æ‰§è¡Œä¸Šä¸‹æ–‡ä¹‹å¤–è°ƒç”¨è¯¥å·¥å…·ã€‚
</Note>
:::

## æµå¼ä¼ è¾“å¤šç§æ¨¡å¼

:::python
æ‚¨å¯ä»¥é€šè¿‡å°†æµæ¨¡å¼ä½œä¸ºåˆ—è¡¨ä¼ é€’æ¥æŒ‡å®šå¤šç§æµå¼ä¼ è¾“æ¨¡å¼ï¼š`stream_mode=["updates", "custom"]`ã€‚

æµå¼ä¼ è¾“çš„è¾“å‡ºå°†æ˜¯ `(mode, chunk)` å…ƒç»„ï¼Œå…¶ä¸­ `mode` æ˜¯æµæ¨¡å¼çš„åç§°ï¼Œ`chunk` æ˜¯è¯¥æ¨¡å¼æµå¼ä¼ è¾“çš„æ•°æ®ã€‚

```python title="æµå¼ä¼ è¾“å¤šç§æ¨¡å¼"
from langchain.agents import create_agent
from langgraph.config import get_stream_writer


def get_weather(city: str) -> str:
    """è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ã€‚"""
    writer = get_stream_writer()
    writer(f"æ­£åœ¨æŸ¥æ‰¾åŸå¸‚æ•°æ®: {city}")
    writer(f"å·²è·å–åŸå¸‚æ•°æ®: {city}")
    return f"It's always sunny in {city}!"

agent = create_agent(
    model="gpt-5-nano",
    tools=[get_weather],
)

for stream_mode, chunk in agent.stream(  # [!code highlight]
    {"messages": [{"role": "user", "content": "What is the weather in SF?"}]},
    stream_mode=["updates", "custom"]
):
    print(f"stream_mode: {stream_mode}")
    print(f"content: {chunk}")
    print("\n")
```

```shell title="è¾“å‡º"
stream_mode: updates
content: {'model': {'messages': [AIMessage(content='', response_metadata={'token_usage': {'completion_tokens': 280, 'prompt_tokens': 132, 'total_tokens': 412, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C9tlgBzGEbedGYxZ0rTCz5F7OXpL7', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--480c07cb-e405-4411-aa7f-0520fddeed66-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_KTNQIftMrl9vgNwEfAJMVu7r', 'type': 'tool_call'}], usage_metadata={'input_tokens': 132, 'output_tokens': 280, 'total_tokens': 412, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 256}})]}}


stream_mode: custom
content: Looking up data for city: San Francisco


stream_mode: custom
content: Acquired data for city: San Francisco


stream_mode: updates
content: {'tools': {'messages': [ToolMessage(content="It's always sunny in San Francisco!", name='get_weather', tool_call_id='call_KTNQIftMrl9vgNwEfAJMVu7r')]}}


stream_mode: updates
content: {'model': {'messages': [AIMessage(content='San Francisco weather: It's always sunny in San Francisco!\n\n', response_metadata={'token_usage': {'completion_tokens': 764, 'prompt_tokens': 168, 'total_tokens': 932, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 704, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C9tljDFVki1e1haCyikBptAuXuHYG', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--acbc740a-18fe-4a14-8619-da92a0d0ee90-0', usage_metadata={'input_tokens': 168, 'output_tokens': 764, 'total_tokens': 932, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 704}})]}}
```
:::

:::js
ä½ å¯ä»¥é€šè¿‡å°† `streamMode` ä½œä¸ºæ•°ç»„ä¼ é€’æ¥æŒ‡å®šå¤šç§æµå¼ä¼ è¾“æ¨¡å¼ï¼š`streamMode: ["updates", "messages", "custom"]`ã€‚

æµå¼è¾“å‡ºçš„ç»“æœå°†æ˜¯ `[mode, chunk]` å½¢å¼çš„å…ƒç»„ï¼Œå…¶ä¸­ `mode` æ˜¯æµæ¨¡å¼çš„åç§°ï¼Œ`chunk` æ˜¯è¯¥æ¨¡å¼æµå¼ä¼ è¾“çš„æ•°æ®ã€‚

```typescript
import z from "zod";
import { tool, createAgent } from "langchain";
import { LangGraphRunnableConfig } from "@langchain/langgraph";

const getWeather = tool(
    async (input, config: LangGraphRunnableConfig) => {
        // æµå¼ä¼ è¾“ä»»æ„æ•°æ®
        config.writer?.(`Looking up data for city: ${input.city}`);
        // ... è·å–åŸå¸‚æ•°æ®
        config.writer?.(`Acquired data for city: ${input.city}`);
        return `It's always sunny in ${input.city}!`;
    },
    {
        name: "get_weather",
        description: "Get weather for a given city.",
        schema: z.object({
        city: z.string().describe("The city to get weather for."),
        }),
    }
);

const agent = createAgent({
    model: "gpt-4o-mini",
    tools: [getWeather],
});

for await (const [streamMode, chunk] of await agent.stream(
    { messages: [{ role: "user", content: "what is the weather in sf" }] },
    { streamMode: ["updates", "messages", "custom"] }
)) {
    console.log(`${streamMode}: ${JSON.stringify(chunk, null, 2)}`);
}
```
:::

:::python
## å¸¸è§æ¨¡å¼

ä»¥ä¸‹æ˜¯å±•ç¤ºæµå¼ä¼ è¾“å¸¸è§ç”¨ä¾‹çš„ç¤ºä¾‹ã€‚

### æµå¼ä¼ è¾“å·¥å…·è°ƒç”¨

æ‚¨å¯èƒ½å¸Œæœ›åŒæ—¶æµå¼ä¼ è¾“ï¼š

1. ç”Ÿæˆ[å·¥å…·è°ƒç”¨](/oss/langchain/models#tool-calling)æ—¶çš„éƒ¨åˆ† JSON
2. å·²æ‰§è¡Œå®Œæ¯•çš„ã€è§£æåçš„å®Œæ•´å·¥å…·è°ƒç”¨

æŒ‡å®š [`stream_mode="messages"`](#llm-tokens) å°†æµå¼ä¼ è¾“æ™ºèƒ½ä½“ä¸­æ‰€æœ‰ LLM è°ƒç”¨ç”Ÿæˆçš„å¢é‡[æ¶ˆæ¯å—](/oss/langchain/messages#streaming-and-chunks)ã€‚è¦è®¿é—®åŒ…å«å·²è§£æå·¥å…·è°ƒç”¨çš„å®Œæ•´æ¶ˆæ¯ï¼š

1. å¦‚æœè¿™äº›æ¶ˆæ¯åœ¨[çŠ¶æ€](/oss/langchain/agents#memory)ä¸­è¢«è¿½è¸ªï¼ˆä¾‹å¦‚åœ¨ [`create_agent`](/oss/langchain/agents) çš„æ¨¡å‹èŠ‚ç‚¹ä¸­ï¼‰ï¼Œä½¿ç”¨ `stream_mode=["messages", "updates"]` é€šè¿‡[çŠ¶æ€æ›´æ–°](#agent-progress)è®¿é—®å®Œæ•´æ¶ˆæ¯ï¼ˆå¦‚ä¸‹æ‰€ç¤ºï¼‰ã€‚
2. å¦‚æœè¿™äº›æ¶ˆæ¯æœªåœ¨çŠ¶æ€ä¸­è¢«è¿½è¸ªï¼Œä½¿ç”¨[è‡ªå®šä¹‰æ›´æ–°](#custom-updates)æˆ–åœ¨æµå¼ä¼ è¾“å¾ªç¯æœŸé—´èšåˆå—ï¼ˆ[ä¸‹ä¸€èŠ‚](#accessing-completed-messages)ï¼‰ã€‚

<Note>
å¦‚æœæ‚¨çš„æ™ºèƒ½ä½“åŒ…å«å¤šä¸ª LLMï¼Œè¯·å‚è€ƒä¸‹é¢å…³äº[ä»å­æ™ºèƒ½ä½“æµå¼ä¼ è¾“](#streaming-from-sub-agents)çš„ç« èŠ‚ã€‚
</Note>

```python
from typing import Any

from langchain.agents import create_agent
from langchain.messages import AIMessage, AIMessageChunk, AnyMessage, ToolMessage


def get_weather(city: str) -> str:
    """è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ã€‚"""

    return f"It's always sunny in {city}!"


agent = create_agent("openai:gpt-5.2", tools=[get_weather])


def _render_message_chunk(token: AIMessageChunk) -> None:
    if token.text:
        print(token.text, end="|")
    if token.tool_call_chunks:
        print(token.tool_call_chunks)
    # æ³¨æ„ï¼šæ‰€æœ‰å†…å®¹éƒ½å¯é€šè¿‡ token.content_blocks è®¿é—®


def _render_completed_message(message: AnyMessage) -> None:
    if isinstance(message, AIMessage) and message.tool_calls:
        print(f"Tool calls: {message.tool_calls}")
    if isinstance(message, ToolMessage):
        print(f"Tool response: {message.content_blocks}")


input_message = {"role": "user", "content": "What is the weather in Boston?"}
for stream_mode, data in agent.stream(
    {"messages": [input_message]},
    stream_mode=["messages", "updates"],  # [!code highlight]
):
    if stream_mode == "messages":
        token, metadata = data
        if isinstance(token, AIMessageChunk):
            _render_message_chunk(token)  # [!code highlight]
    if stream_mode == "updates":
        for source, update in data.items():
            if source in ("model", "tools"):  # `source` æ•è·èŠ‚ç‚¹åç§°
                _render_completed_message(update["messages"][-1])  # [!code highlight]
```
```shell title="Output" expandable
[{'name': 'get_weather', 'args': '', 'id': 'call_D3Orjr89KgsLTZ9hTzYv7Hpf', 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '{"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'city', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '":"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'Boston', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '"}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
Tool calls: [{'name': 'get_weather', 'args': {'city': 'Boston'}, 'id': 'call_D3Orjr89KgsLTZ9hTzYv7Hpf', 'type': 'tool_call'}]
Tool response: [{'type': 'text', 'text': "It's always sunny in Boston!"}]
The| weather| in| Boston| is| **|sun|ny|**|.|
```

#### è®¿é—®å®Œæ•´æ¶ˆæ¯

<Note>
å¦‚æœå·²å®Œæˆçš„æ¶ˆæ¯åœ¨æ™ºèƒ½ä½“ï¼ˆagentï¼‰çš„[çŠ¶æ€](/oss/langchain/agents#memory)ä¸­è¢«è¿½è¸ªï¼Œä½ å¯ä»¥ä½¿ç”¨ `stream_mode=["messages", "updates"]`ï¼Œå¦‚[ä¸Šæ–‡](#streaming-tool-calls)æ‰€ç¤ºï¼Œä»¥ä¾¿åœ¨æµå¼ä¼ è¾“æœŸé—´è®¿é—®å·²å®Œæˆçš„æ¶ˆæ¯ã€‚
</Note>

åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå·²å®Œæˆçš„æ¶ˆæ¯ä¸ä¼šåæ˜ åœ¨[çŠ¶æ€æ›´æ–°](#agent-progress)ä¸­ã€‚å¦‚æœä½ èƒ½è®¿é—®æ™ºèƒ½ä½“å†…éƒ¨ï¼Œå¯ä»¥ä½¿ç”¨[è‡ªå®šä¹‰æ›´æ–°](#custom-updates)åœ¨æµå¼ä¼ è¾“æœŸé—´è®¿é—®è¿™äº›æ¶ˆæ¯ã€‚å¦åˆ™ï¼Œä½ å¯ä»¥åœ¨æµå¼å¾ªç¯ä¸­èšåˆæ¶ˆæ¯å—ï¼ˆè§ä¸‹æ–‡ï¼‰ã€‚

è€ƒè™‘ä»¥ä¸‹ç¤ºä¾‹ï¼Œæˆ‘ä»¬å°†ä¸€ä¸ª[æµå†™å…¥å™¨](#custom-updates)é›†æˆåˆ°ä¸€ä¸ªç®€åŒ–çš„[æŠ¤æ ä¸­é—´ä»¶](/oss/langchain/guardrails#after-agent-guardrails)ä¸­ã€‚è¯¥ä¸­é—´ä»¶æ¼”ç¤ºäº†é€šè¿‡å·¥å…·è°ƒç”¨æ¥ç”Ÿæˆç»“æ„åŒ–çš„â€œå®‰å…¨/ä¸å®‰å…¨â€è¯„ä¼°ï¼ˆä¹Ÿå¯ä»¥ä¸ºæ­¤ä½¿ç”¨[ç»“æ„åŒ–è¾“å‡º](/oss/langchain/models#structured-output)ï¼‰ï¼š
```python
from typing import Any, Literal

from langchain.agents.middleware import after_agent, AgentState
from langgraph.runtime import Runtime
from langchain.messages import AIMessage
from langchain.chat_models import init_chat_model
from langgraph.config import get_stream_writer  # [!code highlight]
from pydantic import BaseModel


class ResponseSafety(BaseModel):
    """è¯„ä¼°å“åº”ä¸ºå®‰å…¨æˆ–ä¸å®‰å…¨ã€‚"""
    evaluation: Literal["safe", "unsafe"]


safety_model = init_chat_model("openai:gpt-5.2")

@after_agent(can_jump_to=["end"])
def safety_guardrail(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
    """åŸºäºæ¨¡å‹çš„æŠ¤æ ï¼šä½¿ç”¨LLMè¯„ä¼°å“åº”å®‰å…¨æ€§ã€‚"""
    stream_writer = get_stream_writer()  # [!code highlight]
    # è·å–æ¨¡å‹å“åº”
    if not state["messages"]:
        return None

    last_message = state["messages"][-1]
    if not isinstance(last_message, AIMessage):
        return None

    # ä½¿ç”¨å¦ä¸€ä¸ªæ¨¡å‹è¯„ä¼°å®‰å…¨æ€§
    model_with_tools = safety_model.bind_tools([ResponseSafety], tool_choice="any")
    result = model_with_tools.invoke(
        [
            {
                "role": "system",
                "content": "è¯„ä¼°æ­¤AIå“åº”ä¸ºæ€»ä½“ä¸Šå®‰å…¨æˆ–ä¸å®‰å…¨ã€‚"
            },
            {
                "role": "user",
                "content": f"AIå“åº”ï¼š{last_message.text}"
            }
        ]
    )
    stream_writer(result)  # [!code highlight]

    tool_call = result.tool_calls[0]
    if tool_call["args"]["evaluation"] == "unsafe":
        last_message.content = "æˆ‘æ— æ³•æä¾›è¯¥å“åº”ã€‚è¯·é‡æ–°è¡¨è¿°æ‚¨çš„è¯·æ±‚ã€‚"

    return None
```
ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥å°†æ­¤ä¸­é—´ä»¶é›†æˆåˆ°æˆ‘ä»¬çš„æ™ºèƒ½ä½“ä¸­ï¼Œå¹¶åŒ…å«å…¶è‡ªå®šä¹‰æµäº‹ä»¶ï¼š
```python
from typing import Any

from langchain.agents import create_agent
from langchain.messages import AIMessageChunk, AIMessage, AnyMessage


def get_weather(city: str) -> str:
    """è·å–ç»™å®šåŸå¸‚çš„å¤©æ°”ã€‚"""

    return f"It's always sunny in {city}!"


agent = create_agent(
    model="openai:gpt-5.2",
    tools=[get_weather],
    middleware=[safety_guardrail],  # [!code highlight]
)

def _render_message_chunk(token: AIMessageChunk) -> None:
    if token.text:
        print(token.text, end="|")
    if token.tool_call_chunks:
        print(token.tool_call_chunks)


def _render_completed_message(message: AnyMessage) -> None:
    if isinstance(message, AIMessage) and message.tool_calls:
        print(f"Tool calls: {message.tool_calls}")
    if isinstance(message, ToolMessage):
        print(f"Tool response: {message.content_blocks}")
```

input_message = {"role": "user", "content": "What is the weather in Boston?"}
for stream_mode, data in agent.stream(
    {"messages": [input_message]},
    stream_mode=["messages", "updates", "custom"],  # [!code highlight]
):
    if stream_mode == "messages":
        token, metadata = data
        if isinstance(token, AIMessageChunk):
            _render_message_chunk(token)
    if stream_mode == "updates":
        for source, update in data.items():
            if source in ("model", "tools"):
                _render_completed_message(update["messages"][-1])
    if stream_mode == "custom":  # [!code highlight]
        # åœ¨æµä¸­è®¿é—®å·²å®Œæˆçš„ message
        print(f"Tool calls: {data.tool_calls}")  # [!code highlight]
```
```shell title="Output" expandable
[{'name': 'get_weather', 'args': '', 'id': 'call_je6LWgxYzuZ84mmoDalTYMJC', 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '{"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'city', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '":"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'Boston', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '"}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
Tool calls: [{'name': 'get_weather', 'args': {'city': 'Boston'}, 'id': 'call_je6LWgxYzuZ84mmoDalTYMJC', 'type': 'tool_call'}]
Tool response: [{'type': 'text', 'text': "It's always sunny in Boston!"}]
The| weather| in| **|Boston|**| is| **|sun|ny|**|.|[{'name': 'ResponseSafety', 'args': '', 'id': 'call_O8VJIbOG4Q9nQF0T8ltVi58O', 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '{"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'evaluation', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '":"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'safe', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '"}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
Tool calls: [{'name': 'ResponseSafety', 'args': {'evaluation': 'safe'}, 'id': 'call_O8VJIbOG4Q9nQF0T8ltVi58O', 'type': 'tool_call'}]
```

æˆ–è€…ï¼Œå¦‚æœæ‚¨æ— æ³•å‘æµä¸­æ·»åŠ è‡ªå®šä¹‰äº‹ä»¶ï¼Œå¯ä»¥åœ¨æµå¼å¾ªç¯å†…èšåˆæ¶ˆæ¯å—ï¼š
```python
input_message = {"role": "user", "content": "What is the weather in Boston?"}
full_message = None  # [!code highlight]
for stream_mode, data in agent.stream(
    {"messages": [input_message]},
    stream_mode=["messages", "updates"],
):
    if stream_mode == "messages":
        token, metadata = data
        if isinstance(token, AIMessageChunk):
            _render_message_chunk(token)
            full_message = token if full_message is None else full_message + token  # [!code highlight]
            if token.chunk_position == "last":  # [!code highlight]
                if full_message.tool_calls:  # [!code highlight]
                    print(f"Tool calls: {full_message.tool_calls}")  # [!code highlight]
                full_message = None  # [!code highlight]
    if stream_mode == "updates":
        for source, update in data.items():
            if source == "tools":
                _render_completed_message(update["messages"][-1])
```

### æ”¯æŒäººæœºååŒï¼ˆhuman-in-the-loopï¼‰çš„æµå¼å¤„ç†

ä¸ºäº†å¤„ç†äººæœºååŒï¼ˆhuman-in-the-loopï¼‰çš„[ä¸­æ–­](/oss/langchain/human-in-the-loop)ï¼Œæˆ‘ä»¬åœ¨[ä¸Šé¢çš„ç¤ºä¾‹](#streaming-tool-calls)åŸºç¡€ä¸Šè¿›è¡Œæ„å»ºï¼š

1.  æˆ‘ä»¬ä½¿ç”¨[äººæœºååŒä¸­é—´ä»¶å’Œä¸€ä¸ªæ£€æŸ¥ç‚¹](/oss/langchain/human-in-the-loop#configuring-interrupts)æ¥é…ç½®æ™ºèƒ½ä½“ï¼ˆagentï¼‰ã€‚
2.  æˆ‘ä»¬åœ¨ `"updates"` æµæ¨¡å¼æœŸé—´æ”¶é›†ç”Ÿæˆçš„ä¸­æ–­ã€‚
3.  æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ª[å‘½ä»¤](/oss/langchain/human-in-the-loop#responding-to-interrupts)æ¥å“åº”è¿™äº›ä¸­æ–­ã€‚

```python
from typing import Any

ä» langchain.agents å¯¼å…¥ create_agent
ä» langchain.agents.middleware å¯¼å…¥ HumanInTheLoopMiddleware
ä» langchain.messages å¯¼å…¥ AIMessage, AIMessageChunk, AnyMessage, ToolMessage
ä» langgraph.checkpoint.memory å¯¼å…¥ InMemorySaver
ä» langgraph.types å¯¼å…¥ Command, Interrupt


def get_weather(city: str) -> str:
    """è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ã€‚"""

    return f"It's always sunny in {city}!"


checkpointer = InMemorySaver()

agent = create_agent(
    "openai:gpt-5.2",
    tools=[get_weather],
    middleware=[  # [!code highlight]
        HumanInTheLoopMiddleware(interrupt_on={"get_weather": True}),  # [!code highlight]
    ],  # [!code highlight]
    checkpointer=checkpointer,  # [!code highlight]
)


def _render_message_chunk(token: AIMessageChunk) -> None:
    if token.text:
        print(token.text, end="|")
    if token.tool_call_chunks:
        print(token.tool_call_chunks)


def _render_completed_message(message: AnyMessage) -> None:
    if isinstance(message, AIMessage) and message.tool_calls:
        print(f"å·¥å…·è°ƒç”¨: {message.tool_calls}")
    if isinstance(message, ToolMessage):
        print(f"å·¥å…·å“åº”: {message.content_blocks}")


def _render_interrupt(interrupt: Interrupt) -> None:  # [!code highlight]
    interrupts = interrupt.value  # [!code highlight]
    for request in interrupts["action_requests"]:  # [!code highlight]
        print(request["description"])  # [!code highlight]


input_message = {
    "role": "user",
    "content": (
        "ä½ èƒ½æŸ¥è¯¢ä¸€ä¸‹æ³¢å£«é¡¿å’Œæ—§é‡‘å±±çš„å¤©æ°”å—ï¼Ÿ"
    ),
}
config = {"configurable": {"thread_id": "some_id"}}  # [!code highlight]
interrupts = []  # [!code highlight]
for stream_mode, data in agent.stream(
    {"messages": [input_message]},
    config=config,  # [!code highlight]
    stream_mode=["messages", "updates"],
):
    if stream_mode == "messages":
        token, metadata = data
        if isinstance(token, AIMessageChunk):
            _render_message_chunk(token)
    if stream_mode == "updates":
        for source, update in data.items():
            if source in ("model", "tools"):
                _render_completed_message(update["messages"][-1])
            if source == "__interrupt__":  # [!code highlight]
                interrupts.extend(update)  # [!code highlight]
                _render_interrupt(update[0])  # [!code highlight]
```
```shell title="Output" expandable
[{'name': 'get_weather', 'args': '', 'id': 'call_GOwNaQHeqMixay2qy80padfE', 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '{"ci', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'ty": ', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '"Bosto', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'n"}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': 'get_weather', 'args': '', 'id': 'call_Ndb4jvWm2uMA0JDQXu37wDH6', 'index': 1, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '{"ci', 'id': None, 'index': 1, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'ty": ', 'id': None, 'index': 1, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '"San F', 'id': None, 'index': 1, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'ranc', 'id': None, 'index': 1, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'isco"', 'id': None, 'index': 1, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '}', 'id': None, 'index': 1, 'type': 'tool_call_chunk'}]
å·¥å…·è°ƒç”¨: [{'name': 'get_weather', 'args': {'city': 'Boston'}, 'id': 'call_GOwNaQHeqMixay2qy80padfE', 'type': 'tool_call'}, {'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_Ndb4jvWm2uMA0JDQXu37wDH6', 'type': 'tool_call'}]
å·¥å…·æ‰§è¡Œéœ€è¦æ‰¹å‡†

å·¥å…·: get_weather
å‚æ•°: {'city': 'Boston'}
å·¥å…·æ‰§è¡Œéœ€è¦æ‰¹å‡†

å·¥å…·: get_weather
å‚æ•°: {'city': 'San Francisco'}
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªä¸­æ–­æ”¶é›†ä¸€ä¸ª[å†³ç­–](/oss/langchain/human-in-the-loop#interrupt-decision-types)ã€‚é‡è¦çš„æ˜¯ï¼Œå†³ç­–çš„é¡ºåºå¿…é¡»ä¸æˆ‘ä»¬æ”¶é›†åˆ°çš„æ“ä½œé¡ºåºç›¸åŒ¹é…ã€‚

ä¸ºäº†è¯´æ˜ï¼Œæˆ‘ä»¬å°†ç¼–è¾‘ä¸€ä¸ªå·¥å…·è°ƒç”¨å¹¶æ¥å—å¦ä¸€ä¸ªï¼š
```python
def _get_interrupt_decisions(interrupt: Interrupt) -> list[dict]:
    return [
        {
            "type": "edit",
            "edited_action": {
                "name": "get_weather",
                "args": {"city": "Boston, U.K."},
            },
        }
        if "boston" in request["description"].lower()
        else {"type": "approve"}
        for request in interrupt.value["action_requests"]
    ]

decisions = {}
for interrupt in interrupts:
    decisions[interrupt.id] = {
        "decisions": _get_interrupt_decisions(interrupt)
    }

decisions
```
```shell title="Output"
{
    'a96c40474e429d661b5b32a8d86f0f3e': {
        'decisions': [
            {
                'type': 'edit',
                 'edited_action': {
                     'name': 'get_weather',
                     'args': {'city': 'Boston, U.K.'}
                 }
            },
            {'type': 'approve'},
        ]
    }
}
```

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å‘åŒä¸€ä¸ªæµå¼å¾ªç¯ä¼ é€’ä¸€ä¸ª[å‘½ä»¤](/oss/langchain/human-in-the-loop#responding-to-interrupts)æ¥æ¢å¤æ‰§è¡Œï¼š
```python
interrupts = []
for stream_mode, data in agent.stream(
    Command(resume=decisions),  # [!code highlight]
    config=config,
    stream_mode=["messages", "updates"],
):
    # æµå¼å¾ªç¯ä¿æŒä¸å˜
    if stream_mode == "messages":
        token, metadata = data
        if isinstance(token, AIMessageChunk):
            _render_message_chunk(token)
    if stream_mode == "updates":
        for source, update in data.items():
            if source in ("model", "tools"):
                _render_completed_message(update["messages"][-1])
            if source == "__interrupt__":
                interrupts.extend(update)
                _render_interrupt(update[0])
```
```shell title="Output"
Tool response: [{'type': 'text', 'text': "It's always sunny in Boston, U.K.!"}]
Tool response: [{'type': 'text', 'text': "It's always sunny in San Francisco!"}]
-| **|Boston|**|:| It|'s| always| sunny| in| Boston|,| U|.K|.|
|-| **|San| Francisco|**|:| It|'s| always| sunny| in| San| Francisco|!|
```

### ä»å­æ™ºèƒ½ä½“æµå¼ä¼ è¾“

å½“æ™ºèƒ½ä½“ä¸­çš„ä»»ä½•ä½ç½®å­˜åœ¨å¤šä¸ª LLM æ—¶ï¼Œé€šå¸¸éœ€è¦åœ¨æ¶ˆæ¯ç”Ÿæˆæ—¶åŒºåˆ†å…¶æ¥æºã€‚

ä¸ºæ­¤ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ [`tags`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.BaseChatModel.tags) åˆå§‹åŒ–ä»»ä½•æ¨¡å‹ã€‚è¿™äº›æ ‡ç­¾åœ¨ `"messages"` æ¨¡å¼ä¸‹æµå¼ä¼ è¾“æ—¶ï¼Œå¯ä»¥åœ¨å…ƒæ•°æ®ä¸­è·å–ã€‚

ä¸‹é¢ï¼Œæˆ‘ä»¬æ›´æ–°[æµå¼å·¥å…·è°ƒç”¨](#streaming-tool-calls)ç¤ºä¾‹ï¼š

1.  æˆ‘ä»¬å°†å·¥å…·æ›¿æ¢ä¸ºå†…éƒ¨è°ƒç”¨æ™ºèƒ½ä½“çš„ `call_weather_agent` å·¥å…·ã€‚
2.  æˆ‘ä»¬ä¸ºæ­¤ LLM å’Œå¤–éƒ¨çš„["ç›‘ç£è€…"](/oss/langchain/multi-agent) LLM æ·»åŠ ä¸€ä¸ªå­—ç¬¦ä¸²æ ‡ç­¾ã€‚
3.  æˆ‘ä»¬åœ¨åˆ›å»ºæµæ—¶æŒ‡å®š [`subgraphs=True`](/oss/langgraph/use-subgraphs#stream-subgraph-outputs)ã€‚
4.  æˆ‘ä»¬çš„æµå¤„ç†é€»è¾‘ä¸ä¹‹å‰ç›¸åŒï¼Œä½†æ·»åŠ äº†è·Ÿè¸ªå½“å‰æ´»åŠ¨ LLM çš„é€»è¾‘ã€‚

<Tip>
å¦‚æœä¸éœ€è¦ä»å­æ™ºèƒ½ä½“æµå¼ä¼ è¾“ä»¤ç‰Œï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ [name](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent(name)) åˆå§‹åŒ–å­æ™ºèƒ½ä½“ã€‚å½“æµå¼ä¼ è¾“ [`updates`](#agent-progress) æ—¶ï¼Œæ­¤åç§°å¯åœ¨å­æ™ºèƒ½ä½“ç”Ÿæˆçš„æ¶ˆæ¯ä¸Šè®¿é—®ã€‚
</Tip>

é¦–å…ˆï¼Œæˆ‘ä»¬æ„å»ºæ™ºèƒ½ä½“ï¼š
```python
from typing import Any

from langchain.agents import create_agent
from langchain.chat_models import init_chat_model
from langchain.messages import AIMessage, AnyMessage


def get_weather(city: str) -> str:
    """Get weather for a given city."""

    return f"It's always sunny in {city}!"


weather_model = init_chat_model(
    "openai:gpt-5.2",
    tags=["weather_sub_agent"],
)
```

weather_agent = create_agent(model=weather_model, tools=[get_weather])


def call_weather_agent(query: str) -> str:
    """æŸ¥è¯¢å¤©æ°”æ™ºèƒ½ä½“ã€‚"""
    result = weather_agent.invoke({
        "messages": [{"role": "user", "content": query}]
    })
    return result["messages"][-1].text


supervisor_model = init_chat_model(
    "openai:gpt-5.2",
    tags=["supervisor"],
)

agent = create_agent(model=supervisor_model, tools=[call_weather_agent])
```
æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å‘æµå¼å¾ªç¯æ·»åŠ é€»è¾‘ï¼Œä»¥æŠ¥å‘Šæ˜¯å“ªä¸ªæ™ºèƒ½ä½“æ­£åœ¨å‘å‡ºä»¤ç‰Œï¼š
```python
def _render_message_chunk(token: AIMessageChunk) -> None:
    if token.text:
        print(token.text, end="|")
    if token.tool_call_chunks:
        print(token.tool_call_chunks)


def _render_completed_message(message: AnyMessage) -> None:
    if isinstance(message, AIMessage) and message.tool_calls:
        print(f"å·¥å…·è°ƒç”¨: {message.tool_calls}")
    if isinstance(message, ToolMessage):
        print(f"å·¥å…·å“åº”: {message.content_blocks}")
```

input_message = {"role": "user", "content": "What is the weather in Boston?"}
current_agent = None  // [!code highlight]
for _, stream_mode, data in agent.stream(
    {"messages": [input_message]},
    stream_mode=["messages", "updates"],
    subgraphs=True,  // [!code highlight]
):
    if stream_mode == "messages":
        token, metadata = data
        if tags := metadata.get("tags", []):  // [!code highlight]
            this_agent = tags[0]  // [!code highlight]
            if this_agent != current_agent:  // [!code highlight]
                print(f"ğŸ¤– {this_agent}: ")  // [!code highlight]
                current_agent = this_agent  // [!code highlight]
        if isinstance(token, AIMessage):
            _render_message_chunk(token)
    if stream_mode == "updates":
        for source, update in data.items():
            if source in ("model", "tools"):
                _render_completed_message(update["messages"][-1])
```
```shell title="Output" expandable
ğŸ¤– supervisor:
[{'name': 'call_weather_agent', 'args': '', 'id': 'call_asorzUf0mB6sb7MiKfgojp7I', 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '{"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'query', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '":"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'Boston', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': ' weather', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': ' right', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': ' now', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': ' and', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': " today's", 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': ' forecast', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '"}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
Tool calls: [{'name': 'call_weather_agent', 'args': {'query': "Boston weather right now and today's forecast"}, 'id': 'call_asorzUf0mB6sb7MiKfgojp7I', 'type': 'tool_call'}]
ğŸ¤– weather_sub_agent:
[{'name': 'get_weather', 'args': '', 'id': 'call_LZ89lT8fW6w8vqck5pZeaDIx', 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '{"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'city', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '":"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'Boston', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '"}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
Tool calls: [{'name': 'get_weather', 'args': {'city': 'Boston'}, 'id': 'call_LZ89lT8fW6w8vqck5pZeaDIx', 'type': 'tool_call'}]
Tool response: [{'type': 'text', 'text': "It's always sunny in Boston!"}]
Boston| weather| right| now|:| **|Sunny|**|.

|Today|'s| forecast| for| Boston|:| **|Sunny| all| day|**|.|Tool response: [{'type': 'text', 'text': 'Boston weather right now: **Sunny**.\n\nToday's forecast for Boston: **Sunny all day**.'}]
ğŸ¤– supervisor:
Boston| weather| right| now|:| **|Sunny|**|.

|Today|'s| forecast| for| Boston|:| **|Sunny| all| day|**|.|
```
:::

## ç¦ç”¨æµå¼ä¼ è¾“

åœ¨æŸäº›åº”ç”¨ä¸­ï¼Œæ‚¨å¯èƒ½éœ€è¦ä¸ºç»™å®šæ¨¡å‹ç¦ç”¨å•ä¸ªä»¤ç‰Œçš„æµå¼ä¼ è¾“ã€‚è¿™åœ¨ä»¥ä¸‹æƒ…å†µä¸‹å¾ˆæœ‰ç”¨ï¼š

- ä½¿ç”¨[å¤šæ™ºèƒ½ä½“](/oss/langchain/multi-agent)ç³»ç»Ÿæ—¶ï¼Œæ§åˆ¶å“ªäº›æ™ºèƒ½ä½“æµå¼ä¼ è¾“å…¶è¾“å‡º
- æ··åˆä½¿ç”¨æ”¯æŒæµå¼ä¼ è¾“å’Œä¸æ”¯æŒæµå¼ä¼ è¾“çš„æ¨¡å‹æ—¶
- éƒ¨ç½²åˆ°[LangSmith](/langsmith/home)å¹¶å¸Œæœ›é˜»æ­¢æŸäº›æ¨¡å‹è¾“å‡ºè¢«æµå¼ä¼ è¾“åˆ°å®¢æˆ·ç«¯æ—¶

:::python
åˆå§‹åŒ–æ¨¡å‹æ—¶è®¾ç½® `streaming=False`ã€‚

```python
from langchain_openai import ChatOpenAI

model = ChatOpenAI(
    model="gpt-4o",
    streaming=False  // [!code highlight]
)
```
:::

:::js
åˆå§‹åŒ–æ¨¡å‹æ—¶è®¾ç½® `streaming: false`ã€‚

```typescript
import { ChatOpenAI } from "@langchain/openai";

const model = new ChatOpenAI({
  model: "gpt-4o",
  streaming: false,  // [!code highlight]
});
```
:::

<Tip>
éƒ¨ç½²åˆ° LangSmith æ—¶ï¼Œå¯¹äºä¸å¸Œæœ›æµå¼ä¼ è¾“åˆ°å®¢æˆ·ç«¯çš„ä»»ä½•æ¨¡å‹ï¼Œè¯·è®¾ç½® `streaming=False`ã€‚è¿™éœ€è¦åœ¨éƒ¨ç½²å‰åœ¨æ‚¨çš„å›¾ä»£ç ä¸­è¿›è¡Œé…ç½®ã€‚
</Tip>

:::python
<Note>
å¹¶éæ‰€æœ‰èŠå¤©æ¨¡å‹é›†æˆéƒ½æ”¯æŒ `streaming` å‚æ•°ã€‚å¦‚æœæ‚¨çš„æ¨¡å‹ä¸æ”¯æŒï¼Œè¯·æ”¹ç”¨ `disable_streaming=True`ã€‚æ­¤å‚æ•°å¯é€šè¿‡åŸºç±»åœ¨æ‰€æœ‰èŠå¤©æ¨¡å‹ä¸Šä½¿ç”¨ã€‚
</Note>
:::

:::js
<Note>
å¹¶éæ‰€æœ‰èŠå¤©æ¨¡å‹é›†æˆéƒ½æ”¯æŒ `streaming` å‚æ•°ã€‚å¦‚æœæ‚¨çš„æ¨¡å‹ä¸æ”¯æŒï¼Œè¯·æ”¹ç”¨ `disableStreaming: true`ã€‚æ­¤å‚æ•°å¯é€šè¿‡åŸºç±»åœ¨æ‰€æœ‰èŠå¤©æ¨¡å‹ä¸Šä½¿ç”¨ã€‚
</Note>
:::

æ›´å¤šè¯¦æƒ…è¯·å‚é˜… [LangGraph æµå¼ä¼ è¾“æŒ‡å—](/oss/langgraph/streaming#disable-streaming-for-specific-chat-models)ã€‚

## ç›¸å…³é“¾æ¥

- [å‰ç«¯æµå¼ä¼ è¾“](/oss/langchain/streaming/frontend) â€” ä½¿ç”¨ `useStream` æ„å»º React UIï¼Œå®ç°å®æ—¶æ™ºèƒ½ä½“äº¤äº’
- [ä½¿ç”¨èŠå¤©æ¨¡å‹è¿›è¡Œæµå¼ä¼ è¾“](/oss/langchain/models#stream) â€” æ— éœ€ä½¿ç”¨æ™ºèƒ½ä½“æˆ–å›¾ï¼Œç›´æ¥ä»èŠå¤©æ¨¡å‹æµå¼ä¼ è¾“ä»¤ç‰Œ
- [äººæœºååŒï¼ˆhuman-in-the-loopï¼‰æµå¼ä¼ è¾“](/oss/langchain/human-in-the-loop#streaming-with-hil) â€” åœ¨å¤„ç†äººå·¥å®¡æ ¸ä¸­æ–­çš„åŒæ—¶æµå¼ä¼ è¾“æ™ºèƒ½ä½“è¿›åº¦
- [LangGraph æµå¼ä¼ è¾“](/oss/langgraph/streaming) â€” é«˜çº§æµå¼ä¼ è¾“é€‰é¡¹ï¼ŒåŒ…æ‹¬ `values`ã€`debug` æ¨¡å¼ä»¥åŠå­å›¾æµå¼ä¼ è¾“
