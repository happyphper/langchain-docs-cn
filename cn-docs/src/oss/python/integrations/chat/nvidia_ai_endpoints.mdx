---
title: ChatNVIDIA
---
这将帮助您开始使用 NVIDIA [聊天模型](/oss/langchain/models)。有关 `ChatNVIDIA` 所有功能和配置的详细文档，请参阅 [API 参考](https://python.langchain.com/api_reference/nvidia_ai_endpoints/chat_models/langchain_nvidia_ai_endpoints.chat_models.ChatNVIDIA.html)。

## 概述

`langchain-nvidia-ai-endpoints` 包包含了 LangChain 集成，用于在 NVIDIA NIM 推理微服务上使用模型构建应用程序。NIM 支持来自社区和 NVIDIA 的跨领域模型，如聊天、嵌入和重排序模型。这些模型由 NVIDIA 优化，以在 NVIDIA 加速基础设施上提供最佳性能，并部署为 NIM。NIM 是易于使用的预构建容器，可在 NVIDIA 加速基础设施上使用单个命令部署到任何地方。

NVIDIA 托管的 NIM 部署可在 [NVIDIA API 目录](https://build.nvidia.com/) 上进行测试。测试后，可以使用 NVIDIA AI Enterprise 许可证从 NVIDIA 的 API 目录导出 NIM，并在本地或云端运行，使企业能够拥有并完全控制其 IP 和 AI 应用程序。

NIM 按模型打包为容器镜像，并通过 NVIDIA NGC 目录作为 NGC 容器镜像分发。NIM 的核心是为 AI 模型运行推理提供简单、一致且熟悉的 API。

本示例将介绍如何使用 LangChain 通过 `ChatNVIDIA` 类与 NVIDIA 支持的模型进行交互。

有关通过此 API 访问聊天模型的更多信息，请查看 [ChatNVIDIA](https://python.langchain.com/docs/integrations/chat/nvidia_ai_endpoints/) 文档。

### 集成详情

| 类 | 包 | 可序列化 | JS 支持 | 下载量 | 版本 |
| :--- | :--- | :---: |  :---: | :---: | :---: |
| [ChatNVIDIA](https://python.langchain.com/api_reference/nvidia_ai_endpoints/chat_models/langchain_nvidia_ai_endpoints.chat_models.ChatNVIDIA.html) | [langchain-nvidia-ai-endpoints](https://python.langchain.com/api_reference/nvidia_ai_endpoints/index.html) | beta | ❌ | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain_nvidia_ai_endpoints?style=flat-square&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/langchain_nvidia_ai_endpoints?style=flat-square&label=%20) |

### 模型特性

| [工具调用](/oss/langchain/tools) | [结构化输出](/oss/langchain/structured-output) | [图像输入](/oss/langchain/messages#multimodal) | 音频输入 | 视频输入 | [令牌级流式传输](/oss/langchain/streaming/) | 原生异步 | [令牌使用量](/oss/langchain/models#token-usage) | [对数概率](/oss/langchain/models#log-probabilities) |
| :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: | :---: |
| ✅ | ✅ | ✅ | ❌ | ❌ | ✅ | ✅ | ✅ | ❌ |

## 设置

**开始之前：**

1.  在托管 NVIDIA AI Foundation 模型的 [NVIDIA](https://build.nvidia.com/) 上创建一个免费账户。
2.  点击您选择的模型。
3.  在 `Input` 下选择 `Python` 选项卡，然后点击 `Get API Key`。接着点击 `Generate Key`。
4.  复制并保存生成的密钥为 `NVIDIA_API_KEY`。之后，您应该就可以访问端点了。

### 凭据

```python
import getpass
import os

if not os.getenv("NVIDIA_API_KEY"):
    # 注意：API 密钥应以 "nvapi-" 开头
    os.environ["NVIDIA_API_KEY"] = getpass.getpass("Enter your NVIDIA API key: ")
```

要启用模型调用的自动追踪，请设置您的 [LangSmith](https://docs.langchain.com/langsmith/home) API 密钥：

```python
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = getpass.getpass("Enter your LangSmith API key: ")
```

### 安装

LangChain NVIDIA AI Endpoints 集成位于 `langchain-nvidia-ai-endpoints` 包中：

```python
pip install -qU langchain-nvidia-ai-endpoints
```

## 实例化

现在我们可以访问 NVIDIA API 目录中的模型了：

```python
## Core LC Chat Interface
from langchain_nvidia_ai_endpoints import ChatNVIDIA

llm = ChatNVIDIA(model="mistralai/mixtral-8x7b-instruct-v0.1")
```

## 调用

```python
result = llm.invoke("Write a ballad about LangChain.")
print(result.content)
```

## 使用 NVIDIA NIM

当准备部署时，您可以使用 NVIDIA NIM（包含在 NVIDIA AI Enterprise 软件许可证中）自托管模型，并在任何地方运行它们，从而拥有您的自定义项并完全控制您的知识产权 (IP) 和 AI 应用程序。

[了解更多关于 NIM 的信息](https://developer.nvidia.com/blog/nvidia-nim-offers-optimized-inference-microservices-for-deploying-ai-models-at-scale/)

```python
from langchain_nvidia_ai_endpoints import ChatNVIDIA

# 连接到运行在 localhost:8000 的嵌入 NIM，指定特定模型
llm = ChatNVIDIA(base_url="http://localhost:8000/v1", model="meta/llama3-8b-instruct")
```

## 流式、批处理和异步

这些模型原生支持流式传输，并且与所有 LangChain LLM 一样，它们公开了一个批处理方法来处理并发请求，以及用于 invoke、stream 和 batch 的异步方法。下面是一些示例。

```python
print(llm.batch(["What's 2*3?", "What's 2*6?"]))
# 或者通过异步 API
# await llm.abatch(["What's 2*3?", "What's 2*6?"])
```

```python
for chunk in llm.stream("How far can a seagull fly in one day?"):
    # 显示令牌分隔
    print(chunk.content, end="|")
```

```python
async for chunk in llm.astream(
    "How long does it take for monarch butterflies to migrate?"
):
    print(chunk.content, end="|")
```

## 支持的模型

查询 `available_models` 仍将返回您的 API 凭据提供的所有其他模型。

`playground_` 前缀是可选的。

```python
ChatNVIDIA.get_available_models()
# llm.get_available_models()
```

## 模型类型

上述所有模型都受支持，并且可以通过 `ChatNVIDIA` 访问。

某些模型类型支持独特的提示技术和聊天消息。我们将在下面回顾一些重要的类型。

**要了解有关特定模型的更多信息，请导航到 AI Foundation 模型的 API 部分，[链接在此](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/codellama-13b/api)。**

### 通用聊天

像 `meta/llama3-8b-instruct` 和 `mistralai/mixtral-8x22b-instruct-v0.1` 这样的模型是优秀的全能模型，您可以将其用于任何 LangChain 聊天消息。示例如下。

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_nvidia_ai_endpoints import ChatNVIDIA

prompt = ChatPromptTemplate.from_messages(
    [("system", "You are a helpful AI assistant named Fred."), ("user", "{input}")]
)
chain = prompt | ChatNVIDIA(model="meta/llama3-8b-instruct") | StrOutputParser()

for txt in chain.stream({"input": "What's your name?"}):
    print(txt, end="")
```

### 代码生成

这些模型接受与常规聊天模型相同的参数和输入结构，但它们在代码生成和结构化代码任务上往往表现更好。`meta/codellama-70b` 就是一个例子。

```python
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are an expert coding AI. Respond only in valid python; no narration whatsoever.",
        ),
        ("user", "{input}"),
    ]
)
chain = prompt | ChatNVIDIA(model="meta/codellama-70b") | StrOutputParser()

for txt in chain.stream({"input": "How do I solve this fizz buzz problem?"}):
    print(txt, end="")
```

## 多模态

NVIDIA 还支持多模态输入，这意味着您可以同时提供图像和文本供模型推理。支持多模态输入的模型示例是 `nvidia/neva-22b`。

以下是使用示例：

```python
import IPython
import requests

image_url = "https://www.nvidia.com/content/dam/en-zz/Solutions/research/ai-playground/nvidia-picasso-3c33-p@2x.jpg"  ## Large Image
image_content = requests.get(image_url).content

IPython.display.Image(image_content)
```

```python
from langchain_nvidia_ai_endpoints import ChatNVIDIA

llm = ChatNVIDIA(model="nvidia/neva-22b")
```

#### 将图像作为 URL 传递

```python
from langchain.messages import HumanMessage

llm.invoke(
    [
        HumanMessage(
            content=[
                {"type": "text", "text": "Describe this image:"},
                {"type": "image_url", "image_url": {"url": image_url}},
            ]
        )
    ]
)
```

#### 将图像作为 base64 编码字符串传递

目前，客户端会进行一些额外的处理以支持像上面那样的大图像。但对于较小的图像（并且为了更好地说明底层过程），我们可以直接传入图像，如下所示：

```python
import IPython
import requests

image_url = "https://picsum.photos/seed/kitten/300/200"
image_content = requests.get(image_url).content

IPython.display.Image(image_content)
```

```python
import base64

from langchain.messages import HumanMessage

## 适用于较简单的图像。对于较大的图像，请参阅实际实现
b64_string = base64.b64encode(image_content).decode("utf-8")

llm.invoke(
    [
        HumanMessage(
            content=[
                {"type": "text", "text": "Describe this image:"},
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/png;base64,{b64_string}"},
                },
            ]
        )
    ]
)
```

#### 直接在字符串中

NVIDIA API 独特地接受内联在 `<img/>` HTML 标签中的 base64 图像。虽然这与其他 LLM 不兼容，但您可以直接相应地提示模型。

```python
base64_with_mime_type = f"data:image/png;base64,{b64_string}"
llm.invoke(f'What\'s in this image?\n<img src="{base64_with_mime_type}" />')
```

## 在 RunnableWithMessageHistory 中的使用示例

与其他集成一样，ChatNVIDIA 可以很好地支持聊天实用程序，如 RunnableWithMessageHistory，这类似于使用 `ConversationChain`。下面，我们将 [LangChain RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) 示例应用于 `mistralai/mixtral-8x22b-instruct-v0.1` 模型。

```python
pip install -qU langchain
```

```python
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

# store 是一个将会话 ID 映射到其对应聊天历史的字典。
store = {}  # 内存维护在链外部


# 一个返回给定会话 ID 聊天历史的函数。
def get_session_history(session_id: str) -> InMemoryChatMessageHistory:
    if session_id not in store:
        store[session_id] = InMemoryChatMessageHistory()
    return store[session_id]


chat = ChatNVIDIA(
    model="mistralai/mixtral-8x22b-instruct-v0.1",
    temperature=0.1,
    max_tokens=100,
    top_p=1.0,
)

#  定义一个 RunnableConfig 对象，带有 `configurable` 键。session_id 决定线程
config = {"configurable": {"session_id": "1"}}

conversation = RunnableWithMessageHistory(
    chat,
    get_session_history,
)

conversation.invoke(
    "Hi I'm Srijan Dubey.",  # 输入或查询
    config=config,
)
```

```python
conversation.invoke(
    "I'm doing well! Just having a conversation with an AI.",
    config=config,
)
```

```python
conversation.invoke(
    "Tell me about yourself.",
    config=config,
)
```

## 工具调用

从 v0.2 开始，`ChatNVIDIA` 支持 [bind_tools](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html#langchain_core.language_models.chat_models.BaseChatModel.bind_tools)。

`ChatNVIDIA` 提供了与 [build.nvidia.com](https://build.nvidia.com) 上各种模型以及本地 NIM 的集成。并非所有这些模型都经过工具调用训练。请务必选择确实支持工具调用的模型进行实验和应用程序开发。

您可以使用以下方法获取已知支持工具调用的模型列表：

```python
tool_models = [
    model for model in ChatNVIDIA.get_available_models() if model.supports_tools
]
tool_models
```

使用一个支持工具的模型：

```python
from langchain.tools import tool
from pydantic import Field


@tool
def get_current_weather(
    location: str = Field(..., description="The location to get the weather for."),
):
    """Get the current weather for a location."""
    ...


llm = ChatNVIDIA(model=tool_models[0].id).bind_tools(tools=[get_current_weather])
response = llm.invoke("What is the weather in Boston?")
response.tool_calls
```

有关更多示例，请参阅 [如何使用聊天模型调用工具](/oss/langchain/tools)。

---

## API 参考

有关 `ChatNVIDIA` 所有功能和配置的详细文档，请参阅 API 参考：[python.langchain.com/api_reference/nvidia_ai_endpoints/chat_models/langchain_nvidia_ai_endpoints.chat_models.ChatNVIDIA.html](https://python.langchain.com/api_reference/nvidia_ai_endpoints/chat_models/langchain_nvidia_ai_endpoints.chat_models.ChatNVIDIA.html)
