---
title: ArxivLoader
---
[arXiv](https://arxiv.org/) 是一个开放获取的学术论文存档库，收录了物理学、数学、计算机科学、定量生物学、定量金融学、统计学、电气工程与系统科学以及经济学领域的 200 万篇学术文章。

## 安装

要使用 Arxiv 文档加载器，你需要安装 `arxiv`、`PyMuPDF` 和 `langchain-community` 集成包。PyMuPDF 将从 arxiv.org 网站下载的 PDF 文件转换为文本格式。

```python
pip install -qU langchain-community arxiv pymupdf
```

## 实例化

现在我们可以实例化模型对象并加载文档：

```python
from langchain_community.document_loaders import ArxivLoader

# 支持 `ArxivAPIWrapper` 的所有参数
loader = ArxivLoader(
    query="reasoning",
    load_max_docs=2,
    # doc_content_chars_max=1000,
    # load_all_available_meta=False,
    # ...
)
```

## 加载

使用 `.load()` 方法将所有文档同步加载到内存中，每个 arXiv 论文对应一个 Document 对象。

让我们通过一个基本示例来了解如何使用 `ArxivLoader` 搜索关于推理（reasoning）的论文：

```python
docs = loader.load()
docs[0]
```

```python
Document(page_content='Hypothesis Testing Prompting Improves Deductive Reasoning in\nLarge Language Models\nYitian Li1,2, Jidong Tian1,2, Hao He1,2, Yaohui Jin1,2\n1MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University\n2State Key Lab of Advanced Optical Communication System and Network\n{yitian_li, frank92, hehao, jinyh}@sjtu.edu.cn\nAbstract\nCombining different forms of prompts with pre-trained large language models has yielded remarkable results on\nreasoning tasks (e.g. Chain-of-Thought prompting). However, along with testing on more complex reasoning, these\nmethods also expose problems such as invalid reasoning and fictional reasoning paths. In this paper, we develop\nHypothesis Testing Prompting, which adds conclusion assumptions, backward reasoning, and fact verification during\nintermediate reasoning steps. Hypothesis Testing prompting involves multiple assumptions and reverses validation of\nconclusions leading to its unique correct answer. Experiments on two challenging deductive reasoning datasets\nProofWriter and RuleTaker show that hypothesis testing prompting not only significantly improves the effect, but also\ngenerates a more reasonable and standardized reasoning process.\nKeywords: Deductive Reasoning, Large Language Models, Prompt\n1.\nIntroduction\nThe release of large language models (LLMs) has\nrevolutionized the NLP landscape recently (Thop-\npilan et al., 2022; Kaplan et al., 2020; Chowdh-\nery et al., 2022). Scaling up the size of language\nmodels and conducting diversified prompt meth-\nods become mainstream (Liu et al., 2023c; Wei\net al., 2022a; Yang et al., 2023). Given In-context\nlearning or Chain-of-Thought prompts have already\nachieved high performance on challenging tasks\nsuch as commonsense, arithmetic, and symbolic\nreasoning (Imani et al., 2023; Lee et al., 2021;\nKojima et al., 2022). Logical reasoning is one of\nthe most important and long-standing problems in\nNLP (Hirschberg and Manning, 2015; Russell and\nNorvig, 2010), and integrating this ability into nat-\nural language understanding systems has always\nbeen a goal pursued (Du et al., 2022).\nNevertheless, scaling has been demonstrated\nto offer limited advantages in resolving complex\nlogical reasoning issues (Kazemi et al., 2022). For\nexample, Saparov and He (2022) show that Chain-\nof-Thought prompting struggles with proof planning\nfor more complex logical reasoning problems. Addi-\ntionally, the performance suffers greatly while han-\ndling recently released and out-of-distribution logi-\ncal reasoning datasets (Liu et al., 2023a). Despite\nmany works have explored variants of Chain-of-\nThought prompts to facilitate LLMs inference (Zelik-\nman et al., 2022; Zheng et al., 2023), we discover\nthat the present logical reasoning task prompts\nplace an excessive amount of emphasis on the\nreasoning process while ignoring the origin, pur-\npose, and effectiveness of reasoning (Creswell\net al., 2022; Xi et al., 2023). As examples shown in\nQ1: Bob is green. True/false? \nInput Facts: Alan is blue. Alan is \nrough. Alan is young. Bob is big. \nBob is round. Charlie is big. Charlie \nis blue. Charlie is green. Dave is \ngreen. Dave is rough.\nInput Rules: Big people are rough. \nIf someone is young and round then \nthey are kind. If someone is round \nand big then they are blue. All\nrough people are green.\nBob is big. \nBig people are rough. \nAll rough people are green.\nAnswer: T\nQ2: Dave is blue. True/false?\nLess inspiring\nFigure 1: Questions in RuleTaker involve logical\nreasoning with facts and rules.\nFigure 1, the difficulty in judging logical problems\narises not only from the process of reasoning but\nalso from the choice of facts and rules to use as a\nstarting point. Even if we were provided the thought\nprocess for some of the issues, it would not be very\nbeneficial for others, based on how we previously\ncreated the prompts.\nIn this paper, we propose Hypothesis Testing\nPrompting, a new and more considerate prompt\ntemplate design idea. Hypothesis testing is a for-\nmal procedure for investigating our ideas about\nthe world using statistics and is often used by sci-\nentists to test specific predictions (Bevans, 2022).\nWe draw inspiration from its process to introduce a\nprocess of conclusion assumptions, backward rea-\nsoning, and fact verification. Experiments on Rule-\nTaker (Clark et al., 2020) and ProofWriter (Tafjord\net al., 2021) show the effectiveness of our novel\nprompting paradigm as a strategy for promoting\ndeductive reasoning in large language models. Fur-\nther analyses show that Hypothesis Test prompting\ngenerates more desirable intermediate processes\narXiv:2405.06707v1  [cs.CL]  9 May 2024\nand significantly improves the "Unknown" label.\n2.\nRelated Work\n2.1.\nFew-Shot Prompting\nBrown et al. (2020) propose in-context learning as\nan alternative few-shot prompting way to stimulate\nability. Besides, chain-of-Thought (CoT) (Wei et al.,\n2022b) is one of the most well-known works, which\ndecomposes the problem into intermediate steps\nand further improves the ability of large language\nmodels. Subsequently, several follow-up works\nwere carried out, including Zero-shot-CoT (simply\nadding "Let’s think step by step" before each an-\nswer) (Kojima et al., 2022), Self-consistency (Wang\net al., 2022), complexity-based (Fu et al., 2022),\nand other prompting work (Liu et al., 2023b; Jung\net al., 2022; Zhou et al., 2022; Saparov and He,\n2022). While these methods enhance the perfor-\nmance of inference by paying attention to indica-\ntions of the reasoning process, they often overlook\nsome aspects such as identifying the root cause of\nthe problem, establishing efficient reasoning strate-\ngies, and determining the direction of logical rea-\nsoning.\n2.2.\nDeductive Reasoning\nDeductive reasoning is defined as the applica-\ntion of general concepts to particular circum-\nstances (Johnson-Laird, 2010). Making logical as-\nsumptions is the foundation of deductive reasoning,\nwhich then bases a conclusion on those assump-\ntions. The deduction task is then applied to a sit-\nuation from the actual world after starting with a\nrule. In light of the principles "All men are mortal."\nand "Socrates is a man." for example, we can draw\nthe conclusion that "Socrates is mortal." (Johnson-\nLaird, 1999).\n3.\nHypothesis Testing Prompting\nHypothesis testing is a formal procedure for investi-\ngating our ideas about the world using statistics and\nused by scientists to test specific predictions that\narise from theories (Bevans, 2022; La et al., 2012).\nThere are 5 main steps in hypothesis testing:\n1. State your research hypothesis;\n2. Collect data in a way designed to test the hy-\npothesis;\n3. Perform an appropriate statistical test;\n4. Decide whether to reject or fail to reject your\nnull hypothesis;\n5. Present the findings in your results and discus-\nsion section;\nWhen completing a challenging reasoning activ-\nity, such as a multi-step deductive reasoning prob-\nlem, one is not conducting random reasoning to\nobtain all possible intermediate results. We shall\nchoose the relevant conditions for inference ver-\nification after initially making assumptions about\nthe judgment problem, such as " First assume the\nconclusion is True and start from ... Then assume\nthe conclusion is False and start from ... because\nthe rules state that ... So the conclusion ...". The\npurpose of this study is to give language models\nthe capacity to build a process that is similar to\nwhat we defined as Hypothesis Testing Prompt-\ning. We will show that large language models can\ngenerate more appropriate thought and more ac-\ncurate results if demonstrations of hypothesis test\nprompting are provided in the exemplars for few-\nshot prompting. Figure 2 shows an example of a\nmodel producing a hypothesis testing thought to\nsolve a deductive reasoning problem.\n4.\nExperiment\n4.1.\nExperimental Setup\nWe explore Hypothesis Test Prompting for Chat-\nGPT (GPT-3.5-Turbo in the OpenAI API) on multiple\nlogical reasoning benchmarks.\nBenchmarks. Considering FOL reasoning in\nquestion answering systems, there are two world\nassumptions (Reiter, 1981) that result in different\nobjectives. One is the closed world assumption\n(CWA), which is the presumption that what is not\ncurrently known to be entailment is contradiction.\nThe other is the open world assumption (OWA),\nwhose objective should distinguish false proposi-\ntions from uncertain ones. Due to differences in\nworld assumptions, our analysis and solutions are\nalso different.\nWe consider the following two deductive reason-\ning problem benchmarks: (1) the RuleTaker (Clark\net al., 2020) benchmark using CWA assumption;\n(2) the ProofWriter (Tafjord et al., 2021) benchmark\nusing OWA assumption. Both datasets are divided\ninto five parts, each part requiring 0, ≤1, ≤2, ≤\n3, and ≤5 hops of reasoning, respectively. We\nconducted comparison tests on the test set of the\ntwo datasets for 5 distinct hops.\nStandard prompting. As one of the baselines,\nwe take into account the common few-shot prompt-\ning, made popular by Brown et al. (2020), in which\na language model is provided with in-context ex-\namples of input-output pairings before producing a\nprediction for a test-time example. Examples are\npresented in the form of questions and answers.\nAs seen in Figure 2(above), the model directly an-\nswers the question.\nChain-of-Thought prompting. We also com-\nChain-of-Thought Prompting\nModel Input\nStandard Prompting\nModel Input\nQ: Judge the following conclusion \'Harry is cold.\' is correct, wrong, or unknown, based on the following texts: Text 1\nA: The conclusion \'Harry is cold.\' is True.\nQ: Judge the following conclusion ‘Bob is not young.’ is correct, wrong, or unknown, based on the following texts: Text 2\nModel Output\nText 1: Charlie is rough. Charlie is white. Erin is blue. Erin is rough. \nErin is young. Gary is blue. Gary is green. Gary is quiet. Gary is \nwhite. Harry is blue. Harry is rough. All rough, white things are \ngreen. All young, blue things are quiet. If something is young then it \nis blue. White, blue things are quiet. If something is green then it is \nyoung. Quiet, green things are cold. All green, cold things are rough.\nQuestion: Harry is cold.     \nAnswer: True\nText 2: Bob is rough. Fiona is not furry. Fiona is white. Young, nice \nthings are big. Blue things are big. If something is young and white \nthen it is nice. All nice things are young. All white things are young. \nIf something is blue and not big then it is not rough.\nQuestion: Bob is not young. \nAnswer: Unknown\nQ: Judge the following conclusion \'Harry is cold.\' is correct, wrong, or unknown, based on the following texts: Text 1\nA: Start from \'Harry is quiet\' and \'Quiet things are nice\'. Then can know \'Harry is nice\'. And \'All nice things are cold\' is given. Finally \nit can be deduced \'Harry is cold\'. So the conclusion \'Harry is cold.\' is True.\nQ: Judge the following conclusion \'Harry is not quiet.\' is correct, wrong, or unknown, based on the following texts: Text  2\nModel Output\nHypothesis Testing Prompting\nModel Input\nQ: Judge the following conclusion \'Harry is cold.\' is correct, wrong, or unknown, based on the following texts: Text 1\nA: First assume the conclusion is True and start from Harry is cold. Because the rules state that all nice things are cold, correct\nconclusion needs to be satisfied that Harry is nice. And the rules states that quiet things are nice. It needs to be satisfied Harry is quiet, \nwhich is also stated in the facts. So the conclusion \'Harry is cold.\' is True.\nQ: Judge the following conclusion ‘Bob is not young.’ is correct, wrong, or unknown, based on the following texts: Text  2\nQ: The conclusion \'Bob is not young.\' is True.\nQ: Start from \'Bob is rough\' and \'If something is blue and not big then it is not rough\'. Then can know \'Bob is not blue\'. And \'All white \nthings are young\' is given. Finally it can be deduced \'Bob is not young\'. So the conclusion \'Bob is not young.\' is True.\nModel Output\nQ: First assume the conclusion is True and start from Bob is not young. Because the rules state that all white things are young, correct \nconclusion needs to be satisfied that Bob is not white. And there are no facts or rules about \'not white\'. Then assume the conclusion is \nFalse and start from Bob is young. Because the rules state that all nice things are young, correct conclusion needs to be satisfied that \nBob is nice. And there are no facts or rules about \'nice\'. So the conclusion \'Bob is not young.\' is Unknown.\n× N\n× N\n× N\nFigure 2: Comparison of three prompting methods: (a) Standard (b) Chain-of-Thought (c) Hypothesis\nTesting. Particularly, we highlight the Hypothesis testing reasoning processes. The comparative experi-\nmental results show that: Hypothesis testing prompting enables large language models to tackle complex\nlogical reasoning.\npare with Chain-of-thought prompting which has\nachieved encouraging results on complex reason-\ning tasks (Wei et al., 2022b).\nAs seen in Fig-\nure 2(middle), the model not only provides the final\nanswer but also comes with the consideration of\nintermediate steps.\nHypothesis Testing Prompting. Our proposed\napproach is to augment each exemplars in few-shot\nprompting with the thought of hypothesis testing for\nan associated answer, as illustrated in Figure 2(be-\nlow). We show one chain of thought exemplars\n(Example: Judge the following conclusion ’<Con-\nclusion>’ is true, false, or unknown, based on the\nfollowing facts and rules: <Facts> ... <Rules> ...).\n4.2.\nExperimental Results\nThe results for Hypothesis Testing Prompting and\nthe baselines on the RuleTaker datasets are pro-\nvided in Figure 3(a), and ProofWriter results are\nshown in Figure 3(b). From the results, we ob-\nserve that our method significantly outperforms the\nother two baselines, especially on ProofWriter. Fig-\nure 3(a) demonstrates that while CoT performs well\nin the low hop, Hypothesis Testing prompting per-\nforms better as the hops count increases on Rule-\nTaker. While on ProofWriter, our approach has a\nthorough lead (improved accuracy by over 4% on\nall hops). Comparing two datasets, the latter dis-\ntinguishes between "False" and "Unknown", which\ndemand a greater level of logic. The results on two\n0.97\n0.93\n0.83\n0.84\n0.81\n0.99\n0.92\n0.77\n0.8\n0.78\n0.78\n0.72\n0.63\n0.63\n0.65\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\ndepth-0\ndepth-1\ndepth-2\ndepth-3\ndepth-5\nAccuracy\nStandard Prompting\nChain-of-Thought Prompting\nHypothesis Testing Prompting\n(a)\n0.6\n0.41\n0.39\n0.34\n0.33\n0.77\n0.54\n0.58\n0.56\n0.5\n0.82\n0.63\n0.62\n0.61\n0.57\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\ndepth-0\ndepth-1\nd
