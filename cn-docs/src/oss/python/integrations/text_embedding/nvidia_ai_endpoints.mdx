---
title: NVIDIAEmbeddings
---
`langchain-nvidia-ai-endpoints` 包包含了 LangChain 与 NVIDIA NIM 推理微服务上的模型构建应用程序的集成。NIM 支持来自社区和 NVIDIA 的跨领域模型，如聊天、嵌入和重排序模型。这些模型由 NVIDIA 优化，以在 NVIDIA 加速基础设施上提供最佳性能，并部署为 NIM，这是一种易于使用的预构建容器，可在 NVIDIA 加速基础设施上使用单个命令随处部署。

NVIDIA 托管的 NIM 部署可在 [NVIDIA API 目录](https://build.nvidia.com/) 上进行测试。测试后，可以使用 NVIDIA AI Enterprise 许可证从 NVIDIA 的 API 目录导出 NIM，并在本地或云端运行，使企业能够拥有并完全控制其 IP 和 AI 应用程序。

NIM 以每个模型为基础打包为容器镜像，并通过 NVIDIA NGC 目录作为 NGC 容器镜像分发。NIM 的核心是为 AI 模型运行推理提供简单、一致且熟悉的 API。

本示例介绍了如何通过 `NVIDIAEmbeddings` 类，使用 LangChain 与支持的 [NVIDIA Retrieval QA 嵌入模型](https://build.nvidia.com/nvidia/embed-qa-4) 进行交互，以实现 [检索增强生成](https://developer.nvidia.com/blog/build-enterprise-retrieval-augmented-generation-apps-with-nvidia-retrieval-qa-embedding-model/)。

有关通过此 API 访问聊天模型的更多信息，请查看 [ChatNVIDIA](https://python.langchain.com/docs/integrations/chat/nvidia_ai_endpoints/) 文档。

## 安装

```python
pip install -qU  langchain-nvidia-ai-endpoints
```

## 设置

**开始之前：**

1.  在托管 NVIDIA AI Foundation 模型的 [NVIDIA](https://build.nvidia.com/) 上创建一个免费账户。
2.  选择 `Retrieval` 选项卡，然后选择您想要的模型。
3.  在 `Input` 下选择 `Python` 选项卡，然后单击 `Get API Key`。接着点击 `Generate Key`。
4.  复制并保存生成的密钥为 `NVIDIA_API_KEY`。之后，您应该可以访问这些端点。

```python
import getpass
import os

# del os.environ['NVIDIA_API_KEY']  ## 删除密钥并重置
if os.environ.get("NVIDIA_API_KEY", "").startswith("nvapi-"):
    print("环境中已存在有效的 NVIDIA_API_KEY。删除以重置")
else:
    nvapi_key = getpass.getpass("NVAPI 密钥（以 nvapi- 开头）: ")
    assert nvapi_key.startswith("nvapi-"), f"{nvapi_key[:5]}... 不是有效的密钥"
    os.environ["NVIDIA_API_KEY"] = nvapi_key
```

我们应该能在列表中看到一个嵌入模型，该模型可以与 LLM 结合使用以实现有效的 RAG 解决方案。我们可以通过 `NVIDIAEmbeddings` 类与此模型以及 NIM 支持的其他嵌入模型进行交互。

## 在 NVIDIA API 目录上使用 NIM

初始化嵌入模型时，您可以通过传递模型名称来选择模型，例如下面的 `NV-Embed-QA`，或者不传递任何参数以使用默认模型。

```python
from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings

embedder = NVIDIAEmbeddings(model="NV-Embed-QA")
```

该模型是一个经过微调的 E5-large 模型，支持预期的 @[`Embeddings`] 方法，包括：

-   `embed_query`：为查询样本生成查询嵌入。
-   `embed_documents`：为您想要搜索的文档列表生成段落嵌入。
-   `aembed_query`/`aembed_documents`：上述方法的异步版本。

## 使用自托管的 NVIDIA NIM

准备部署时，您可以使用 NVIDIA NIM（包含在 NVIDIA AI Enterprise 软件许可证中）自托管模型，并在任何地方运行它们，从而拥有您的自定义项并完全控制您的知识产权 (IP) 和 AI 应用程序。

[了解更多关于 NIM 的信息](https://developer.nvidia.com/blog/nvidia-nim-offers-optimized-inference-microservices-for-deploying-ai-models-at-scale/)

```python
from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings

# 连接到运行在 localhost:8080 的嵌入 NIM
embedder = NVIDIAEmbeddings(base_url="http://localhost:8080/v1")
```

### **相似度**

以下是对这些数据点相似度的快速测试：

**查询：**

-   What's the weather like in Komchatka?
-   What kinds of food is Italy known for?
-   What's my name? I bet you don't remember...
-   What's the point of life anyways?
-   The point of life is to have fun :D

**文档：**

-   Komchatka's weather is cold, with long, severe winters.
-   Italy is famous for pasta, pizza, gelato, and espresso.
-   I can't recall personal names, only provide information.
-   Life's purpose varies, often seen as personal fulfillment.
-   Enjoying life's moments is indeed a wonderful approach.

### 嵌入运行时

```python
print("\n顺序嵌入: ")
q_embeddings = [
    embedder.embed_query("What's the weather like in Komchatka?"),
    embedder.embed_query("What kinds of food is Italy known for?"),
    embedder.embed_query("What's my name? I bet you don't remember..."),
    embedder.embed_query("What's the point of life anyways?"),
    embedder.embed_query("The point of life is to have fun :D"),
]
print("形状:", (len(q_embeddings), len(q_embeddings[0])))
```

### 文档嵌入

```python
print("\n批量文档嵌入: ")
d_embeddings = embedder.embed_documents(
    [
        "Komchatka's weather is cold, with long, severe winters.",
        "Italy is famous for pasta, pizza, gelato, and espresso.",
        "I can't recall personal names, only provide information.",
        "Life's purpose varies, often seen as personal fulfillment.",
        "Enjoying life's moments is indeed a wonderful approach.",
    ]
)
print("形状:", (len(d_embeddings), len(d_embeddings[0])))
```

现在我们已经生成了嵌入向量，我们可以对结果进行简单的相似度检查，看看在检索任务中哪些文档会被触发作为合理的答案：

```python
pip install -qU  matplotlib scikit-learn
```

```python
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# 计算 q_embeddings 和 d_embeddings 之间的相似度矩阵
cross_similarity_matrix = cosine_similarity(
    np.array(q_embeddings),
    np.array(d_embeddings),
)

# 绘制交叉相似度矩阵
plt.figure(figsize=(8, 6))
plt.imshow(cross_similarity_matrix, cmap="Greens", interpolation="nearest")
plt.colorbar()
plt.title("交叉相似度矩阵")
plt.xlabel("查询嵌入")
plt.ylabel("文档嵌入")
plt.grid(True)
plt.show()
```

提醒一下，发送到我们系统的查询和文档是：

**查询：**

-   What's the weather like in Komchatka?
-   What kinds of food is Italy known for?
-   What's my name? I bet you don't remember...
-   What's the point of life anyways?
-   The point of life is to have fun :D

**文档：**

-   Komchatka's weather is cold, with long, severe winters.
-   Italy is famous for pasta, pizza, gelato, and espresso.
-   I can't recall personal names, only provide information.
-   Life's purpose varies, often seen as personal fulfillment.
-   Enjoying life's moments is indeed a wonderful approach.

## 截断

嵌入模型通常具有固定的上下文窗口，该窗口决定了可以嵌入的最大输入令牌数。这个限制可能是一个硬限制，等于模型的最大输入令牌长度，也可能是一个有效限制，超过该限制后嵌入的准确性会下降。

由于模型对令牌进行操作，而应用程序通常处理文本，因此应用程序确保其输入保持在模型的令牌限制内可能具有挑战性。默认情况下，如果输入过大，则会抛出异常。

为了帮助解决这个问题，NVIDIA 的 NIM（API 目录或本地）提供了一个 `truncate` 参数，如果输入过大，该参数会在服务器端截断输入。

`truncate` 参数有三个选项：

-   "NONE"：默认选项。如果输入过大，则抛出异常。
-   "START"：服务器从开头（左侧）截断输入，根据需要丢弃令牌。
-   "END"：服务器从末尾（右侧）截断输入，根据需要丢弃令牌。

```python
long_text = "AI is amazing, amazing is " * 100
```

```python
strict_embedder = NVIDIAEmbeddings()
try:
    strict_embedder.embed_query(long_text)
except Exception as e:
    print("错误:", e)
```

```python
truncating_embedder = NVIDIAEmbeddings(truncate="END")
truncating_embedder.embed_query(long_text)[:5]
```

## RAG 检索

以下是 [LangChain 表达式语言检索 Cookbook 条目](https://python.langchain.com/docs/expression_language/cookbook/retrieval) 初始示例的重新利用，但使用 AI Foundation Models 的 [Mixtral 8x7B Instruct](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/mixtral-8x7b) 和 [NVIDIA Retrieval QA Embedding](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/nvolve-40k) 模型在其 playground 环境中执行。Cookbook 中的后续示例也按预期运行，我们鼓励您使用这些选项进行探索。

**提示：** 我们建议使用 Mixtral 进行内部推理（即遵循指令进行数据提取、工具选择等），并使用 Llama-Chat 进行单一的最终“根据历史和上下文为此用户生成一个简单的总结性响应”响应。

```python
pip install -qU  langchain faiss-cpu tiktoken langchain-community

from operator import itemgetter

from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_nvidia_ai_endpoints import ChatNVIDIA
```

```python
vectorstore = FAISS.from_texts(
    ["harrison worked at kensho"],
    embedding=NVIDIAEmbeddings(model="NV-Embed-QA"),
)
retriever = vectorstore.as_retriever()

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "Answer solely based on the following context:\n<Documents>\n{context}\n</Documents>",
        ),
        ("user", "{question}"),
    ]
)

model = ChatNVIDIA(model="ai-mixtral-8x7b-instruct")

chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)

chain.invoke("where did harrison work?")
```

```python
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "Answer using information solely based on the following context:\n<Documents>\n{context}\n</Documents>"
            "\nSpeak only in the following language: {language}",
        ),
        ("user", "{question}"),
    ]
)

chain = (
    {
        "context": itemgetter("question") | retriever,
        "question": itemgetter("question"),
        "language": itemgetter("language"),
    }
    | prompt
    | model
    | StrOutputParser()
)

chain.invoke({"question": "where did harrison work", "language": "italian"})
```
