---
title: 特定应用场景的评估方法
sidebarTitle: Evaluation approaches
---
下面我们将讨论几种流行的 LLM 应用类型的评估。

## 智能体（Agents）

[LLM 驱动的自主智能体](https://lilianweng.github.io/posts/2023-06-23-agent/)结合了三个组件：(1) 工具调用，(2) 记忆，和 (3) 规划。智能体通过规划（例如，通常通过提示）和记忆（例如，通常是短期的消息历史）来[使用工具调用](https://python.langchain.com/v0.1/docs/modules/agents/agent_types/tool_calling/)生成响应。[工具调用](https://python.langchain.com/v0.1/docs/modules/model_io/chat/function_calling/)允许模型通过生成两样东西来响应给定的提示：(1) 要调用的工具和 (2) 所需的输入参数。

![工具使用](/langsmith/images/tool-use.png)

以下是 [LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/) 中的一个工具调用智能体。`assistant node` 是一个 LLM，它根据输入决定是否调用工具。`tool condition` 检查 `assistant node` 是否选择了工具，如果是，则路由到 `tool node`。`tool node` 执行工具并将输出作为工具消息返回给 `assistant node`。只要 `assistant node` 选择工具，这个循环就会继续。如果没有选择工具，则智能体直接返回 LLM 的响应。

![智能体](/langsmith/images/langgraph-agent.png)

这为智能体评估设置了三种用户通常感兴趣的一般类型：

* `最终响应`：评估智能体的最终响应。
* `单步评估`：独立评估智能体的任何步骤（例如，它是否选择了合适的工具）。
* `轨迹评估`：评估智能体是否采取了预期的路径（例如，工具调用序列）来得出最终答案。

![智能体评估](/langsmith/images/agent-eval.png)

下面我们将介绍这些是什么，每种评估所需的组件（输入、输出、评估器），以及何时应考虑使用它们。请注意，您可能希望进行多种（如果不是全部！）这些类型的评估——它们并不相互排斥！

### 评估智能体的最终响应

评估智能体的一种方法是评估其在任务上的整体表现。这基本上是将智能体视为一个黑盒，简单地评估它是否完成了工作。

输入应该是用户输入和（可选的）工具列表。在某些情况下，工具是作为智能体的一部分硬编码的，不需要传入。在其他情况下，智能体更通用，意味着它没有固定的工具集，需要在运行时传入工具。

输出应该是智能体的最终响应。

评估器根据您要求智能体执行的任务而变化。许多智能体执行一系列相对复杂的步骤并输出最终的文本响应。与 RAG 类似，在这些情况下，`LLM-as-judge` 评估器通常对评估有效，因为它们可以直接从文本响应中评估智能体是否完成了工作。

然而，这种类型的评估有几个缺点。首先，它通常需要一段时间来运行。其次，您没有评估智能体内部发生的任何事情，因此在发生故障时可能难以调试。第三，有时可能很难定义合适的评估指标。

### 评估智能体的单个步骤

智能体通常执行多个动作。虽然端到端地评估它们很有用，但评估这些单独的动作也很有用。这通常涉及评估智能体的单个步骤——即决定下一步做什么的 LLM 调用。

输入应该是单个步骤的输入。根据您测试的内容，这可能只是原始用户输入（例如，提示和/或一组工具），也可以包括先前完成的步骤。

输出只是该步骤的输出，通常是 LLM 响应。LLM 响应通常包含工具调用，指示智能体下一步应采取什么动作。

此评估的评估器通常是一些二元分数，用于判断是否选择了正确的工具调用，以及一些启发式方法来判断工具的输入是否正确。参考工具可以简单地指定为一个字符串。

这种类型的评估有几个好处。它允许您评估单个动作，从而可以精确定位应用程序可能失败的地方。它们的运行速度也相对较快（因为它们只涉及一次 LLM 调用），并且评估通常使用相对于参考工具的简单启发式评估。一个缺点是它们不能捕捉完整的智能体——只能捕捉一个特定的步骤。另一个缺点是数据集创建可能具有挑战性，特别是如果您想在智能体输入中包含过去的历史记录。为智能体轨迹早期的步骤生成数据集相当容易（例如，这可能只包括输入提示），但为轨迹后期的步骤生成数据集可能很困难（例如，包括许多先前的智能体动作和响应）。

### 评估智能体的轨迹

评估智能体的轨迹涉及评估智能体采取的所有步骤。

输入同样是整个智能体的输入（用户输入，以及可选的工具列表）。

输出是工具调用列表，可以表述为“精确”轨迹（例如，预期的工具调用序列），或者只是一组预期的工具调用（以任何顺序）。

这里的评估器是对所采取步骤的某种函数。评估“精确”轨迹可以使用一个二元分数来确认序列中每个工具名称的完全匹配。这很简单，但有一些缺陷。有时可能存在多个正确的路径。这种评估也不能捕捉轨迹仅差一步与完全错误之间的差异。

为了解决这些缺陷，评估指标可以侧重于所采取的“错误”步骤的数量，这能更好地解释接近的轨迹与显著偏离的轨迹之间的差异。评估指标也可以侧重于是否以任何顺序调用了所有预期的工具。

然而，这些方法都没有评估工具的输入；它们只关注选择的工具。为了解决这个问题，另一种评估技术是将智能体的完整轨迹（连同参考轨迹）作为一组消息（例如，所有 LLM 响应和工具调用）传递给 `LLM-as-judge`。这可以评估智能体的完整行为，但这是最具挑战性的参考数据来编译（幸运的是，使用像 LangGraph 这样的框架可以帮助解决这个问题！）。另一个缺点是评估指标可能有些难以确定。

## 检索增强生成（RAG）

检索增强生成（RAG）是一种强大的技术，它涉及根据用户的输入检索相关文档，并将其传递给语言模型进行处理。RAG 使 AI 应用程序能够通过利用外部知识生成更具信息性和上下文感知的响应。

<Info>

有关 RAG 概念的全面回顾，请参阅我们的 [`RAG From Scratch` 系列](https://github.com/langchain-ai/rag-from-scratch)。

</Info>

### 数据集

在评估 RAG 应用程序时，一个关键的考虑因素是您是否拥有（或可以轻松获得）每个输入问题的参考答案。参考答案作为评估生成响应正确性的基本事实。然而，即使没有参考答案，仍然可以使用无参考的 RAG 评估提示（下面提供了示例）进行各种评估。

### 评估器

`LLM-as-judge` 是 RAG 常用的评估器，因为它是评估文本之间事实准确性或一致性的有效方法。

![rag-types.png](/langsmith/images/rag-types.png)

评估 RAG 应用程序时，可以有需要参考输出的评估器和不需要的评估器：

1. **需要参考输出**：将 RAG 链生成的答案或检索结果与参考答案（或检索结果）进行比较，以评估其正确性。
2. **不需要参考输出**：使用不需要参考答案的提示进行自洽性检查（上图中以橙色、绿色和红色表示）。

### 应用 RAG 评估

应用 RAG 评估时，请考虑以下方法：

1. `离线评估`：对任何依赖参考答案的提示使用离线评估。这最常用于 RAG 答案正确性评估，其中参考是基本事实（正确）答案。

2. `在线评估`：对任何无参考提示使用在线评估。这允许您在实时场景中评估 RAG 应用程序的性能。

3. `成对比较评估`：利用成对比较评估来比较不同 RAG 链产生的答案。此评估侧重于用户指定的标准（例如，答案格式或风格），而不是正确性，后者可以使用自洽性或基本事实参考来评估。

### RAG 评估摘要

| 评估器           | 详情                                             | 需要参考输出 | LLM-as-judge?                                                                         | 与成对比较相关 |
| ---------------- | ------------------------------------------------ | ------------ | ------------------------------------------------------------------------------------- | -------------- |
| 文档相关性       | 文档是否与问题相关？                             | 否           | 是 - [提示](https://smith.langchain.com/hub/langchain-ai/rag-document-relevance)      | 否             |
| 答案忠实度       | 答案是否基于文档？                               | 否           | 是 - [提示](https://smith.langchain.com/hub/langchain-ai/rag-answer-hallucination)    | 否             |
| 答案有用性       | 答案是否有助于解决问题？                         | 否           | 是 - [提示](https://smith.langchain.com/hub/langchain-ai/rag-answer-helpfulness)      | 否             |
| 答案正确性       | 答案是否与参考答案一致？                         | 是           | 是 - [提示](https://smith.langchain.com/hub/langchain-ai/rag-answer-vs-reference)     | 否             |
| 成对比较         | 多个答案版本如何比较？                           | 否           | 是 - [提示](https://smith.langchain.com/hub/langchain-ai/pairwise-evaluation-rag)     | 是             |

## 摘要生成

摘要生成是自由形式写作的一种特定类型。评估目标通常是根据一组标准检查写作（摘要）。

`开发者整理的示例` 文本用于摘要评估很常见（参见数据集示例[此处](https://smith.langchain.com/public/659b07af-1cab-4e18-b21a-91a69a4c3990/d)）。然而，来自生产（摘要）应用程序的 `用户日志` 可以与下面任何 `无参考` 评估提示一起用于在线评估。

`LLM-as-judge` 通常用于使用 `无参考` 提示来评估摘要（以及其他类型的写作），这些提示遵循提供的标准对摘要进行评分。提供特定的 `参考` 摘要不太常见，因为摘要是一项创造性任务，存在许多可能的正确答案。

由于使用了 `无参考` 提示，`在线` 或 `离线` 评估都是可行的。`成对比较` 评估也是在不同摘要链（例如，不同的摘要提示或 LLM）之间进行比较的强大方式：

| 用例         | 详情                                                                     | 需要参考输出 | LLM-as-judge?                                                                                | 与成对比较相关 |
| ------------ | ------------------------------------------------------------------------ | ------------ | -------------------------------------------------------------------------------------------- | -------------- |
| 事实准确性   | 摘要相对于源文档是否准确？                                               | 否           | 是 - [提示](https://smith.langchain.com/hub/langchain-ai/summary-accurancy-evaluator)        | 是             |
| 忠实度       | 摘要是否基于源文档（例如，没有幻觉）？                                   | 否           | 是 - [提示](https://smith.langchain.com/hub/langchain-ai/summary-hallucination-evaluator)    | 是             |
| 有用性       | 摘要是否有助于满足用户需求？                                             | 否           | 是 - [提示](https://smith.langchain.com/hub/langchain-ai/summary-helpfulness-evaluator)      | 是             |

## 分类和标记

分类和标记为给定输入应用标签（例如，用于毒性检测、情感分析等）。分类/标记评估通常采用以下组件，我们将在下面详细回顾：

分类/标记评估的一个核心考虑因素是您是否拥有带有 `参考` 标签的数据集。如果没有，用户通常希望定义一个评估器，该评估器使用标准将标签（例如，毒性等）应用于输入（例如，文本、用户问题等）。但是，如果提供了基本事实类别标签，则评估目标侧重于根据基本事实类别标签对分类/标记链进行评分（例如，使用精确率、召回率等指标）。

如果提供了基本事实参考标签，通常只需定义一个[自定义启发式评估器](/langsmith/code-evaluator)来比较基本事实标签和链输出。然而，随着 LLM 的出现，越来越常见的做法是简单地使用 `LLM-as-judge` 根据指定的标准（无需基本事实参考）对输入进行分类/标记。

当使用 `LLM-as-judge` 和 `无参考` 提示时，`在线` 或 `离线` 评估是可行的。特别是，当用户想要标记/分类应用程序输入（例如，用于毒性检测等）时，这非常适合 `在线` 评估。

| 用例     | 详情               | 需要参考输出 | LLM-as-judge? | 与成对比较相关 |
| -------- | ------------------ | ------------ | ------------- | -------------- |
| 准确率   | 标准定义           | 是           | 否            | 否             |
| 精确率   | 标准定义           | 是           | 否            | 否             |
| 召回率   | 标准定义           | 是           | 否            | 否             |
