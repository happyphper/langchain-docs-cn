---
title: Snowflake Cortex
---
[Snowflake Cortex](https://docs.snowflake.com/en/user-guide/snowflake-cortex/llm-functions) 让您能够即时访问由 Mistral、Reka、Meta 和 Google 等公司的研究人员训练的行业领先的大型语言模型 (LLMs)，其中包括由 Snowflake 开发的开源企业级模型 [Snowflake Arctic](https://www.snowflake.com/en/data-cloud/arctic/)。

本示例将介绍如何使用 LangChain 与 Snowflake Cortex 进行交互。

### 安装与设置

我们首先使用下面的命令安装 `snowflake-snowpark-python` 库。然后，我们将配置连接 Snowflake 所需的凭据，可以通过环境变量设置，也可以直接传递。

```python
pip install -qU snowflake-snowpark-python
```

```python
import getpass
import os

# 第一步是设置环境变量以连接到 Snowflake，
# 您也可以在实例化模型时直接传递这些 Snowflake 凭据

if os.environ.get("SNOWFLAKE_ACCOUNT") is None:
    os.environ["SNOWFLAKE_ACCOUNT"] = getpass.getpass("Account: ")

if os.environ.get("SNOWFLAKE_USERNAME") is None:
    os.environ["SNOWFLAKE_USERNAME"] = getpass.getpass("Username: ")

if os.environ.get("SNOWFLAKE_PASSWORD") is None:
    os.environ["SNOWFLAKE_PASSWORD"] = getpass.getpass("Password: ")

if os.environ.get("SNOWFLAKE_DATABASE") is None:
    os.environ["SNOWFLAKE_DATABASE"] = getpass.getpass("Database: ")

if os.environ.get("SNOWFLAKE_SCHEMA") is None:
    os.environ["SNOWFLAKE_SCHEMA"] = getpass.getpass("Schema: ")

if os.environ.get("SNOWFLAKE_WAREHOUSE") is None:
    os.environ["SNOWFLAKE_WAREHOUSE"] = getpass.getpass("Warehouse: ")

if os.environ.get("SNOWFLAKE_ROLE") is None:
    os.environ["SNOWFLAKE_ROLE"] = getpass.getpass("Role: ")
```

```python
from langchain_community.chat_models import ChatSnowflakeCortex
from langchain.messages import HumanMessage, SystemMessage

# 默认情况下，我们将使用 cortex 提供的模型：`mistral-large` 和函数：`complete`
chat = ChatSnowflakeCortex()
```

上面的代码单元假设您的 Snowflake 凭据已设置在环境变量中。如果您希望手动指定它们，请使用以下代码：

```python
chat = ChatSnowflakeCortex(
    # 更改默认的 cortex 模型和函数
    model="mistral-large",
    cortex_function="complete",

    # 更改默认的生成参数
    temperature=0,
    max_tokens=10,
    top_p=0.95,

    # 指定您的 Snowflake 凭据
    account="YOUR_SNOWFLAKE_ACCOUNT",
    username="YOUR_SNOWFLAKE_USERNAME",
    password="YOUR_SNOWFLAKE_PASSWORD",
    database="YOUR_SNOWFLAKE_DATABASE",
    schema="YOUR_SNOWFLAKE_SCHEMA",
    role="YOUR_SNOWFLAKE_ROLE",
    warehouse="YOUR_SNOWFLAKE_WAREHOUSE"
)
```

### 调用聊天模型

现在，我们可以使用 `invoke` 或 `stream` 方法来调用聊天模型。

messages = [
SystemMessage(content="You are a friendly assistant."),
HumanMessage(content="What are large language models?"),
]
chat.invoke(messages)

### 流式传输

```python
# 示例输入提示
messages = [
    SystemMessage(content="You are a friendly assistant."),
    HumanMessage(content="What are large language models?"),
]

# 调用 stream 方法并打印每个到达的块
print("Stream Method Response:")
for chunk in chat._stream(messages):
    print(chunk.message.content)
```
