---
title: æ— ç—•çˆ¬å–
---
[**Scrapeless**](https://www.scrapeless.com/) æä¾›çµæ´»ä¸”åŠŸèƒ½ä¸°å¯Œçš„æ•°æ®è·å–æœåŠ¡ï¼Œæ”¯æŒå¹¿æ³›çš„å‚æ•°å®šåˆ¶å’Œå¤šæ ¼å¼å¯¼å‡ºã€‚è¿™äº›èƒ½åŠ›ä½¿ LangChain èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°é›†æˆå’Œåˆ©ç”¨å¤–éƒ¨æ•°æ®ã€‚å…¶æ ¸å¿ƒåŠŸèƒ½æ¨¡å—åŒ…æ‹¬ï¼š

**DeepSerp**

- **Google æœç´¢**ï¼šæ”¯æŒå…¨é¢æå–æ‰€æœ‰ç»“æœç±»å‹çš„ Google SERP æ•°æ®ã€‚
  - æ”¯æŒé€‰æ‹©æœ¬åœ°åŒ–çš„ Google åŸŸåï¼ˆä¾‹å¦‚ `google.com`ã€`google.ad`ï¼‰ä»¥è·å–ç‰¹å®šåŒºåŸŸçš„æœç´¢ç»“æœã€‚
  - æ”¯æŒåˆ†é¡µï¼Œå¯è·å–ç¬¬ä¸€é¡µä¹‹åçš„ç»“æœã€‚
  - æ”¯æŒæœç´¢ç»“æœè¿‡æ»¤å¼€å…³ï¼Œç”¨äºæ§åˆ¶æ˜¯å¦æ’é™¤é‡å¤æˆ–ç›¸ä¼¼å†…å®¹ã€‚
- **Google è¶‹åŠ¿**ï¼šä» Google æ£€ç´¢å…³é”®è¯è¶‹åŠ¿æ•°æ®ï¼ŒåŒ…æ‹¬éšæ—¶é—´å˜åŒ–çš„æµè¡Œåº¦ã€åŒºåŸŸå…´è¶£åº¦å’Œç›¸å…³æœç´¢ã€‚
  - æ”¯æŒå¤šå…³é”®è¯å¯¹æ¯”ã€‚
  - æ”¯æŒå¤šç§æ•°æ®ç±»å‹ï¼š`interest_over_time`ã€`interest_by_region`ã€`related_queries` å’Œ `related_topics`ã€‚
  - å…è®¸æŒ‰ç‰¹å®š Google å±æ€§ï¼ˆç½‘é¡µã€YouTubeã€æ–°é—»ã€è´­ç‰©ï¼‰è¿›è¡Œè¿‡æ»¤ï¼Œä»¥è¿›è¡Œç‰¹å®šæ¥æºçš„è¶‹åŠ¿åˆ†æã€‚

**é€šç”¨æŠ“å–**

- ä¸“ä¸ºç°ä»£ã€JavaScript å¯†é›†çš„ç½‘ç«™è®¾è®¡ï¼Œå…è®¸æå–åŠ¨æ€å†…å®¹ã€‚
  - æ”¯æŒå…¨çƒä¼˜è´¨ä»£ç†ï¼Œç”¨äºç»•è¿‡åœ°ç†é™åˆ¶å¹¶æé«˜å¯é æ€§ã€‚

**çˆ¬è™«**

- **çˆ¬å–**ï¼šé€’å½’çˆ¬å–ç½‘ç«™åŠå…¶é“¾æ¥é¡µé¢ï¼Œä»¥æå–å…¨ç«™å†…å®¹ã€‚
  - æ”¯æŒå¯é…ç½®çš„çˆ¬å–æ·±åº¦å’Œé™å®šèŒƒå›´çš„ URL ç›®æ ‡ã€‚
- **æŠ“å–**ï¼šé«˜ç²¾åº¦åœ°ä»å•ä¸ªç½‘é¡µæå–å†…å®¹ã€‚
  - æ”¯æŒâ€œä»…ä¸»è¦å†…å®¹â€æå–ï¼Œä»¥æ’é™¤å¹¿å‘Šã€é¡µè„šå’Œå…¶ä»–éå¿…è¦å…ƒç´ ã€‚
  - å…è®¸æ‰¹é‡æŠ“å–å¤šä¸ªç‹¬ç«‹çš„ URLã€‚

## æ¦‚è¿°

### é›†æˆè¯¦æƒ…

| ç±» | åŒ… | å¯åºåˆ—åŒ– | JS æ”¯æŒ | ç‰ˆæœ¬ |
| :--- | :--- | :---: | :---: | :---: |
| [ScrapelessCrawlerScrapeTool](https://pypi.org/project/langchain-scrapeless/) | [langchain-scrapeless](https://pypi.org/project/langchain-scrapeless/) | âœ… | âŒ |  ![PyPI - Version](https://img.shields.io/pypi/v/langchain-scrapeless?style=flat-square&label=%20) |
| [ScrapelessCrawlerCrawlTool](https://pypi.org/project/langchain-scrapeless/) | [langchain-scrapeless](https://pypi.org/project/langchain-scrapeless/) | âœ… | âŒ |  ![PyPI - Version](https://img.shields.io/pypi/v/langchain-scrapeless?style=flat-square&label=%20) |

### å·¥å…·ç‰¹æ€§

|åŸç”Ÿå¼‚æ­¥|è¿”å›å·¥ä»¶|è¿”å›æ•°æ®|
|:-:|:-:|:-:|
|âœ…|âœ…|markdown, rawHtml, screenshot@fullPage, json, links, screenshot, html|

## è®¾ç½®

è¯¥é›†æˆä½äº `langchain-scrapeless` åŒ…ä¸­ã€‚
!pip install langchain-scrapeless

### å‡­è¯

æ‚¨éœ€è¦ä¸€ä¸ª Scrapeless API å¯†é’¥æ‰èƒ½ä½¿ç”¨æ­¤å·¥å…·ã€‚æ‚¨å¯ä»¥å°†å…¶è®¾ç½®ä¸ºç¯å¢ƒå˜é‡ï¼š

```python
import os

os.environ["SCRAPELESS_API_KEY"] = "your-api-key"
```

## å®ä¾‹åŒ–

### ScrapelessCrawlerScrapeTool

ScrapelessCrawlerScrapeTool å…è®¸æ‚¨ä½¿ç”¨ Scrapeless çš„ Crawler Scrape API ä»ä¸€ä¸ªæˆ–å¤šä¸ªç½‘ç«™æŠ“å–å†…å®¹ã€‚æ‚¨å¯ä»¥æå–ä¸»è¦å†…å®¹ã€æ§åˆ¶æ ¼å¼ã€è¯·æ±‚å¤´ã€ç­‰å¾…æ—¶é—´å’Œè¾“å‡ºç±»å‹ã€‚

è¯¥å·¥å…·æ¥å—ä»¥ä¸‹å‚æ•°ï¼š

- `urls` ï¼ˆå¿…éœ€ï¼ŒList[str]ï¼‰ï¼šè¦æŠ“å–çš„ä¸€ä¸ªæˆ–å¤šä¸ªç½‘ç«™çš„ URLã€‚
- `formats` ï¼ˆå¯é€‰ï¼ŒList[str]ï¼‰ï¼šå®šä¹‰æŠ“å–è¾“å‡ºçš„æ ¼å¼ã€‚é»˜è®¤ä¸º `['markdown']`ã€‚é€‰é¡¹åŒ…æ‹¬ï¼š
  - `'markdown'`
  - `'rawHtml'`
  - `'screenshot@fullPage'`
  - `'json'`
  - `'links'`
  - `'screenshot'`
  - `'html'`
- `only_main_content` ï¼ˆå¯é€‰ï¼Œboolï¼‰ï¼šæ˜¯å¦ä»…è¿”å›é¡µé¢ä¸»è¦å†…å®¹ï¼Œæ’é™¤é¡µçœ‰ã€å¯¼èˆªæ ã€é¡µè„šç­‰ã€‚é»˜è®¤ä¸º Trueã€‚
- `include_tags` ï¼ˆå¯é€‰ï¼ŒList[str]ï¼‰ï¼šè¦åŒ…å«åœ¨è¾“å‡ºä¸­çš„ HTML æ ‡ç­¾åˆ—è¡¨ï¼ˆä¾‹å¦‚ `['h1', 'p']`ï¼‰ã€‚å¦‚æœè®¾ç½®ä¸º Noneï¼Œåˆ™ä¸æ˜¾å¼åŒ…å«ä»»ä½•æ ‡ç­¾ã€‚
- `exclude_tags` ï¼ˆå¯é€‰ï¼ŒList[str]ï¼‰ï¼šè¦ä»è¾“å‡ºä¸­æ’é™¤çš„ HTML æ ‡ç­¾åˆ—è¡¨ã€‚å¦‚æœè®¾ç½®ä¸º Noneï¼Œåˆ™ä¸æ˜¾å¼æ’é™¤ä»»ä½•æ ‡ç­¾ã€‚
- `headers` ï¼ˆå¯é€‰ï¼ŒDict[str, str]ï¼‰ï¼šéšè¯·æ±‚å‘é€çš„è‡ªå®šä¹‰è¯·æ±‚å¤´ï¼ˆä¾‹å¦‚ç”¨äº Cookie æˆ– User-Agentï¼‰ã€‚é»˜è®¤ä¸º Noneã€‚
- `wait_for` ï¼ˆå¯é€‰ï¼Œintï¼‰ï¼šæŠ“å–å‰çš„ç­‰å¾…æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ã€‚ç”¨äºç»™é¡µé¢æ—¶é—´å®Œå…¨åŠ è½½ã€‚é»˜è®¤ä¸º `0`ã€‚
- `timeout` ï¼ˆå¯é€‰ï¼Œintï¼‰ï¼šè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ã€‚é»˜è®¤ä¸º `30000`ã€‚

### ScrapelessCrawlerCrawlTool

ScrapelessCrawlerCrawlTool å…è®¸æ‚¨ä½¿ç”¨ Scrapeless çš„ Crawler Crawl API ä»åŸºç¡€ URL å¼€å§‹çˆ¬å–ç½‘ç«™ã€‚å®ƒæ”¯æŒ URL çš„é«˜çº§è¿‡æ»¤ã€çˆ¬å–æ·±åº¦æ§åˆ¶ã€å†…å®¹æŠ“å–é€‰é¡¹ã€è¯·æ±‚å¤´è‡ªå®šä¹‰ç­‰ã€‚

è¯¥å·¥å…·æ¥å—ä»¥ä¸‹å‚æ•°ï¼š

- `url` ï¼ˆå¿…éœ€ï¼Œstrï¼‰ï¼šå¼€å§‹çˆ¬å–çš„åŸºç¡€ URLã€‚

- `limit` ï¼ˆå¯é€‰ï¼Œintï¼‰ï¼šè¦çˆ¬å–çš„æœ€å¤§é¡µé¢æ•°ã€‚é»˜è®¤ä¸º `10000`ã€‚
- `include_paths` ï¼ˆå¯é€‰ï¼ŒList[str]ï¼‰ï¼šURL è·¯å¾„åæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼Œç”¨äºåœ¨çˆ¬å–ä¸­åŒ…å«åŒ¹é…çš„ URLã€‚åªæœ‰åŒ¹é…è¿™äº›æ¨¡å¼çš„ URL æ‰ä¼šè¢«åŒ…å«ã€‚ä¾‹å¦‚ï¼Œè®¾ç½® `["blog/.*"]` å°†ä»…åŒ…å« `/blog/` è·¯å¾„ä¸‹çš„ URLã€‚é»˜è®¤ä¸º Noneã€‚
- `exclude_paths` ï¼ˆå¯é€‰ï¼ŒList[str]ï¼‰ï¼šURL è·¯å¾„åæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼Œç”¨äºä»çˆ¬å–ä¸­æ’é™¤åŒ¹é…çš„ URLã€‚ä¾‹å¦‚ï¼Œè®¾ç½® `["blog/.*"]` å°†æ’é™¤ `/blog/` è·¯å¾„ä¸‹çš„ URLã€‚é»˜è®¤ä¸º Noneã€‚
- `max_depth` ï¼ˆå¯é€‰ï¼Œintï¼‰ï¼šç›¸å¯¹äºåŸºç¡€ URL çš„æœ€å¤§çˆ¬å–æ·±åº¦ï¼Œé€šè¿‡ URL è·¯å¾„ä¸­çš„æ–œæ æ•°é‡è¡¡é‡ã€‚é»˜è®¤ä¸º `10`ã€‚
- `max_discovery_depth` ï¼ˆå¯é€‰ï¼Œintï¼‰ï¼šåŸºäºå‘ç°é¡ºåºçš„æœ€å¤§çˆ¬å–æ·±åº¦ã€‚æ ¹é¡µé¢å’Œç«™ç‚¹åœ°å›¾é¡µé¢æ·±åº¦ä¸º `0`ã€‚ä¾‹å¦‚ï¼Œè®¾ç½®ä¸º `1` å¹¶å¿½ç•¥ç«™ç‚¹åœ°å›¾å°†ä»…çˆ¬å–è¾“å…¥çš„ URL åŠå…¶ç›´æ¥é“¾æ¥ã€‚é»˜è®¤ä¸º Noneã€‚
- `ignore_sitemap` ï¼ˆå¯é€‰ï¼Œboolï¼‰ï¼šçˆ¬å–æœŸé—´æ˜¯å¦å¿½ç•¥ç½‘ç«™ç«™ç‚¹åœ°å›¾ã€‚é»˜è®¤ä¸º Falseã€‚
- `ignore_query_params` ï¼ˆå¯é€‰ï¼Œboolï¼‰ï¼šæ˜¯å¦å¿½ç•¥æŸ¥è¯¢å‚æ•°å·®å¼‚ä»¥é¿å…é‡å¤æŠ“å–ç›¸ä¼¼ URLã€‚é»˜è®¤ä¸º Falseã€‚
- `deduplicate_similar_urls` ï¼ˆå¯é€‰ï¼Œboolï¼‰ï¼šæ˜¯å¦å¯¹ç›¸ä¼¼ URL è¿›è¡Œå»é‡ã€‚é»˜è®¤ä¸º Trueã€‚
- `regex_on_full_url` ï¼ˆå¯é€‰ï¼Œboolï¼‰ï¼šæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ˜¯å¦åº”ç”¨äºå®Œæ•´ URL è€Œä¸ä»…ä»…æ˜¯è·¯å¾„ã€‚é»˜è®¤ä¸º Trueã€‚
- `allow_backward_links` ï¼ˆå¯é€‰ï¼Œboolï¼‰ï¼šæ˜¯å¦å…è®¸çˆ¬å– URL å±‚æ¬¡ç»“æ„ä¹‹å¤–çš„å›é“¾ã€‚é»˜è®¤ä¸º Falseã€‚
- `allow_external_links` ï¼ˆå¯é€‰ï¼Œboolï¼‰ï¼šæ˜¯å¦å…è®¸çˆ¬å–æŒ‡å‘å¤–éƒ¨ç½‘ç«™çš„é“¾æ¥ã€‚é»˜è®¤ä¸º Falseã€‚
- `delay` ï¼ˆå¯é€‰ï¼Œintï¼‰ï¼šé¡µé¢æŠ“å–ä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰ï¼Œä»¥éµå®ˆé€Ÿç‡é™åˆ¶ã€‚é»˜è®¤ä¸º `1`ã€‚
- `formats` ï¼ˆå¯é€‰ï¼ŒList[str]ï¼‰ï¼šæŠ“å–å†…å®¹çš„æ ¼å¼ã€‚é»˜è®¤ä¸º ["markdown"]ã€‚é€‰é¡¹åŒ…æ‹¬ï¼š
  - `'markdown'`
  - `'rawHtml'`
  - `'screenshot@fullPage'`
  - `'json'`
  - `'links'`
  - `'screenshot'`
  - `'html'`
- `only_main_content` ï¼ˆå¯é€‰ï¼Œboolï¼‰ï¼šæ˜¯å¦ä»…è¿”å›ä¸»è¦å†…å®¹ï¼Œæ’é™¤é¡µçœ‰ã€å¯¼èˆªæ ã€é¡µè„šç­‰ã€‚é»˜è®¤ä¸º Trueã€‚
- `include_tags` ï¼ˆå¯é€‰ï¼ŒList[str]ï¼‰ï¼šè¦åŒ…å«åœ¨è¾“å‡ºä¸­çš„ HTML æ ‡ç­¾åˆ—è¡¨ï¼ˆä¾‹å¦‚ `['h1', 'p']`ï¼‰ã€‚é»˜è®¤ä¸º Noneï¼ˆæ— æ˜¾å¼åŒ…å«è¿‡æ»¤å™¨ï¼‰ã€‚
- `exclude_tags` ï¼ˆå¯é€‰ï¼ŒList[str]ï¼‰ï¼šè¦ä»è¾“å‡ºä¸­æ’é™¤çš„ HTML æ ‡ç­¾åˆ—è¡¨ã€‚é»˜è®¤ä¸º Noneï¼ˆæ— æ˜¾å¼æ’é™¤è¿‡æ»¤å™¨ï¼‰ã€‚
- `headers` ï¼ˆå¯é€‰ï¼ŒDict[str, str]ï¼‰ï¼šéšè¯·æ±‚å‘é€çš„è‡ªå®šä¹‰ HTTP è¯·æ±‚å¤´ï¼Œä¾‹å¦‚ Cookie æˆ– User-Agent å­—ç¬¦ä¸²ã€‚é»˜è®¤ä¸º Noneã€‚
- `wait_for` ï¼ˆå¯é€‰ï¼Œintï¼‰ï¼šæŠ“å–å†…å®¹å‰çš„ç­‰å¾…æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ï¼Œå…è®¸é¡µé¢å®Œå…¨åŠ è½½ã€‚é»˜è®¤ä¸º `0`ã€‚
- `timeout` ï¼ˆå¯é€‰ï¼Œintï¼‰ï¼šè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ã€‚é»˜è®¤ä¸º `30000`ã€‚

## è°ƒç”¨

### ScrapelessCrawlerCrawlTool

#### å¸¦å‚æ•°ä½¿ç”¨

```python
from langchain_scrapeless import ScrapelessCrawlerCrawlTool

tool = ScrapelessCrawlerCrawlTool()

# é«˜çº§ç”¨æ³•
result = tool.invoke({"url": "https://exmaple.com", "limit": 4})
print(result)
```

```python
{'success': True, 'status': 'completed', 'completed': 1, 'total': 1, 'data': [{'markdown': '# Well hello there.\n\nWelcome to exmaple.com.\n\nChances are you got here by mistake (example.com, anyone?)', 'metadata': {'scrapeId': '547b2478-a41a-4a17-8015-8db378ee455f', 'sourceURL': 'https://exmaple.com', 'url': 'https://exmaple.com', 'statusCode': 200}}]}
```

#### åœ¨æ™ºèƒ½ä½“ä¸­ä½¿ç”¨

```python
from langchain_openai import ChatOpenAI
from langchain_scrapeless import ScrapelessCrawlerCrawlTool
from langchain.agents import create_agent

model = ChatOpenAI()

tool = ScrapelessCrawlerCrawlTool()

# åœ¨æ™ºèƒ½ä½“ä¸­ä½¿ç”¨è¯¥å·¥å…·
tools = [tool]
agent = create_agent(model, tools)

for chunk in agent.stream(
    {
        "messages": [
            (
                "human",
                "Use the scrapeless crawler crawl tool to crawl the website https://example.com and output the markdown content as a string.",
            )
        ]
    },
    stream_mode="values",
):
    chunk["messages"][-1].pretty_print()
```

```text
================================ Human Message =================================

Use the scrapeless crawler crawl tool to crawl the website https://example.com and output the markdown content as a string.
================================== Ai Message ==================================
Tool Calls:
  scrapeless_crawler_crawl (call_Ne5HbxqsYDOKFaGDSuc4xppB)
 Call ID: call_Ne5HbxqsYDOKFaGDSuc4xppB
  Args:
    url: https://example.com
    formats: ['markdown']
    limit: 1
================================= Tool Message =================================
Name: scrapeless_crawler_crawl

{"success": true, "status": "completed", "completed": 1, "total": 1, "data": [{"markdown": "# Example Domain\n\nThis domain is for use in illustrative examples in documents. You may use this\ndomain in literature without prior coordination or asking for permission.\n\n[More information...](https://www.iana.org/domains/example)", "metadata": {"viewport": "width=device-width, initial-scale=1", "title": "Example Domain", "scrapeId": "00561460-9166-492b-8fed-889667383e55", "sourceURL": "https://example.com", "url": "https://example.com", "statusCode": 200}}]}
================================== Ai Message ==================================

The crawl of the website https://example.com has been completed. Here is the markdown content extracted from the website:

\`\`\`
# Example Domain

This domain is for use in illustrative examples in documents. You may use this
domain in literature without prior coordination or asking for permission.

[More information...](https://www.iana.org/domains/example)
\`\`\`

You can find more information on the website [here](https://www.iana.org/domains/example).
```

### ScrapelessCrawlerScrapeTool

#### å¸¦å‚æ•°ä½¿ç”¨

```python
from langchain_scrapeless import ScrapelessDeepSerpGoogleTrendsTool

tool = ScrapelessDeepSerpGoogleTrendsTool()

# åŸºæœ¬ç”¨æ³•
result = tool.invoke("Funny 2048,negamon monster trainer")
print(result)
```

```python
{'parameters': {'engine': 'google.trends.search', 'hl': 'en', 'data_type': 'INTEREST_OVER_TIME', 'tz': '0', 'cat': '0', 'date': 'today 1-m', 'q': 'Funny 2048,negamon monster trainer'}, 'interest_over_time': {'timeline_data': [{'date': 'Jul 11, 2025', 'timestamp': '1752192000', 'value': [0, 0]}, {'date': 'Jul 12, 2025', 'timestamp': '1752278400', 'value': [0, 0]}, {'date': 'Jul 13, 2025', 'timestamp': '1752364800', 'value': [0, 0]}, {'date': 'Jul 14, 2025', 'timestamp': '1752451200', 'value': [0, 0]}, {'date': 'Jul 15, 2025', 'timestamp': '1752537600', 'value': [0, 0]}, {'date': 'Jul 16, 2025', 'timestamp': '1752624000', 'value': [0, 0]}, {'date': 'Jul 17, 2025', 'timestamp': '1752710400', 'value': [0, 0]}, {'date': 'Jul 18, 2025', 'timestamp': '1752796800', 'value': [0, 0]}, {'date': 'Jul 19, 2025', 'timestamp': '1752883200', 'value': [0, 0]}, {'date': 'Jul 20, 2025', 'timestamp': '1752969600', 'value': [0, 0]}, {'date': 'Jul 21, 2025', 'timestamp': '1753056000', 'value': [0, 0]}, {'date': 'Jul 22, 2025', 'timestamp': '1753142400', 'value': [0, 0]}, {'date': 'Jul 23, 2025', 'timestamp': '1753228800', 'value': [0, 0]}, {'date': 'Jul 24, 2025', 'timestamp': '1753315200', 'value': [0, 0]}, {'date': 'Jul 25, 2025', 'timestamp': '1753401600', 'value': [0, 0]}, {'date': 'Jul 26, 2025', 'timestamp': '1753488000', 'value': [0, 0]}, {'date': 'Jul 27, 2025', 'timestamp': '1753574400', 'value': [0, 0]}, {'date': 'Jul 28, 2025', 'timestamp': '1753660800', 'value': [0, 0]}, {'date': 'Jul 29, 2025', 'timestamp': '1753747200', 'value': [0, 0]}, {'date': 'Jul 30, 2025', 'timestamp': '1753833600', 'value': [0, 0]}, {'date': 'Jul 31, 2025', 'timestamp': '1753920000', 'value': [0, 0]}, {'date': 'Aug 1, 2025', 'timestamp': '1754006400', 'value': [0, 0]}, {'date': 'Aug 2, 2025', 'timestamp': '1754092800', 'value': [0, 0]}, {'date': 'Aug 3, 2025', 'timestamp': '1754179200', 'value': [0, 0]}, {'date': 'Aug 4, 2025', 'timestamp': '1754265600', 'value': [0, 0]}, {'date': 'Aug 5, 2025', 'timestamp': '1754352000', 'value': [0, 0]}, {'date': 'Aug 6, 2025', 'timestamp': '1754438400', 'value': [0, 0]}, {'date': 'Aug 7, 2025', 'timestamp': '1754524800', 'value': [0, 0]}, {'date': 'Aug 8, 2025', 'timestamp': '1754611200', 'value': [0, 0]}, {'date': 'Aug 9, 2025', 'timestamp': '1754697600', 'value': [0, 0]}, {'date': 'Aug 10, 2025', 'timestamp': '1754784000', 'value': [0, 100]}, {'date': 'Aug 11, 2025', 'timestamp': '1754870400', 'value': [0, 0]}], 'averages': [{'value': 0}, {'value': 3}], 'isPartial': True}}
```

#### å¸¦å‚æ•°çš„é«˜çº§ç”¨æ³•

```python
from langchain_scrapeless import ScrapelessCrawlerScrapeTool

tool = ScrapelessCrawlerScrapeTool()

result = tool.invoke(
    {
        "urls": ["https://exmaple.com", "https://www.scrapeless.com/en"],
        "formats": ["markdown"],
    }
)
print(result)
```

```python
{'success': True, 'status': 'completed', 'completed': 1, 'total': 1, 'data': [{'markdown': "[ğŸ©µ Don't just take our word for it. See what our users say on Product Hunt.](https://www.producthunt.com/posts/scrapeless-deep-serpapi)\n\n# Effortless Web Scraping Toolkit  for Business and Developers\n\nThe ultimate scraper's companion: an expandable suite of tools, including\n\nScraping Browser, Scraping API, Universal Scraping API\n\nand Anti-Bot Solutionsâ€”designed to work together or independently.\n\n[**4.8**](https://www.g2.com/products/scrapeless/reviews) [**4.5**](https://www.trustpilot.com/re
