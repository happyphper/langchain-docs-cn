---
title: å‰ç«¯
description: åˆ©ç”¨ LangChain æ™ºèƒ½ä½“ï¼ˆagentï¼‰ã€LangGraph å›¾ï¼ˆgraphï¼‰å’Œè‡ªå®šä¹‰ API çš„å®æ—¶æµå¼ä¼ è¾“ï¼Œæ„å»ºç”Ÿæˆå¼ç”¨æˆ·ç•Œé¢ï¼ˆUIï¼‰ã€‚
---
`useStream` React é’©å­æä¾›äº†ä¸ LangGraph æµå¼å¤„ç†èƒ½åŠ›çš„æ— ç¼é›†æˆã€‚å®ƒå¤„ç†äº†æµå¼å¤„ç†ã€çŠ¶æ€ç®¡ç†å’Œåˆ†æ”¯é€»è¾‘çš„æ‰€æœ‰å¤æ‚æ€§ï¼Œè®©ä½ å¯ä»¥ä¸“æ³¨äºæ„å»ºå‡ºè‰²çš„ç”Ÿæˆå¼ UI ä½“éªŒã€‚

ä¸»è¦ç‰¹æ€§ï¼š

* <Icon icon="messages" :size="16" /> **æ¶ˆæ¯æµå¼å¤„ç†** â€” å¤„ç†æ¶ˆæ¯å—æµä»¥å½¢æˆå®Œæ•´çš„æ¶ˆæ¯
* <Icon icon="arrows-rotate" :size="16" /> **è‡ªåŠ¨çŠ¶æ€ç®¡ç†** â€” ç”¨äºæ¶ˆæ¯ã€ä¸­æ–­ã€åŠ è½½çŠ¶æ€å’Œé”™è¯¯
* <Icon icon="code-branch" :size="16" /> **å¯¹è¯åˆ†æ”¯** â€” ä»èŠå¤©å†å²ä¸­çš„ä»»æ„ç‚¹åˆ›å»ºæ›¿ä»£å¯¹è¯è·¯å¾„
* <Icon icon="palette" :size="16" /> **UI æ— å…³è®¾è®¡** â€” ä½¿ç”¨ä½ è‡ªå·±çš„ç»„ä»¶å’Œæ ·å¼

## å®‰è£…

å®‰è£… LangGraph SDK ä»¥åœ¨ä½ çš„ React åº”ç”¨ä¸­ä½¿ç”¨ `useStream` é’©å­ï¼š

## åŸºæœ¬ç”¨æ³•

`useStream` é’©å­å¯ä»¥è¿æ¥åˆ°ä»»ä½• LangGraph å›¾ï¼Œæ— è®ºæ˜¯ä»ä½ è‡ªå·±çš„ç«¯ç‚¹è¿è¡Œï¼Œè¿˜æ˜¯ä½¿ç”¨ [LangSmith éƒ¨ç½²](/langsmith/deployments) éƒ¨ç½²çš„ã€‚

```tsx
import { useStream } from "@langchain/langgraph-sdk/react";

function Chat() {
  const stream = useStream({
    assistantId: "agent",
    // æœ¬åœ°å¼€å‘
    apiUrl: "http://localhost:2024",
    // ç”Ÿäº§éƒ¨ç½²ï¼ˆLangSmith æ‰˜ç®¡ï¼‰
    // apiUrl: "https://your-deployment.us.langgraph.app"
  });

  const handleSubmit = (message: string) => {
    stream.submit({
      messages: [
        { content: message, type: "human" }
      ],
    });
  };

  return (
    <div>
      {stream.messages.map((message, idx) => (
        <div key={message.id ?? idx}>
          {message.type}: {message.content}
        </div>
      ))}

      {stream.isLoading && <div>Loading...</div>}
      {stream.error && <div>Error: {stream.error.message}</div>}
    </div>
  );
}
```

<Tip>

äº†è§£å¦‚ä½• [å°†ä½ çš„æ™ºèƒ½ä½“éƒ¨ç½²åˆ° LangSmith](/oss/python/langchain/deploy)ï¼Œä»¥è·å¾—å…·å¤‡å†…ç½®å¯è§‚æµ‹æ€§ã€èº«ä»½éªŒè¯å’Œæ‰©å±•èƒ½åŠ›çš„ç”Ÿäº§å°±ç»ªæ‰˜ç®¡æœåŠ¡ã€‚

</Tip>

:::: details `useStream` å‚æ•°

<ParamField body="assistantId" type="string" required>

è¦è¿æ¥çš„æ™ºèƒ½ä½“ IDã€‚ä½¿ç”¨ LangSmith éƒ¨ç½²æ—¶ï¼Œæ­¤ ID å¿…é¡»ä¸éƒ¨ç½²ä»ªè¡¨æ¿ä¸­æ˜¾ç¤ºçš„æ™ºèƒ½ä½“ ID åŒ¹é…ã€‚å¯¹äºè‡ªå®šä¹‰ API éƒ¨ç½²æˆ–æœ¬åœ°å¼€å‘ï¼Œè¿™å¯ä»¥æ˜¯ä½ çš„æœåŠ¡å™¨ç”¨äºæ ‡è¯†æ™ºèƒ½ä½“çš„ä»»ä½•å­—ç¬¦ä¸²ã€‚

</ParamField>

<ParamField body="apiUrl" type="string">

LangGraph æœåŠ¡å™¨çš„ URLã€‚æœ¬åœ°å¼€å‘æ—¶é»˜è®¤ä¸º `http://localhost:2024`ã€‚

</ParamField>

<ParamField body="apiKey" type="string">

ç”¨äºèº«ä»½éªŒè¯çš„ API å¯†é’¥ã€‚è¿æ¥åˆ° LangSmith ä¸Šå·²éƒ¨ç½²çš„æ™ºèƒ½ä½“æ—¶éœ€è¦ã€‚

</ParamField>

<ParamField body="threadId" type="string">

è¿æ¥åˆ°ç°æœ‰çº¿ç¨‹è€Œä¸æ˜¯åˆ›å»ºæ–°çº¿ç¨‹ã€‚å¯¹äºæ¢å¤å¯¹è¯å¾ˆæœ‰ç”¨ã€‚

</ParamField>

<ParamField body="onThreadId" type="(id: string) =>
void">
åˆ›å»ºæ–°çº¿ç¨‹æ—¶è°ƒç”¨çš„å›è°ƒå‡½æ•°ã€‚ä½¿ç”¨æ­¤å‡½æ•°æ¥æŒä¹…åŒ–çº¿ç¨‹ ID ä»¥ä¾›åç»­ä½¿ç”¨ã€‚

</ParamField>

<ParamField body="reconnectOnMount" type="boolean | (() =>
Storage)">
åœ¨ç»„ä»¶æŒ‚è½½æ—¶è‡ªåŠ¨æ¢å¤æ­£åœ¨è¿›è¡Œçš„è¿è¡Œã€‚è®¾ç½®ä¸º `true` ä»¥ä½¿ç”¨ä¼šè¯å­˜å‚¨ï¼Œæˆ–æä¾›è‡ªå®šä¹‰å­˜å‚¨å‡½æ•°ã€‚

</ParamField>

<ParamField body="onCreated" type="(run: Run) =>
void">
åˆ›å»ºæ–°è¿è¡Œæ—¶è°ƒç”¨çš„å›è°ƒå‡½æ•°ã€‚å¯¹äºæŒä¹…åŒ–è¿è¡Œå…ƒæ•°æ®ä»¥ä¾¿æ¢å¤å¾ˆæœ‰ç”¨ã€‚

</ParamField>

<ParamField body="onError" type="(error: Error) =>
void">
æµå¼å¤„ç†æœŸé—´å‘ç”Ÿé”™è¯¯æ—¶è°ƒç”¨çš„å›è°ƒå‡½æ•°ã€‚

</ParamField>

<ParamField body="onFinish" type="(state: StateType, run?: Run) =>
void">
æµæˆåŠŸå®Œæˆå¹¶è¿”å›æœ€ç»ˆçŠ¶æ€æ—¶è°ƒç”¨çš„å›è°ƒå‡½æ•°ã€‚

</ParamField>

<ParamField body="onCustomEvent" type="(data: unknown, context: { mutate }) =>
void">
ä½¿ç”¨ `writer` å¤„ç†ä»ä½ çš„æ™ºèƒ½ä½“å‘å‡ºçš„è‡ªå®šä¹‰äº‹ä»¶ã€‚è¯·å‚é˜… [è‡ªå®šä¹‰æµå¼å¤„ç†äº‹ä»¶](#custom-streaming-events)ã€‚

</ParamField>

<ParamField body="onUpdateEvent" type="(data: unknown, context: { mutate }) =>
void">
å¤„ç†æ¯ä¸ªå›¾æ­¥éª¤åçš„çŠ¶æ€æ›´æ–°äº‹ä»¶ã€‚

</ParamField>

<ParamField body="onMetadataEvent" type="(metadata: { run_id, thread_id }) =>
void">
å¤„ç†åŒ…å«è¿è¡Œå’Œçº¿ç¨‹ä¿¡æ¯çš„å…ƒæ•°æ®äº‹ä»¶ã€‚

</ParamField>

<ParamField body="messagesKey" type="string" default="messages">

å›¾çŠ¶æ€ä¸­åŒ…å«æ¶ˆæ¯æ•°ç»„çš„é”®ã€‚

</ParamField>

<ParamField body="throttle" type="boolean" default="true">

æ‰¹é‡å¤„ç†çŠ¶æ€æ›´æ–°ä»¥è·å¾—æ›´å¥½çš„æ¸²æŸ“æ€§èƒ½ã€‚ç¦ç”¨æ­¤é€‰é¡¹å¯ç«‹å³æ›´æ–°ã€‚

</ParamField>

<ParamField body="initialValues" type="StateType | null">

åœ¨ç¬¬ä¸€ä¸ªæµåŠ è½½æ—¶æ˜¾ç¤ºçš„åˆå§‹çŠ¶æ€å€¼ã€‚å¯¹äºç«‹å³æ˜¾ç¤ºç¼“å­˜çš„çº¿ç¨‹æ•°æ®å¾ˆæœ‰ç”¨ã€‚

</ParamField>

::::

:::: details `useStream` è¿”å›å€¼

<ParamField body="messages" type="Message[]">

å½“å‰çº¿ç¨‹ä¸­çš„æ‰€æœ‰æ¶ˆæ¯ï¼ŒåŒ…æ‹¬äººç±»å’Œ AI æ¶ˆæ¯ã€‚

</ParamField>

<ParamField body="values" type="StateType">

å½“å‰çš„å›¾çŠ¶æ€å€¼ã€‚ç±»å‹ä»æ™ºèƒ½ä½“æˆ–å›¾ç±»å‹å‚æ•°æ¨æ–­ã€‚

</ParamField>

<ParamField body="isLoading" type="boolean">

å½“å‰æ˜¯å¦æœ‰æµæ­£åœ¨è¿›è¡Œã€‚ä½¿ç”¨æ­¤å€¼æ¥æ˜¾ç¤ºåŠ è½½æŒ‡ç¤ºå™¨ã€‚

</ParamField>

<ParamField body="error" type="Error | null">

æµå¼ä¼ è¾“æœŸé—´å‘ç”Ÿçš„ä»»ä½•é”™è¯¯ã€‚æ— é”™è¯¯æ—¶ä¸º `null`ã€‚

</ParamField>

<ParamField body="interrupt" type="Interrupt | undefined">

å½“å‰éœ€è¦ç”¨æˆ·è¾“å…¥çš„ä¸­æ–­ï¼Œä¾‹å¦‚äººæœºååŒï¼ˆhuman-in-the-loopï¼‰çš„æ‰¹å‡†è¯·æ±‚ã€‚

</ParamField>

<ParamField body="toolCalls" type="ToolCallWithResult[]">

æ‰€æœ‰æ¶ˆæ¯ä¸­çš„æ‰€æœ‰å·¥å…·è°ƒç”¨ï¼ŒåŒ…å«å…¶ç»“æœå’ŒçŠ¶æ€ï¼ˆ`pending`ã€`completed` æˆ– `error`ï¼‰ã€‚

</ParamField>

<ParamField body="submit" type="(input, options?) =>
Promise<void>">
å‘æ™ºèƒ½ä½“æäº¤æ–°çš„è¾“å…¥ã€‚å½“ä»ä¸­æ–­æ¢å¤å¹¶å¸¦æœ‰å‘½ä»¤æ—¶ï¼Œå°† `null` ä½œä¸ºè¾“å…¥ä¼ é€’ã€‚é€‰é¡¹åŒ…æ‹¬ç”¨äºåˆ†æ”¯çš„ `checkpoint`ã€ç”¨äºä¹è§‚æ›´æ–°çš„ `optimisticValues` ä»¥åŠç”¨äºä¹è§‚çº¿ç¨‹åˆ›å»ºçš„ `threadId`ã€‚

</ParamField>

<ParamField body="stop" type="() =>
void">
ç«‹å³åœæ­¢å½“å‰æµã€‚

</ParamField>

<ParamField body="joinStream" type="(runId: string) =>
void">
é€šè¿‡è¿è¡Œ ID æ¢å¤ç°æœ‰çš„æµã€‚ä¸ `onCreated` ä¸€èµ·ä½¿ç”¨ä»¥æ‰‹åŠ¨æ¢å¤æµã€‚

</ParamField>

<ParamField body="setBranch" type="(branch: string) =>
void">
åˆ‡æ¢åˆ°å¯¹è¯å†å²ä¸­çš„ä¸åŒåˆ†æ”¯ã€‚

</ParamField>

<ParamField body="getToolCalls" type="(message) =>
ToolCall[]">
è·å–ç‰¹å®š AI æ¶ˆæ¯çš„æ‰€æœ‰å·¥å…·è°ƒç”¨ã€‚

</ParamField>

<ParamField body="getMessagesMetadata" type="(message) =>
MessageMetadata">
è·å–æ¶ˆæ¯çš„å…ƒæ•°æ®ï¼ŒåŒ…æ‹¬æµä¿¡æ¯ï¼Œä¾‹å¦‚ç”¨äºè¯†åˆ«æºèŠ‚ç‚¹çš„ `langgraph_node`ï¼Œä»¥åŠç”¨äºåˆ†æ”¯çš„ `firstSeenState`ã€‚

</ParamField>

<ParamField body="experimental_branchTree" type="BranchTree">

çº¿ç¨‹çš„æ ‘çŠ¶è¡¨ç¤ºï¼Œç”¨äºåœ¨éåŸºäºæ¶ˆæ¯çš„å›¾ä¸­è¿›è¡Œé«˜çº§åˆ†æ”¯æ§åˆ¶ã€‚

</ParamField>

::::

## çº¿ç¨‹ç®¡ç†

é€šè¿‡å†…ç½®çš„çº¿ç¨‹ç®¡ç†æ¥è·Ÿè¸ªå¯¹è¯ã€‚æ‚¨å¯ä»¥è®¿é—®å½“å‰çº¿ç¨‹ IDï¼Œå¹¶åœ¨æ–°çº¿ç¨‹åˆ›å»ºæ—¶æ”¶åˆ°é€šçŸ¥ï¼š

```tsx
import { useState } from "react";
import { useStream } from "@langchain/langgraph-sdk/react";

function Chat() {
  const [threadId, setThreadId] = useState<string | null>(null);

  const stream = useStream({
    apiUrl: "http://localhost:2024",
    assistantId: "agent",
    threadId: threadId,
    onThreadId: setThreadId,
  });

  // threadId åœ¨æ–°çº¿ç¨‹åˆ›å»ºæ—¶æ›´æ–°
  // å°†å…¶å­˜å‚¨åœ¨ URL å‚æ•°æˆ– localStorage ä¸­ä»¥å®ç°æŒä¹…åŒ–
}
```

æˆ‘ä»¬å»ºè®®å­˜å‚¨ `threadId`ï¼Œä»¥ä¾¿ç”¨æˆ·åœ¨é¡µé¢åˆ·æ–°åèƒ½å¤Ÿæ¢å¤å¯¹è¯ã€‚

### é¡µé¢åˆ·æ–°åæ¢å¤

`useStream` é’©å­å¯ä»¥é€šè¿‡è®¾ç½® `reconnectOnMount: true` åœ¨æŒ‚è½½æ—¶è‡ªåŠ¨æ¢å¤æ­£åœ¨è¿›è¡Œçš„è¿è¡Œã€‚è¿™å¯¹äºåœ¨é¡µé¢åˆ·æ–°åç»§ç»­æµå¼ä¼ è¾“éå¸¸æœ‰ç”¨ï¼Œç¡®ä¿åœ¨åœæœºæœŸé—´ç”Ÿæˆçš„æ¶ˆæ¯å’Œäº‹ä»¶ä¸ä¼šä¸¢å¤±ã€‚

```tsx
const stream = useStream({
  apiUrl: "http://localhost:2024",
  assistantId: "agent",
  reconnectOnMount: true,
});
```

é»˜è®¤æƒ…å†µä¸‹ï¼Œåˆ›å»ºçš„è¿è¡Œ ID å­˜å‚¨åœ¨ `window.sessionStorage` ä¸­ï¼Œå¯ä»¥é€šè¿‡ä¼ é€’è‡ªå®šä¹‰å­˜å‚¨å‡½æ•°æ¥æ›¿æ¢ï¼š

```tsx
const stream = useStream({
  apiUrl: "http://localhost:2024",
  assistantId: "agent",
  reconnectOnMount: () => window.localStorage,
});
```

è¦æ‰‹åŠ¨æ§åˆ¶æ¢å¤è¿‡ç¨‹ï¼Œè¯·ä½¿ç”¨è¿è¡Œå›è°ƒæ¥æŒä¹…åŒ–å…ƒæ•°æ®ï¼Œå¹¶ä½¿ç”¨ `joinStream` æ¥æ¢å¤ï¼š

```tsx
import { useStream } from "@langchain/langgraph-sdk/react";
import { useEffect, useRef } from "react";

function Chat({ threadId }: { threadId: string | null }) {
  const stream = useStream({
    apiUrl: "http://localhost:2024",
    assistantId: "agent",
    threadId,
    onCreated: (run) => {
      // æµå¼€å§‹æ—¶æŒä¹…åŒ–è¿è¡Œ ID
      window.sessionStorage.setItem(`resume:${run.thread_id}`, run.run_id);
    },
    onFinish: (_, run) => {
      // æµå®Œæˆæ—¶æ¸…ç†
      window.sessionStorage.removeItem(`resume:${run?.thread_id}`);
    },
  });

  // å¦‚æœå­˜åœ¨å­˜å‚¨çš„è¿è¡Œ IDï¼Œåˆ™åœ¨æŒ‚è½½æ—¶æ¢å¤æµ
  const joinedThreadId = useRef<string | null>(null);
  useEffect(() => {
    if (!threadId) return;
    const runId = window.sessionStorage.getItem(`resume:${threadId}`);
    if (runId && joinedThreadId.current !== threadId) {
      stream.joinStream(runId);
      joinedThreadId.current = threadId;
    }
  }, [threadId]);

  const handleSubmit = (text: string) => {
    // ä½¿ç”¨ streamResumable ç¡®ä¿äº‹ä»¶ä¸ä¼šä¸¢å¤±
    stream.submit(
      { messages: [{ type: "human", content: text }] },
      { streamResumable: true }
    );
  };
}
```

<Card title="å°è¯•ä¼šè¯æŒä¹…åŒ–ç¤ºä¾‹" icon="rotate" href="https://github.com/langchain-ai/langgraphjs/tree/main/examples/ui-react/src/examples/session-persistence">

åœ¨ `session-persistence` ç¤ºä¾‹ä¸­æŸ¥çœ‹ä½¿ç”¨ `reconnectOnMount` å’Œçº¿ç¨‹æŒä¹…åŒ–å®ç°æµæ¢å¤çš„å®Œæ•´å®ç°ã€‚

</Card>

## ä¹è§‚æ›´æ–°

æ‚¨å¯ä»¥åœ¨æ‰§è¡Œç½‘ç»œè¯·æ±‚ä¹‹å‰ä¹è§‚åœ°æ›´æ–°å®¢æˆ·ç«¯çŠ¶æ€ï¼Œä¸ºç”¨æˆ·æä¾›å³æ—¶åé¦ˆï¼š

```tsx
const stream = useStream({
  apiUrl: "http://localhost:2024",
  assistantId: "agent",
});

const handleSubmit = (text: string) => {
  const newMessage = { type: "human" as const, content: text };

  stream.submit(
    { messages: [newMessage] },
    {
      optimisticValues(prev) {
        const prevMessages = prev.messages ?? [];
        return { ...prev, messages: [...prevMessages, newMessage] };
      },
    }
  );
};
```

### ä¹è§‚çº¿ç¨‹åˆ›å»º

åœ¨ `submit` ä¸­ä½¿ç”¨ `threadId` é€‰é¡¹ï¼Œä»¥å®ç°åœ¨çº¿ç¨‹åˆ›å»ºä¹‹å‰éœ€è¦çŸ¥é“çº¿ç¨‹ ID çš„ä¹è§‚ UI æ¨¡å¼ï¼š

```tsx
import { useState } from "react";
import { useStream } from "@langchain/langgraph-sdk/react";

function Chat() {
  const [threadId, setThreadId] = useState<string | null>(null);
  const [optimisticThreadId] = useState(() => crypto.randomUUID());

  const stream = useStream({
    apiUrl: "http://localhost:2024",
    assistantId: "agent",
    threadId,
    onThreadId: setThreadId,
  });

  const handleSubmit = (text: string) => {
    // ç«‹å³å¯¼èˆªï¼Œæ— éœ€ç­‰å¾…çº¿ç¨‹åˆ›å»º
    window.history.pushState({}, "", `/threads/${optimisticThreadId}`);

    // ä½¿ç”¨é¢„å®šçš„ ID åˆ›å»ºçº¿ç¨‹
    stream.submit(
      { messages: [{ type: "human", content: text }] },
      { threadId: optimisticThreadId }
    );
  };
}
```

### ç¼“å­˜çº¿ç¨‹æ˜¾ç¤º

ä½¿ç”¨ `initialValues` é€‰é¡¹åœ¨ä»æœåŠ¡å™¨åŠ è½½å†å²è®°å½•æ—¶ç«‹å³æ˜¾ç¤ºç¼“å­˜çš„çº¿ç¨‹æ•°æ®ï¼š

```tsx
function Chat({ threadId, cachedData }) {
  const stream = useStream({
    apiUrl: "http://localhost:2024",
    assistantId: "agent",
    threadId,
    initialValues: cachedData?.values,
  });

  // ç«‹å³æ˜¾ç¤ºç¼“å­˜çš„æ¶ˆæ¯ï¼Œç„¶ååœ¨æœåŠ¡å™¨å“åº”æ—¶æ›´æ–°
}
```

## åˆ†æ”¯

é€šè¿‡ç¼–è¾‘å…ˆå‰çš„æ¶ˆæ¯æˆ–é‡æ–°ç”Ÿæˆ AI å“åº”ï¼Œå¯ä»¥åˆ›å»ºæ›¿ä»£çš„å¯¹è¯è·¯å¾„ã€‚ä½¿ç”¨ `getMessagesMetadata()` æ¥è®¿é—®ç”¨äºåˆ†æ”¯çš„æ£€æŸ¥ç‚¹ä¿¡æ¯ï¼š

::: code-group

```tsx [Chat.tsx]
import { useStream } from "@langchain/langgraph-sdk/react";
import { BranchSwitcher } from "./BranchSwitcher";

function Chat() {
  const stream = useStream({
    apiUrl: "http://localhost:2024",
    assistantId: "agent",
  });

  return (
    <div>
      {stream.messages.map((message) => {
        const meta = stream.getMessagesMetadata(message);
        const parentCheckpoint = meta?.firstSeenState?.parent_checkpoint;

        return (
          <div key={message.id}>
            <div>{message.content as string}</div>

            {/* ç¼–è¾‘äººç±»æ¶ˆæ¯ */}
            {message.type === "human" && (
              <button
                onClick={() => {
                  const newContent = prompt("ç¼–è¾‘æ¶ˆæ¯:", message.content as string);
                  if (newContent) {
                    stream.submit(
                      { messages: [{ type: "human", content: newContent }] },
                      { checkpoint: parentCheckpoint }
                    );
                  }
                }}
              >
                ç¼–è¾‘
              </button>
            )}

            {/* é‡æ–°ç”Ÿæˆ AI æ¶ˆæ¯ */}
            {message.type === "ai" && (
              <button
                onClick={() => stream.submit(undefined, { checkpoint: parentCheckpoint })}
              >
                é‡æ–°ç”Ÿæˆ
              </button>
            )}

            {/* åœ¨åˆ†æ”¯ä¹‹é—´åˆ‡æ¢ */}
            <BranchSwitcher
              branch={meta?.branch}
              branchOptions={meta?.branchOptions}
              onSelect={(branch) => stream.setBranch(branch)}
            />
          </div>
        );
      })}
    </div>
  );
}
```

```tsx [BranchSwitcher.tsx]
/**
 * ç”¨äºåœ¨å¯¹è¯åˆ†æ”¯ä¹‹é—´å¯¼èˆªçš„ç»„ä»¶ã€‚
 * æ˜¾ç¤ºå½“å‰åˆ†æ”¯ä½ç½®å¹¶å…è®¸åœ¨å¤‡é€‰åˆ†æ”¯ä¹‹é—´åˆ‡æ¢ã€‚
 */
export function BranchSwitcher({
  branch,
  branchOptions,
  onSelect,
}: {
  branch: string | undefined;
  branchOptions: string[] | undefined;
  onSelect: (branch: string) => void;
}) {
  if (!branchOptions || !branch) return null;
  const index = branchOptions.indexOf(branch);

  return (
    <div className="flex items-center gap-2">
      <button
        type="button"
        disabled={index <= 0}
        onClick={() => onSelect(branchOptions[index - 1])}
      >
        â†
      </button>
      <span>{index + 1} / {branchOptions.length}</span>
      <button
        type="button"
        disabled={index >= branchOptions.length - 1}
        onClick={() => onSelect(branchOptions[index + 1])}
      >
        â†’
      </button>
    </div>
  );
}
```

:::

å¯¹äºé«˜çº§ç”¨ä¾‹ï¼Œä½¿ç”¨ `experimental_branchTree` å±æ€§æ¥è·å–çº¿ç¨‹çš„æ ‘å½¢è¡¨ç¤ºï¼Œé€‚ç”¨äºéåŸºäºæ¶ˆæ¯çš„å›¾ã€‚

<Card title="å°è¯•åˆ†æ”¯ç¤ºä¾‹" icon="code-branch" href="https://github.com/langchain-ai/langgraphjs/tree/main/examples/ui-react/src/examples/branching-chat">

åœ¨ `branching-chat` ç¤ºä¾‹ä¸­æŸ¥çœ‹å¯¹è¯åˆ†æ”¯çš„å®Œæ•´å®ç°ï¼ŒåŒ…æ‹¬ç¼–è¾‘ã€é‡æ–°ç”Ÿæˆå’Œåˆ†æ”¯åˆ‡æ¢åŠŸèƒ½ã€‚

</Card>

## ç±»å‹å®‰å…¨çš„æµå¼å¤„ç†

å½“ä¸é€šè¿‡ @[`createAgent`] åˆ›å»ºçš„æ™ºèƒ½ä½“ï¼ˆagentï¼‰æˆ–ä½¿ç”¨ <a href="https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph" target="_blank" rel="noreferrer" class="link"><code>StateGraph</code></a> åˆ›å»ºçš„å›¾ä¸€èµ·ä½¿ç”¨æ—¶ï¼Œ`useStream` é’©å­æ”¯æŒå®Œæ•´çš„ç±»å‹æ¨æ–­ã€‚å°† `typeof agent` æˆ– `typeof graph` ä½œä¸ºç±»å‹å‚æ•°ä¼ é€’ï¼Œä»¥è‡ªåŠ¨æ¨æ–­å·¥å…·è°ƒç”¨ç±»å‹ã€‚

### ä½¿ç”¨ `createAgent`

å½“ä½¿ç”¨ @[`createAgent`] æ—¶ï¼Œå·¥å…·è°ƒç”¨ç±»å‹ä¼šæ ¹æ®ä½ æ³¨å†Œåˆ°æ™ºèƒ½ä½“ï¼ˆagentï¼‰çš„å·¥å…·è‡ªåŠ¨æ¨æ–­ï¼š

::: code-group

```python [agent.py]
from langchain import create_agent, tool

@tool
def get_weather(location: str) -> str:
    """Get weather for a location."""
    return f"Weather in {location}: Sunny, 72Â°F"

agent = create_agent(
    model="openai:gpt-4o-mini",
    tools=[get_weather],
)
```

```tsx [Chat.tsx]
import { useStream } from "@langchain/langgraph-sdk/react";
import type { AgentState } from "./types";

function Chat() {
  // ä½¿ç”¨æ‰‹åŠ¨å®šä¹‰çš„çŠ¶æ€ç±»å‹
  const stream = useStream<AgentState>({
    assistantId: "agent",
    apiUrl: "http://localhost:2024",
  });

  // stream.toolCalls[0].call.name çš„ç±»å‹ä¸º "get_weather"
  // stream.toolCalls[0].call.args çš„ç±»å‹ä¸º { location: string }
}
```

```typescript [types.ts]
import type { Message } from "@langchain/langgraph-sdk";

// å®šä¹‰å·¥å…·è°ƒç”¨ç±»å‹ä»¥åŒ¹é…ä½ çš„ Python æ™ºèƒ½ä½“
export type GetWeatherToolCall = {
  name: "get_weather";
  args: { location: string };
  id?: string;
};

export type AgentToolCalls = GetWeatherToolCall;

export interface AgentState {
  messages: Message<AgentToolCalls>[];
}
```

:::

### ä½¿ç”¨ `StateGraph`

å¯¹äºè‡ªå®šä¹‰çš„ <a href="https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph" target="_blank" rel="noreferrer" class="link"><code>StateGraph</code></a> åº”ç”¨ç¨‹åºï¼ŒçŠ¶æ€ç±»å‹æ˜¯ä»å›¾çš„æ³¨è§£ä¸­æ¨æ–­å‡ºæ¥çš„ï¼š

::: code-group

```python [graph.py]
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langchain_openai import ChatOpenAI
from typing import TypedDict, Annotated

class State(TypedDict):
    messages: Annotated[list, add_messages]

model = ChatOpenAI(model="gpt-4o-mini")

async def agent(state: State) -> dict:
    response = await model.ainvoke(state["messages"])
    return {"messages": [response]}

workflow = StateGraph(State)
workflow.add_node("agent", agent)
workflow.add_edge(START, "agent")
workflow.add_edge("agent", END)

graph = workflow.compile()
```

```tsx [Chat.tsx]
import { useStream } from "@langchain/langgraph-sdk/react";
import type { GraphState } from "./types";

function Chat() {
  // ä½¿ç”¨æ‰‹åŠ¨å®šä¹‰çš„çŠ¶æ€ç±»å‹
  const stream = useStream<GraphState>({
    assistantId: "my-graph",
    apiUrl: "http://localhost:2024",
  });

  // stream.values çš„ç±»å‹åŸºäºä½ å®šä¹‰çš„çŠ¶æ€
}
```

```typescript [types.ts]
import type { Message } from "@langchain/langgraph-sdk";

// å®šä¹‰çŠ¶æ€ä»¥åŒ¹é…ä½ çš„ Python å›¾çš„ State TypedDict
export interface GraphState {
  messages: Message[];
}
```

:::

### ä½¿ç”¨æ³¨è§£ç±»å‹

å¦‚æœä½ åœ¨ä½¿ç”¨ LangGraph.jsï¼Œå¯ä»¥å¤ç”¨ä½ å›¾çš„æ³¨è§£ç±»å‹ã€‚è¯·ç¡®ä¿åªå¯¼å…¥ç±»å‹ï¼Œä»¥é¿å…å¯¼å…¥æ•´ä¸ª LangGraph.js è¿è¡Œæ—¶ï¼š

### é«˜çº§ç±»å‹é…ç½®

ä½ å¯ä»¥ä¸ºä¸­æ–­ã€è‡ªå®šä¹‰äº‹ä»¶å’Œå¯é…ç½®é€‰é¡¹æŒ‡å®šé¢å¤–çš„ç±»å‹å‚æ•°ï¼š

## æ¸²æŸ“å·¥å…·è°ƒç”¨

ä½¿ç”¨ `getToolCalls` ä» AI æ¶ˆæ¯ä¸­æå–å¹¶æ¸²æŸ“å·¥å…·è°ƒç”¨ã€‚å·¥å…·è°ƒç”¨åŒ…æ‹¬è°ƒç”¨è¯¦æƒ…ã€ç»“æœï¼ˆå¦‚æœå·²å®Œæˆï¼‰å’ŒçŠ¶æ€ã€‚

::: code-group

```python [agent.py]
from langchain import create_agent, tool

@tool
def get_weather(location: str) -> str:
    """è·å–æŒ‡å®šåœ°ç‚¹çš„å½“å‰å¤©æ°”ã€‚"""
    return f'{{"status": "success", "content": "Weather in {location}: Sunny, 72Â°F"}}'

agent = create_agent(
    model="openai:gpt-4o-mini",
    tools=[get_weather],
)
```

```tsx [Chat.tsx]
import { useStream } from "@langchain/langgraph-sdk/react";
import type { AgentState, AgentToolCalls } from "./types";
import { ToolCallCard } from "./ToolCallCard";
import { MessageBubble } from "./MessageBubble";

function Chat() {
  const stream = useStream<AgentState>({
    assistantId: "agent",
    apiUrl: "http://localhost:2024",
  });

  return (
    <div className="flex flex-col gap-4">
      {stream.messages.map((message, idx) => {
        if (message.type === "ai") {
          const toolCalls = stream.getToolCalls(message);

          if (toolCalls.length > 0) {
            return (
              <div key={message.id ?? idx} className="flex flex-col gap-2">
                {toolCalls.map((toolCall) => (
                  <ToolCallCard key={toolCall.id} toolCall={toolCall} />
                ))}
              </div>
            );
          }
        }

        return <MessageBubble key={message.id ?? idx} message={message} />;
      })}
    </div>
  );
}
```

```tsx [ToolCallCard.tsx]
import type { ToolCallWithResult, ToolCallState } from "@langchain/langgraph-sdk/react";
import type { ToolMessage } from "@langchain/langgraph-sdk";
import type { AgentToolCalls, GetWeatherToolCall } from "./types";
import { parseToolResult } from "./utils";
import { WeatherCard } from "./WeatherCard";
import { GenericToolCallCard } from "./GenericToolCallCard";

export function ToolCallCard({
  toolCall,
}: {
  toolCall: ToolCallWithResult<AgentToolCalls>;
}) {
  const { call, result, state } = toolCall;

  if (call.name === "get_weather") {
    return <WeatherCard call={call} result={result} state={state} />;
  }

  return <GenericToolCallCard call={call} result={result} state={state} />;
}
```

```tsx [WeatherCard.tsx]
import type { ToolCallState } from "@langchain/langgraph-sdk/react";
import type { ToolMessage } from "@langchain/langgraph-sdk";
import type { GetWeatherToolCall } from "./types";
import { parseToolResult } from "./utils";

export function WeatherCard({
  call,
  result,
  state,
}: {
  call: GetWeatherToolCall;
  result?: ToolMessage;
  state: ToolCallState;
}) {
  const isLoading = state === "pending";
  const parsedResult = parseToolResult(result);

  return (
    <div className="relative overflow-hidden rounded-xl">
      <div className="absolute inset-0 bg-gradient-to-br from-sky-600 to-indigo-600" />
      <div className="relative p-4">
        <div className="flex items-center gap-2 text-white/80 text-xs mb-3">
          <span className="font-medium">{call.args.location}</span>
          {isLoading && <span className="ml-auto">Loading...</span>}
        </div>
        {parsedResult.status === "error" ? (
          <div className="bg-red-500/20 rounded-lg p-3 text-red-200 text-sm">
            {parsedResult.content}
          </div>
        ) : (
          <div className="text-white text-lg font-medium">
            {parsedResult.content || "Fetching weather..."}
          </div>
        )}
      </div>
    </div>
  );
}
```

```typescript [types.ts]
import type { Message } from "@langchain/langgraph-sdk";

// å®šä¹‰å·¥å…·è°ƒç”¨ç±»å‹ä»¥åŒ¹é…ä½ çš„ Python æ™ºèƒ½ä½“çš„å·¥å…·
export type GetWeatherToolCall = {
  name: "get_weather";
  args: { location: string };
  id?: string;
};

// ä½ çš„æ™ºèƒ½ä½“ä¸­æ‰€æœ‰å·¥å…·è°ƒç”¨çš„è”åˆç±»å‹
export type AgentToolCalls = GetWeatherToolCall;

// ä½¿ç”¨ä½ çš„å·¥å…·è°ƒç”¨å®šä¹‰çŠ¶æ€ç±»å‹
export interface AgentState {
  messages: Message<AgentToolCalls>[];
}
```

```typescript [utils.ts]
import type { ToolMessage } from "@langchain/langgraph-sdk";

export function parseToolResult(result?: ToolMessage): {
  status: string;
  content: string;
} {
  if (!result) return { status: "pending", content: "" };
  try {
    return JSON.parse(result.content as string);
  } catch {
    return { status: "success", content: result.content as string };
  }
}
```

:::

<Card title="å°è¯•å·¥å…·è°ƒç”¨ç¤ºä¾‹" icon="hammer" href="https://github.com/langchain-ai/langgraphjs/tree/main/examples/ui-react/src/examples/tool-calling-agent">

åœ¨ `tool-calling-agent` ç¤ºä¾‹ä¸­æŸ¥çœ‹åŒ…å«å¤©æ°”ã€è®¡ç®—å™¨å’Œç¬”è®°å·¥å…·çš„å®Œæ•´å·¥å…·è°ƒç”¨æ¸²æŸ“å®ç°ã€‚

</Card>

## è‡ªå®šä¹‰æµå¼äº‹ä»¶

ä½¿ç”¨å·¥å…·æˆ–èŠ‚ç‚¹ä¸­çš„ `writer` ä»æ‚¨çš„æ™ºèƒ½ä½“ï¼ˆagentï¼‰æµå¼ä¼ è¾“è‡ªå®šä¹‰æ•°æ®ã€‚åœ¨ UI ä¸­ä½¿ç”¨ `onCustomEvent` å›è°ƒå¤„ç†è¿™äº›äº‹ä»¶ã€‚

::: code-group

```python [agent.py]
import asyncio
import time
from langchain import create_agent, tool
from langchain.types import ToolRuntime

@tool
async def analyze_data(data_source: str, *, config: ToolRuntime) -> str:
    """åˆ†ææ•°æ®å¹¶æ›´æ–°è¿›åº¦ã€‚"""
    steps = ["è¿æ¥ä¸­...", "è·å–ä¸­...", "å¤„ç†ä¸­...", "å®Œæˆï¼"]

    for i, step in enumerate(steps):
        # åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­å‘å‡ºè¿›åº¦äº‹ä»¶
        if config.writer:
            config.writer({
                "type": "progress",
                "id": f"analysis-{int(time.time() * 1000)}",
                "message": step,
                "progress": ((i + 1) / len(steps)) * 100,
            })
        await asyncio.sleep(0.5)

    return '{"result": "åˆ†æå®Œæˆ"}'

agent = create_agent(
    model="openai:gpt-4o-mini",
    tools=[analyze_data],
)
```

```tsx [Chat.tsx]
import { useState, useCallback } from "react";
import { useStream } from "@langchain/langgraph-sdk/react";
import type { AgentState } from "./types";

interface ProgressData {
  type: "progress";
  id: string;
  message: string;
  progress: number;
}

function isProgressData(data: unknown): data is ProgressData {
  return (
    typeof data === "object" &&
    data !== null &&
    "type" in data &&
    (data as ProgressData).type === "progress"
  );
}

function CustomStreamingUI() {
  const [progressData, setProgressData] = useState<Map<string, ProgressData>>(
    new Map()
  );

  const handleCustomEvent = useCallback((data: unknown) => {
    if (isProgressData(data)) {
      setProgressData((prev) => {
        const updated = new Map(prev);
        updated.set(data.id, data);
        return updated;
      });
    }
  }, []);

  const stream = useStream<AgentState>({
    assistantId: "custom-streaming",
    apiUrl: "http://localhost:2024",
    onCustomEvent: handleCustomEvent,
  });

  return (
    <div>
      {Array.from(progressData.values()).map((data) => (
        <div key={data.id} className="bg-neutral-800 rounded-lg p-4 mb-4">
          <div className="flex justify-between mb-2">
            <span className="text-sm text-white">{data.message}</span>
            <span className="text-xs text-neutral-400">{data.progress}%</span>
          </div>
          <div className="w-full bg-neutral-700 rounded-full h-2">
            <div
              className="bg-blue-500 h-2 rounded-full transition-all"
              style={{ width: `${data.progress}%` }}
            />
          </div>
        </div>
      ))}
    </div>
  );
}
```

```typescript [types.ts]
import type { Message } from "@langchain/langgraph-sdk";

// å®šä¹‰å·¥å…·è°ƒç”¨ä»¥åŒ¹é…æ‚¨çš„ Python æ™ºèƒ½ä½“ï¼ˆagentï¼‰
export type AnalyzeDataToolCall = {
  name: "analyze_data";
  args: { data_source: string };
  id?: string;
};

export type AgentToolCalls = AnalyzeDataToolCall;

export interface AgentState {
  messages: Message<AgentToolCalls>[];
}
```

:::

<Card title="å°è¯•è‡ªå®šä¹‰æµå¼å¤„ç†ç¤ºä¾‹" icon="bolt" href="https://github.com/langchain-ai/langgraphjs/tree/main/examples/ui-react/src/examples/custom-streaming">

åœ¨ `custom-streaming` ç¤ºä¾‹ä¸­æŸ¥çœ‹åŒ…å«è¿›åº¦æ¡ã€çŠ¶æ€å¾½ç« å’Œæ–‡ä»¶æ“ä½œå¡ç‰‡çš„è‡ªå®šä¹‰äº‹ä»¶çš„å®Œæ•´å®ç°ã€‚

</Card>

## äº‹ä»¶å¤„ç†

`useStream` é’©å­æä¾›äº†å›è°ƒé€‰é¡¹ï¼Œè®©ä½ å¯ä»¥è®¿é—®ä¸åŒç±»å‹çš„æµå¼äº‹ä»¶ã€‚ä½ æ— éœ€æ˜¾å¼é…ç½®æµæ¨¡å¼â€”â€”åªéœ€ä¸ºä½ æƒ³è¦å¤„ç†çš„äº‹ä»¶ç±»å‹ä¼ é€’å›è°ƒå‡½æ•°ï¼š

### å¯ç”¨çš„å›è°ƒ

| å›è°ƒå‡½æ•° | æè¿° | æµæ¨¡å¼ |
|----------|-------------|-------------|
| `onUpdateEvent` | åœ¨æ¯ä¸ªå›¾æ­¥éª¤åæ¥æ”¶åˆ°çŠ¶æ€æ›´æ–°æ—¶è°ƒç”¨ | `updates` |
| `onCustomEvent` | ä»ä½ çš„å›¾ä¸­æ¥æ”¶åˆ°è‡ªå®šä¹‰äº‹ä»¶æ—¶è°ƒç”¨ | `custom` |
| `onMetadataEvent` | æ¥æ”¶åˆ°è¿è¡Œå’Œçº¿ç¨‹å…ƒæ•°æ®æ—¶è°ƒç”¨ | `metadata` |
| `onError` | å‘ç”Ÿé”™è¯¯æ—¶è°ƒç”¨ | - |
| `onFinish` | æµå®Œæˆæ—¶è°ƒç”¨ | - |

## å¤šæ™ºèƒ½ä½“æµå¼å¤„ç†

åœ¨å¤„ç†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæˆ–å…·æœ‰å¤šä¸ªèŠ‚ç‚¹çš„å›¾æ—¶ï¼Œä½¿ç”¨æ¶ˆæ¯å…ƒæ•°æ®æ¥è¯†åˆ«æ¯æ¡æ¶ˆæ¯æ˜¯ç”±å“ªä¸ªèŠ‚ç‚¹ç”Ÿæˆçš„ã€‚å½“å¤šä¸ª LLM å¹¶è¡Œè¿è¡Œï¼Œå¹¶ä¸”ä½ å¸Œæœ›ä»¥ä¸åŒçš„è§†è§‰æ ·å¼æ˜¾ç¤ºå®ƒä»¬çš„è¾“å‡ºæ—¶ï¼Œè¿™å°¤å…¶æœ‰ç”¨ã€‚

<CodeGroup>

```python [agent.py]
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, START, END, Send
from langgraph.graph.state import CompiledStateGraph
from langchain.messages import BaseMessage, AIMessage
from typing import TypedDict, Annotated
import operator

# ä½¿ç”¨ä¸åŒçš„æ¨¡å‹å®ä¾‹ä»¥å¢åŠ å¤šæ ·æ€§
analytical_model = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)
creative_model = ChatOpenAI(model="gpt-4o-mini", temperature=0.9)
practical_model = ChatOpenAI(model="gpt-4o-mini", temperature=0.5)

class State(TypedDict):
    messages: Annotated[list[BaseMessage], operator.add]
    topic: str
    analytical_research: str
    creative_research: str
    practical_research: str

def fan_out_to_researchers(state: State) -> list[Send]:
    return [
        Send("researcher_analytical", state),
        Send("researcher_creative", state),
        Send("researcher_practical", state),
    ]

def dispatcher(state: State) -> dict:
    last_message = state["messages"][-1] if state["messages"] else None
    topic = last_message.content if last_message else ""
    return {"topic": topic}

async def researcher_analytical(state: State) -> dict:
    response = await analytical_model.ainvoke([
        {"role": "system", "content": "You are an analytical research expert."},
        {"role": "user", "content": f"Research: {state['topic']}"},
    ])
    return {
        "analytical_research": response.content,
        "messages": [AIMessage(content=response.content, name="researcher_analytical")],
    }

# ä¸ºåˆ›æ„å‹å’Œå®ç”¨å‹ç ”ç©¶å‘˜å®šä¹‰ç±»ä¼¼çš„èŠ‚ç‚¹...

workflow = StateGraph(State)
workflow.add_node("dispatcher", dispatcher)
workflow.add_node("researcher_analytical", researcher_analytical)
workflow.add_node("researcher_creative", researcher_creative)
workflow.add_node("researcher_practical", researcher_practical)
workflow.add_edge(START, "dispatcher")
workflow.add_conditional_edges("dispatcher", fan_out_to_researchers)
workflow.add_edge("researcher_analytical", END)
workflow.add_edge("researcher_creative", END)
workflow.add_edge("researcher_practical", END)

agent: CompiledStateGraph = workflow.compile()
```

```tsx [Chat.tsx]
import { useStream } from "@langchain/langgraph-sdk/react";
import type { AgentState } from "./types";
import { MessageBubble } from "./MessageBubble";

// ç”¨äºè§†è§‰æ˜¾ç¤ºçš„èŠ‚ç‚¹é…ç½®
const NODE_CONFIG: Record<string, { label: string; color: string }> = {
  researcher_analytical: { label: "Analytical Research", color: "cyan" },
  researcher_creative: { label: "Creative Research", color: "purple" },
  researcher_practical: { label: "Practical Research", color: "emerald" },
};

function MultiAgentChat() {
  const stream = useStream<AgentState>({
    assistantId: "parallel-research",
    apiUrl: "http://localhost:2024",
  });

  return (
    <div className="flex flex-col gap-4">
      {stream.messages.map((message, idx) => {
        if (message.type !== "ai") {
          return <MessageBubble key={message.id ?? idx} message={message} />;
        }

        const metadata = stream.getMessagesMetadata?.(message);
        const nodeName =
          (metadata?.streamMetadata?.langgraph_node as string) ||
          (message as { name?: string }).name;

```typescript types.ts

// çŠ¶æ€ä¸ä½ çš„ Python æ™ºèƒ½ä½“çš„ State TypedDict åŒ¹é…

  topic: string;
  analytical_research: string;
  creative_research: string;
  practical_research: string;
}
```

</CodeGroup>
:::

:::js
<CodeGroup>

```tsx Chat.tsx

// ç”¨äºè§†è§‰æ˜¾ç¤ºçš„èŠ‚ç‚¹é…ç½®
const NODE_CONFIG: Record<string, { label: string; color: string }> = {
  researcher_analytical: { label: "åˆ†æç ”ç©¶", color: "cyan" },
  researcher_creative: { label: "åˆ›æ„ç ”ç©¶", color: "purple" },
  researcher_practical: { label: "å®ç”¨ç ”ç©¶", color: "emerald" },
};

function MultiAgentChat() {
  const stream = useStream<typeof agent>({
assistantId: "parallel-research",
apiUrl: "http://localhost:2024",
  });

  return (
    
<div className="flex flex-col gap-4">

{stream.messages.map((message, idx) => {
  if (message.type !== "ai") {
return <MessageBubble :key="message.id ?? idx" :message="message" />;
  }

  // è·å–æµå¼å…ƒæ•°æ®ä»¥è¯†åˆ«æºèŠ‚ç‚¹
  const metadata = stream.getMessagesMetadata?.(message);
  const nodeName =
(metadata?.streamMetadata?.langgraph_node as string) ||
(message as { name?: string }).name;

  const config = nodeName ? NODE_CONFIG[nodeName] : null;

  if (!config) {
return <MessageBubble :key="message.id ?? idx" :message="message" />;
  }

  return (
<div
:key="message.id ?? idx"
:className="`bg-${config.color"-950/30 border border-${config.color}-500/30 rounded-xl p-4`}
>
<div :className="`text-sm font-semibold text-${config.color"-400 mb-2`}>
{config.label}

</div>

 
<div className="text-neutral-200 whitespace-pre-wrap">

{typeof message.content === "string" ? message.content : ""}

</div>

 </div>
);
})}
    </div>
  );
}
```

```typescript agent.ts

// ä½¿ç”¨ä¸åŒçš„æ¨¡å‹å®ä¾‹ä»¥è·å¾—å¤šæ ·æ€§
const analyticalModel = new ChatOpenAI({ model: "gpt-4o-mini", temperature: 0.3 });
const creativeModel = new ChatOpenAI({ model: "gpt-4o-mini", temperature: 0.9 });
const practicalModel = new ChatOpenAI({ model: "gpt-4o-mini", temperature: 0.5 });

// å®šä¹‰çŠ¶æ€æ¨¡å¼
const StateAnnotation = z.object({
  messages: withLangGraph(z.custom<BaseMessage[]>(), {
reducer: {
fn: (left: BaseMessage[], right: BaseMessage | BaseMessage[]) =>
Array.isArray(right) ? left.concat(right) : left.concat([right]),
},
default: () => [],
  }),
  topic: z.string().default(""),
  analyticalResearch: z.string().default(""),
  creativeResearch: z.string().default(""),
  practicalResearch: z.string().default(""),
});

type State = z.infer<typeof StateAnnotation>;

// æ‰‡å‡ºåˆ°å¹¶è¡Œç ”ç©¶äººå‘˜
function fanOutToResearchers(state: State): Send[] {
  return [
new Send("researcher_analytical", state),
new Send("researcher_creative", state),
new Send("researcher_practical", state),
  ];
}

async function dispatcherNode(state: State): Promise<Partial<State>> {
  const lastMessage = state.messages.at(-1);
  const topic = typeof lastMessage?.content === "string" ? lastMessage.content : "";
  return { topic };
}

async function analyticalResearcherNode(state: State): Promise<Partial<State>> {
  const response = await analyticalModel.invoke([
{ role: "system", content: "ä½ æ˜¯ä¸€ä½åˆ†æç ”ç©¶ä¸“å®¶ã€‚ä¸“æ³¨äºæ•°æ®å’Œè¯æ®ã€‚" },
{ role: "user", content: `ç ”ç©¶ä¸»é¢˜: ${state.topic}` },
  ]);
  return {
analyticalResearch: response.content as string,
messages: [new AIMessage({ content: response.content as string, name: "researcher_analytical" })],
  };
}

// åˆ›æ„å‹å’Œå®ç”¨å‹ç ”ç©¶äººå‘˜çš„ç±»ä¼¼èŠ‚ç‚¹...

// æ„å»ºæ”¯æŒå¹¶è¡Œæ‰§è¡Œçš„å›¾
const workflow = new StateGraph(StateAnnotation)
  .addNode("dispatcher", dispatcherNode)
  .addNode("researcher_analytical", analyticalResearcherNode)
  .addNode("researcher_creative", creativeResearcherNode)
  .addNode("researcher_practical", practicalResearcherNode)
  .addEdge(START, "dispatcher")
  .addConditionalEdges("dispatcher", fanOutToResearchers)
  .addEdge("researcher_analytical", END)
  .addEdge("researcher_creative", END)
  .addEdge("researcher_practical", END);

```

</CodeGroup>
:::

<Card title="å°è¯•å¹¶è¡Œç ”ç©¶ç¤ºä¾‹" icon="users" href="https://github.com/langchain-ai/langgraphjs/tree/main/examples/ui-react/src/examples/parallel-research">
  åœ¨ `parallel-research` ç¤ºä¾‹ä¸­æŸ¥çœ‹ä¸€ä¸ªå®Œæ•´çš„å¤šæ™ºèƒ½ä½“æµå¼å¤„ç†å®ç°ï¼ŒåŒ…å«ä¸‰ä¸ªå¹¶è¡Œç ”ç©¶äººå‘˜å’Œç‹¬ç‰¹çš„è§†è§‰æ ·å¼ã€‚
</Card>

## äººæœºååŒï¼ˆHuman-in-the-loopï¼‰

å½“æ™ºèƒ½ä½“éœ€è¦äººå·¥æ‰¹å‡†æ‰èƒ½æ‰§è¡Œå·¥å…·æ—¶ï¼Œå¤„ç†ä¸­æ–­ã€‚åœ¨[å¦‚ä½•å¤„ç†ä¸­æ–­](/oss/langgraph/interrupts#pause-using-interrupt)æŒ‡å—ä¸­äº†è§£æ›´å¤šä¿¡æ¯ã€‚

:::python
<CodeGroup>

```python agent.py
from langchain import create_agent, tool, human_in_the_loop_middleware
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver

model = ChatOpenAI(model="gpt-4o-mini")

@tool
def send_email(to: str, subject: str, body: str) -> dict:
"""å‘é€ç”µå­é‚®ä»¶ã€‚éœ€è¦äººå·¥æ‰¹å‡†ã€‚"""
return {
"status": "success",
"content": f'å·²å‘ {to} å‘é€ä¸»é¢˜ä¸º "{subject}" çš„é‚®ä»¶',
}

@tool
def delete_file(path: str) -> dict:
"""åˆ é™¤æ–‡ä»¶ã€‚éœ€è¦äººå·¥æ‰¹å‡†ã€‚"""
return {"status": "success", "content": f'æ–‡ä»¶ "{path}" å·²åˆ é™¤'}

@tool
def read_file(path: str) -> dict:
"""è¯»å–æ–‡ä»¶å†…å®¹ã€‚æ— éœ€æ‰¹å‡†ã€‚"""
return {"status": "success", "content": f"{path} çš„å†…å®¹..."}

agent = create_agent(
model=model,
tools=[send_email, delete_file, read_file],
middleware=[
human_in_the_loop_middleware(
interrupt_on={
"send_email": {
"allowed_decisions": ["approve", "edit", "reject"],
"description": "ğŸ“§ å‘é€å‰å®¡æ ¸é‚®ä»¶",
},
"delete_file": {
"allowed_decisions": ["approve", "reject"],
"description": "ğŸ—‘ï¸ ç¡®è®¤æ–‡ä»¶åˆ é™¤",
},
"read_file": False,  // å®‰å…¨ - è‡ªåŠ¨æ‰¹å‡†
}
),
],
checkpointer=MemorySaver(),
)
```

```tsx Chat.tsx

function HumanInTheLoopChat() {
  const stream = useStream<AgentState, { InterruptType: HITLRequest }>({
assistantId: "human-in-the-loop",
apiUrl: "http://localhost:2024",
  });

  const [isProcessing, setIsProcessing] = useState(false);
  const hitlRequest = stream.interrupt?.value as HITLRequest | undefined;

  const handleApprove = async (index: number) => {
if (!hitlRequest) return;
setIsProcessing(true);

try {
const decisions: HITLResponse["decisions"] =
hitlRequest.actionRequests.map((_, i) =>
i === index ? { type: "approve" } : { type: "approve" }
);

await stream.submit(null, {
command: { resume: { decisions } as HITLResponse },
});
} finally {
setIsProcessing(false);
}
  };

  const handleReject = async (index: number, reason: string) => {
if (!hitlRequest) return;
setIsProcessing(true);

try {
const decisions: HITLResponse["decisions"] =
hitlRequest.actionRequests.map((_, i) =>
i === index
? { type: "reject", message: reason }
: { type: "reject", message: "Rejected along with other actions" }
);

await stream.submit(null, {
command: { resume: { decisions } as HITLResponse },
});
} finally {
setIsProcessing(false);
}
  };

  return (
    <div>
{stream.messages.map((message, idx) => (
 <MessageBubble :key="message.id ?? idx" :message="message" />
))}

{hitlRequest && hitlRequest.actionRequests.length > 0 && (
 <div className="bg-amber-900/20 border border-amber-500/30 rounded-xl p-4 mt-4">
 <h3 className="text-amber-400 font-semibold mb-4">
æ“ä½œéœ€è¦æ‰¹å‡†
 </h3>

{hitlRequest.actionRequests.map((action, idx) => (
 <div :key="idx" className="bg-neutral-900 rounded-lg p-4 mb-4 last:mb-0">
 <div className="text-sm font-mono text-white mb-2">{action.name}</div>
 <pre className="text-xs bg-black rounded p-2 mb-3 overflow-x-auto">
{JSON.stringify(action.args, null, 2)}
 </pre>
 
<div className="flex gap-2">

<button
  onClick={() => handleApprove(idx)}
  disabled={isProcessing}
  className="px-3 py-1.5 bg-green-600 hover:bg-green-500 text-white text-sm rounded-lg"
>
  æ‰¹å‡†
</button>
<button
  onClick={() => handleReject(idx, "ç”¨æˆ·å·²æ‹’ç»")}
  disabled={isProcessing}
  className="px-3 py-1.5 bg-red-600 hover:bg-red-500 text-white text-sm rounded-lg"
>
  æ‹’ç»
</button>

</div>

 </div>
))}
 </div>
)}
    </div>
  );
}
```

```typescript types.ts

// ä¸æ‚¨çš„ Python æ™ºèƒ½ä½“åŒ¹é…çš„å·¥å…·è°ƒç”¨ç±»å‹

  name: "send_email";
  args: { to: string; subject: string; body: string };
  id?: string;
};

  name: "delete_file";
  args: { path: string };
  id?: string;
};

  name: "read_file";
  args: { path: string };
  id?: string;
};

}

// äººæœºååŒï¼ˆHITLï¼‰ç±»å‹

args: Record<string, unknown>;
  }>;
}

    | { type: "edit"; newArgs: Record<string, unknown> }
  >;
}
```

</CodeGroup>
:::

:::js
<CodeGroup>

```tsx Chat.tsx

function HumanInTheLoopChat() {
  const stream = useStream<typeof agent, { InterruptType: HITLRequest }>({
assistantId: "human-in-the-loop",
apiUrl: "http://localhost:2024",
  });

  const [isProcessing, setIsProcessing] = useState(false);

  // å¯¹ä¸­æ–­å€¼çš„ç±»å‹æ–­è¨€
  const hitlRequest = stream.interrupt?.value as HITLRequest | undefined;

  const handleApprove = async (index: number) => {
if (!hitlRequest) return;
setIsProcessing(true);

try {
const decisions: HITLResponse["decisions"] =
hitlRequest.actionRequests.map((_, i) =>
i === index ? { type: "approve" } : { type: "approve" }
);

await stream.submit(null, {
command: {
resume: { decisions } as HITLResponse,
},
});
} finally {
setIsProcessing(false);
}
  };

  const handleReject = async (index: number, reason: string) => {
if (!hitlRequest) return;
setIsProcessing(true);

try {
const decisions: HITLResponse["decisions"] =
hitlRequest.actionRequests.map((_, i) =>
i === index
? { type: "reject", message: reason }
: { type: "reject", message: "Rejected along with other actions" }
);

await stream.submit(null, {
command: {
resume: { decisions } as HITLResponse,
},
});
} finally {
setIsProcessing(false);
}
  };

  return (
    <div>
 
{stream.messages.map((message, idx) => (
 <MessageBubble :key="message.id ?? idx" :message="message" />
))}

 
{hitlRequest && hitlRequest.actionRequests.length > 0 && (
 <div className="bg-amber-900/20 border border-amber-500/30 rounded-xl p-4 mt-4">
 <h3 className="text-amber-400 font-semibold mb-4">
æ“ä½œéœ€è¦å®¡æ‰¹
 </h3>

{hitlRequest.actionRequests.map((action, idx) => (
 <div
:key="idx"
className="bg-neutral-900 rounded-lg p-4 mb-4 last:mb-0"
>
 <div className="flex items-center gap-2 mb-2">
 <span className="text-sm font-mono text-white">
{action.name}
 </span>
 </div>

 <pre className="text-xs bg-black rounded p-2 mb-3 overflow-x-auto">
{JSON.stringify(action.args, null, 2)}
 </pre>

 
<div className="flex gap-2">

<button
  onClick={() => handleApprove(idx)}
  disabled={isProcessing}
  className="px-3 py-1.5 bg-green-600 hover:bg-green-700 text-white text-sm rounded disabled:opacity-50"
>
  æ‰¹å‡†
</button>
<button
  onClick={() => handleReject(idx, "User rejected")}
  disabled={isProcessing}
  className="px-3 py-1.5 bg-red-600 hover:bg-red-700 text-white text-sm rounded disabled:opacity-50"
>
  æ‹’ç»
</button>

</div>

 </div>
))}
 </div>
)}
    </div>
  );
}
```

```typescript agent.ts

const model = new ChatOpenAI({ model: "gpt-4o-mini" });

// éœ€è¦äººå·¥æ‰¹å‡†çš„å·¥å…·
const sendEmail = tool(
  async ({ to, subject, body }) => {
return {
status: "success",
content: `Email sent to ${to} with subject "${subject}"`,
};
  },
  {
name: "send_email",
description: "å‘é€ç”µå­é‚®ä»¶ã€‚éœ€è¦äººå·¥æ‰¹å‡†ã€‚",
schema: z.object({
to: z.string().describe("æ”¶ä»¶äººé‚®ç®±åœ°å€"),
subject: z.string().describe("é‚®ä»¶ä¸»é¢˜"),
body: z.string().describe("é‚®ä»¶æ­£æ–‡"),
}),
  }
);

// éœ€è¦æ‰¹å‡†ä¸”é€‰é¡¹æœ‰é™çš„å·¥å…·
const deleteFile = tool(
  async ({ path }) => {
return { status: "success", content: `File "${path}" deleted` };
  },
  {
name: "delete_file",
description: "åˆ é™¤æ–‡ä»¶ã€‚éœ€è¦äººå·¥æ‰¹å‡†ã€‚",
schema: z.object({
path: z.string().describe("è¦åˆ é™¤çš„æ–‡ä»¶è·¯å¾„"),
}),
  }
);

// å®‰å…¨å·¥å…· - æ— éœ€æ‰¹å‡†
const readFile = tool(
  async ({ path }) => {
return { status: "success", content: `Contents of ${path}...` };
  },
  {
name: "read_file",
description: "è¯»å–æ–‡ä»¶å†…å®¹ã€‚æ— éœ€æ‰¹å‡†ã€‚",
schema: z.object({
path: z.string().describe("è¦è¯»å–çš„æ–‡ä»¶è·¯å¾„"),
}),
  }
);

// åˆ›å»ºå¸¦æœ‰äººæœºååŒï¼ˆhuman-in-the-loopï¼‰ä¸­é—´ä»¶çš„æ™ºèƒ½ä½“ï¼ˆagentï¼‰

  model,
  tools: [sendEmail, deleteFile, readFile],
  middleware: [
humanInTheLoopMiddleware({
interruptOn: {
// ç”µå­é‚®ä»¶éœ€è¦æ‰€æœ‰å†³ç­–ç±»å‹
send_email: {
allowedDecisions: ["approve", "edit", "reject"],
description: "ğŸ“§ å‘é€å‰å®¡æ ¸é‚®ä»¶",
},
// åˆ é™¤æ“ä½œä»…å…è®¸æ‰¹å‡†/æ‹’ç»
delete_file: {
allowedDecisions: ["approve", "reject"],
description: "ğŸ—‘ï¸ ç¡®è®¤æ–‡ä»¶åˆ é™¤",
},
// è¯»å–æ“ä½œæ˜¯å®‰å…¨çš„ - è‡ªåŠ¨æ‰¹å‡†
read_file: false,
},
}),
  ],
  // äººæœºååŒï¼ˆhuman-in-the-loopï¼‰å¿…éœ€ - åœ¨ä¸­æ–­æœŸé—´æŒä¹…åŒ–çŠ¶æ€
  checkpointer: new MemorySaver(),
});
```

</CodeGroup>
:::

<Card title="å°è¯•äººæœºååŒï¼ˆhuman-in-the-loopï¼‰ç¤ºä¾‹" icon="hand" href="https://github.com/langchain-ai/langgraphjs/tree/main/examples/ui-react/src/examples/human-in-the-loop">
  åœ¨ `human-in-the-loop` ç¤ºä¾‹ä¸­æŸ¥çœ‹åŒ…å«æ‰¹å‡†ã€æ‹’ç»å’Œç¼–è¾‘æ“ä½œçš„å®Œæ•´å®¡æ‰¹å·¥ä½œæµå®ç°ã€‚
</Card>

## æ¨ç†æ¨¡å‹

<Warning>
æ‰©å±•æ¨ç†/æ€è€ƒæ”¯æŒç›®å‰å¤„äºå®éªŒé˜¶æ®µã€‚æ¨ç†ä»¤ç‰Œï¼ˆreasoning tokensï¼‰çš„æµå¼æ¥å£å› æä¾›å•†ï¼ˆOpenAI ä¸ Anthropicï¼‰è€Œå¼‚ï¼Œå¹¶å¯èƒ½éšç€æŠ½è±¡å±‚çš„å¼€å‘è€Œæ”¹å˜ã€‚
</Warning>

å½“ä½¿ç”¨å…·æœ‰æ‰©å±•æ¨ç†èƒ½åŠ›çš„æ¨¡å‹ï¼ˆå¦‚ OpenAI çš„æ¨ç†æ¨¡å‹æˆ– Anthropic çš„æ‰©å±•æ€è€ƒï¼‰æ—¶ï¼Œæ€è€ƒè¿‡ç¨‹ä¼šåµŒå…¥åˆ°æ¶ˆæ¯å†…å®¹ä¸­ã€‚æ‚¨éœ€è¦å•ç‹¬æå–å¹¶æ˜¾ç¤ºå®ƒã€‚

:::python
<CodeGroup>

```python agent.py
from langchain import create_agent
from langchain_openai import ChatOpenAI

# ä½¿ç”¨å…·å¤‡æ¨ç†èƒ½åŠ›çš„æ¨¡å‹
# OpenAI: o1, o1-mini, o1-preview
# Anthropic: å¯ç”¨æ‰©å±•æ€è€ƒçš„ claude-sonnet-4-20250514
model = ChatOpenAI(model="o1-mini")

agent = create_agent(
model=model,
tools=[],  # æ¨ç†æ¨¡å‹æœ€é€‚åˆå¤æ‚çš„æ¨ç†ä»»åŠ¡
)
```

```tsx Chat.tsx

function ReasoningChat() {
  const stream = useStream<AgentState>({
assistantId: "reasoning-agent",
apiUrl: "http://localhost:2024",
  });

  return (
    
<div className="flex flex-col gap-4">

{stream.messages.map((message, idx) => {
  if (message.type === "ai") {
const reasoning = getReasoningFromMessage(message);
const textContent = getTextContent(message);

return (
<div :key="message.id ?? idx">
{reasoning && (
<div className="mb-4">
<div className="text-xs font-medium text-amber-400/80 mb-2">
æ¨ç†è¿‡ç¨‹

</div>

 
<div className="bg-amber-950/50 border border-amber-500/20 rounded-2xl px-4 py-3">

<div className="text-sm text-amber-100/90 whitespace-pre-wrap">
  {reasoning}

</div>

 </div>
 </div>
)}

{textContent && (
 
<div className="text-neutral-100 whitespace-pre-wrap">

{textContent}

</div>

)}
 </div>
);
}

return <MessageBubble :key="message.id ?? idx" :message="message" />;
})}

{stream.isLoading && (
 
<div className="flex items-center gap-2 text-amber-400/70">

<span className="text-sm">æ€è€ƒä¸­...</span>

</div>

)}
    </div>
  );
}
```

```typescript types.ts

}
```

```typescript utils.ts

/**
 * ä» AI æ¶ˆæ¯ä¸­æå–æ¨ç†/æ€è€ƒå†…å®¹ã€‚
 * åŒæ—¶æ”¯æŒ OpenAI æ¨ç†å’Œ Anthropic æ‰©å±•æ€è€ƒã€‚
 */

additional_kwargs?: {
reasoning?: {
summary?: Array<{ type: string; text: string }>;
};
};
contentBlocks?: Array<{ type: string; thinking?: string }>;
  };

  const msg = message as MessageWithExtras;

  // æ£€æŸ¥ additional_kwargs ä¸­æ˜¯å¦å­˜åœ¨ OpenAI æ¨ç†
  if (msg.additional_kwargs?.reasoning?.summary) {
const content = msg.additional_kwargs.reasoning.summary
.filter((item) => item.type === "summary_text")
.map((item) => item.text)
.join("");
if (content.trim()) return content;
  }

  // æ£€æŸ¥ contentBlocks ä¸­æ˜¯å¦å­˜åœ¨ Anthropic æ€è€ƒ
  if (msg.contentBlocks?.length) {
const thinking = msg.contentBlocks
.filter((b) => b.type === "thinking" && b.thinking)
.map((b) => b.thinking)
.join("\n");
if (thinking) return thinking;
  }

  // æ£€æŸ¥ message.content æ•°ç»„ä¸­æ˜¯å¦å­˜åœ¨æ€è€ƒå†…å®¹
  if (Array.isArray(msg.content)) {
const thinking = msg.content
.filter((b): b is { type: "thinking"; thinking: string } =>
typeof b === "object" && b?.type === "thinking" && "thinking" in b
)
.map((b) => b.thinking)
.join("\n");
if (thinking) return thinking;
  }

  return undefined;
}

/**
 * ä»æ¶ˆæ¯ä¸­æå–æ–‡æœ¬å†…å®¹ã€‚
 */

  if (Array.isArray(message.content)) {
return message.content
.filter((c): c is { type: "text"; text: string } => c.type === "text")
.map((c) => c.text)
.join("");
  }
  return "";
}
```

</CodeGroup>
:::

:::js
<CodeGroup>

```tsx Chat.tsx

function ReasoningChat() {
  const stream = useStream<typeof agent>({
assistantId: "reasoning-agent",
apiUrl: "http://localhost:2024",
  });

  return (
    
<div className="flex flex-col gap-4">

{stream.messages.map((message, idx) => {
  if (message.type === "ai") {
const reasoning = getReasoningFromMessage(message);
const textContent = getTextContent(message);

return (
<div :key="message.id ?? idx">

{reasoning && (
<div className="mb-4">
<div className="text-xs font-medium text-amber-400/80 mb-2">
æ¨ç†

</div>

 
<div className="bg-amber-950/50 border border-amber-500/20 rounded-2xl px-4 py-3">

<div className="text-sm text-amber-100/90 whitespace-pre-wrap">
  {reasoning}

</div>

 </div>
 </div>
)}

    
{textContent && (
 
<div className="text-neutral-100 whitespace-pre-wrap">

{textContent}

</div>

)}
  </div>
);
}

return <MessageBubble :key="message.id ?? idx" :message="message" />;
})}

{stream.isLoading && (

<div className="flex items-center gap-2 text-amber-400/70">

<span className="text-sm">æ€è€ƒä¸­...</span>

</div>

)}
</div>
);
}
```

```typescript utils.ts

/**
 * ä» AI æ¶ˆæ¯ä¸­æå–æ¨ç†/æ€è€ƒå†…å®¹ã€‚
 * åŒæ—¶æ”¯æŒ OpenAI æ¨ç† (additional_kwargs.reasoning.summary)
 * å’Œ Anthropic æ‰©å±•æ€è€ƒ (ç±»å‹ä¸º "thinking" çš„å†…å®¹å—)ã€‚
 */

additional_kwargs?: {
reasoning?: {
summary?: Array<{ type: string; text: string }>;
};
};
contentBlocks?: Array<{ type: string; thinking?: string }>;
  };

  const msg = message as MessageWithExtras;

  // æ£€æŸ¥ additional_kwargs ä¸­æ˜¯å¦å­˜åœ¨ OpenAI æ¨ç†
  if (msg.additional_kwargs?.reasoning?.summary) {
const content = msg.additional_kwargs.reasoning.summary
.filter((item) => item.type === "summary_text")
.map((item) => item.text)
.join("");

if (content.trim()) return content;
  }

  // æ£€æŸ¥ contentBlocks ä¸­æ˜¯å¦å­˜åœ¨ Anthropic æ€è€ƒ
  if (msg.contentBlocks?.length) {
const thinking = msg.contentBlocks
.filter((b) => b.type === "thinking" && b.thinking)
.map((b) => b.thinking)
.join("\n");

if (thinking) return thinking;
  }

  // æ£€æŸ¥ message.content æ•°ç»„ä¸­æ˜¯å¦å­˜åœ¨æ€è€ƒå†…å®¹
  if (Array.isArray(msg.content)) {
const thinking = msg.content
.filter((b): b is { type: "thinking"; thinking: string } =>
typeof b === "object" && b?.type === "thinking" && "thinking" in b
)
.map((b) => b.thinking)
.join("\n");

if (thinking) return thinking;
  }

  return undefined;
}

/**
 * ä»æ¶ˆæ¯ä¸­æå–æ–‡æœ¬å†…å®¹ã€‚
 */

  if (Array.isArray(message.content)) {
return message.content
.filter((c): c is { type: "text"; text: string } => c.type === "text")
.map((c) => c.text)
.join("");
  }

  return "";
}
```

</CodeGroup>
:::

<Card title="å°è¯•æ¨ç†ç¤ºä¾‹" icon="brain" href="https://github.com/langchain-ai/langgraphjs/tree/main/examples/ui-react/src/examples/reasoning-agent">
  åœ¨ `reasoning-agent` ç¤ºä¾‹ä¸­æŸ¥çœ‹ä½¿ç”¨ OpenAI å’Œ Anthropic æ¨¡å‹å±•ç¤ºæ¨ç†ä»¤ç‰Œçš„å®Œæ•´å®ç°ã€‚
</Card>

## è‡ªå®šä¹‰çŠ¶æ€ç±»å‹

å¯¹äºè‡ªå®šä¹‰çš„ LangGraph åº”ç”¨ç¨‹åºï¼Œè¯·å°†æ‚¨çš„å·¥å…·è°ƒç”¨ç±»å‹åµŒå…¥åˆ°çŠ¶æ€çš„ messages å±æ€§ä¸­ã€‚

:::js

```tsx

// å°†æ‚¨çš„å·¥å…·è°ƒç”¨ç±»å‹å®šä¹‰ä¸ºå¯è¾¨è¯†è”åˆç±»å‹
type MyToolCalls =
  | { name: "search"; args: { query: string }; id?: string }
  | { name: "calculate"; args: { expression: string }; id?: string };

// åœ¨çŠ¶æ€æ¶ˆæ¯ä¸­åµŒå…¥å·¥å…·è°ƒç”¨ç±»å‹
interface MyGraphState {
  messages: Message<MyToolCalls>[];
  context?: string;
}

function CustomGraphChat() {
  const stream = useStream<MyGraphState>({
assistantId: "my-graph",
apiUrl: "http://localhost:2024",
  });

  // stream.values è¢«ç±»å‹åŒ–ä¸º MyGraphState
  // stream.toolCalls[0].call.name è¢«ç±»å‹åŒ–ä¸º "search" | "calculate"
}
```

ä½ ä¹Ÿå¯ä»¥ä¸ºä¸­æ–­ï¼ˆinterruptsï¼‰å’Œå¯é…ç½®é€‰é¡¹æŒ‡å®šé¢å¤–çš„ç±»å‹é…ç½®ï¼š

```tsx
interface MyGraphState {
  messages: Message<MyToolCalls>[];
}

function CustomGraphChat() {
  const stream = useStream<
MyGraphState,
{
InterruptType: { question: string };
ConfigurableType: { userId: string };
}
  >({
assistantId: "my-graph",
apiUrl: "http://localhost:2024",
  });

  // stream.interrupt è¢«ç±»å‹åŒ–ä¸º { question: string } | undefined
}
```
:::

## è‡ªå®šä¹‰ä¼ è¾“

å¯¹äºè‡ªå®šä¹‰ API ç«¯ç‚¹æˆ–éæ ‡å‡†éƒ¨ç½²ï¼Œå¯ä»¥ä½¿ç”¨ `transport` é€‰é¡¹é…åˆ `FetchStreamTransport` æ¥è¿æ¥åˆ°ä»»ä½•æµå¼ APIã€‚

:::js

```tsx

function CustomAPIChat({ apiKey }: { apiKey: string }) {
  // åˆ›å»ºå¸¦æœ‰è‡ªå®šä¹‰è¯·æ±‚å¤„ç†çš„ä¼ è¾“å™¨
  const transport = useMemo(() => {
return new FetchStreamTransport({
apiUrl: "/api/my-agent",
onRequest: async (url: string, init: RequestInit) => {
// å°† API å¯†é’¥æˆ–å…¶ä»–è‡ªå®šä¹‰æ•°æ®æ³¨å…¥è¯·æ±‚
const customBody = JSON.stringify({
...(JSON.parse(init.body as string) || {}),
apiKey,
});

return {
...init,
body: customBody,
headers: {
...init.headers,
"X-Custom-Header": "value",
},
};
},
});
  }, [apiKey]);

  const stream = useStream({
transport,
  });

  // æ­£å¸¸ä½¿ç”¨ stream
  return (
    <div>
{stream.messages.map((message, idx) => (
 <MessageBubble :key="message.id ?? idx" :message="message" />
))}
    </div>
  );
}
```

## ç›¸å…³

- [æµå¼å¤„ç†æ¦‚è¿°](/oss/python/langchain/streaming/overview) â€” ä½¿ç”¨ LangChain æ™ºèƒ½ä½“è¿›è¡ŒæœåŠ¡å™¨ç«¯æµå¼å¤„ç†
- [useStream API å‚è€ƒ](https://reference.langchain.com/javascript/functions/_langchain_langgraph-sdk.react.useStream.html) â€” å®Œæ•´çš„ API æ–‡æ¡£
- [æ™ºèƒ½ä½“èŠå¤©ç•Œé¢](/oss/python/langchain/ui) â€” LangGraph æ™ºèƒ½ä½“çš„é¢„æ„å»ºèŠå¤©ç•Œé¢
- [äººæœºååŒ](/oss/python/langchain/human-in-the-loop) â€” é…ç½®äººå·¥å®¡æ ¸çš„ä¸­æ–­
- [å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ](/oss/python/langchain/multi-agent) â€” æ„å»ºä½¿ç”¨å¤šä¸ª LLM çš„æ™ºèƒ½ä½“
